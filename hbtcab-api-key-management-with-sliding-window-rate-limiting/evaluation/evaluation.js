const fs = require("fs");
const path = require("path");
const { spawn } = require("child_process");
const { v4: uuidv4 } = require("uuid");
const os = require("os");

const ROOT = path.resolve(__dirname, "..");
const REPORTS_DIR = path.join(ROOT, "evaluation", "reports");

// Ensure reports directory exists
if (!fs.existsSync(REPORTS_DIR)) {
  fs.mkdirSync(REPORTS_DIR, { recursive: true });
}

function getEnvironmentInfo() {
  return {
    node_version: process.version,
    platform: os.platform(),
    arch: os.arch(),
    cpus: os.cpus().length,
  };
}

/**
 * Run the test suite in tests/ via our custom runner. Returns rich metrics.
 */
function runTests(repoPath, opts = {}) {
  const { timeoutMs = 30_000 } = opts;
  return new Promise((resolve) => {
    const started = Date.now();
    const proc = spawn("node", ["tests/api-keys.test.js"], {
      cwd: ROOT,
      env: {
        ...process.env,
        REPO_PATH: repoPath,
        SKIP_SCHEMA_INIT: "1",
      },
    });

    let stdout = "";
    let stderr = "";

    const killTimer = setTimeout(() => {
      try { proc.kill("SIGKILL"); } catch (_) {}
    }, timeoutMs);

    proc.stdout.on("data", (d) => { stdout += d.toString(); });
    proc.stderr.on("data", (d) => { stderr += d.toString(); });

    proc.on("close", (code, signal) => {
      clearTimeout(killTimer);
      const finished = Date.now();
      const duration = (finished - started) / 1000;

      // Determine pass/fail robustly
      let passed = code === 0;
      const passMarker = (stdout || "").includes("âœ“ All requirements verified") || (stdout || "").includes("Total: 12, Passed: 12");
      if (!passed && passMarker) passed = true;

      // Parse counts if available
      let totals = { total: null, passedCount: null, failedCount: null };
      const m = /Total:\s*(\d+),\s*Passed:\s*(\d+),\s*Failed:\s*(\d+)/.exec(stdout);
      if (m) {
        totals = { total: Number(m[1]), passedCount: Number(m[2]), failedCount: Number(m[3]) };
      }

      const outputDetails = (stderr || stdout).slice(0, 1000);
      resolve({
        passed,
        return_code: code ?? null,
        duration_seconds: duration,
        output: outputDetails,
        totals,
      });
    });
  });
}

async function runEvaluation() {
  const runId = uuidv4();
  const startTime = new Date();
  const startTimeIso = startTime.toISOString();

  console.log(`Starting evaluation (Run ID: ${runId})...`);

  // In mock mode, baseline (repository_before) is intentionally empty and shouldn't be executed.
  const autoSkipBefore = process.env.USE_MOCKS === "1";
  const skipBefore = autoSkipBefore || process.env.EVAL_SKIP_BEFORE === "1";

  // 1. Run Tests against "repository_before" (Baseline)
  let beforeResult = {
    passed: false,
    return_code: null,
    duration_seconds: 0,
    output: "",
    totals: { total: null, passedCount: null, failedCount: null },
  };
  if (skipBefore) {
    console.log("Skipping baseline tests (EVAL_SKIP_BEFORE=1)");
  } else {
    console.log("Running baseline tests (before)...");
    beforeResult = await runTests("repository_before", { timeoutMs: 15_000 });
  }

  // 2. Run Tests against "repository_after" (Refactor)
  console.log("Running refactor tests (after)...");
  const afterResult = await runTests("repository_after", { timeoutMs: 60_000 });

  const endTime = new Date();
  const endTimeIso = endTime.toISOString();
  const durationSeconds = (endTime - startTime) / 1000;

  // 3. Generate Comparison Summary
  let improvementSummary = "No improvement detected.";
  if (!beforeResult.passed && afterResult.passed) {
    improvementSummary = "Refactor fixed failing tests and met distributed requirements.";
  } else if (beforeResult.passed && afterResult.passed) {
    improvementSummary = "Tests passed in both states (Verify baseline expectation).";
  } else if (!afterResult.passed) {
    improvementSummary = "Refactored code failed to pass requirements.";
  }

  // 4. Construct the Final Report Object
  const report = {
    run_id: runId,
    started_at: startTimeIso,
    finished_at: endTimeIso,
    duration_seconds: durationSeconds,
    environment: getEnvironmentInfo(),
    before: {
      tests: {
        passed: beforeResult.passed,
        return_code: beforeResult.return_code,
        duration_seconds: beforeResult.duration_seconds,
        totals: beforeResult.totals,
        output: (beforeResult.output || "").substring(0, 500)
      },
      metrics: {} // Placeholders for future metrics (e.g. memory usage)
    },
    after: {
      tests: {
        passed: afterResult.passed,
        return_code: afterResult.return_code,
        duration_seconds: afterResult.duration_seconds,
        totals: afterResult.totals,
        output: (afterResult.output || "").substring(0, 500)
      },
      metrics: {}
    },
    comparison: {
      passed_gate: afterResult.passed,
      improvement_summary: improvementSummary
    },
    success: afterResult.passed,
    error: null
  };

  // Write the report to disk
  const reportPath = path.join(REPORTS_DIR, "report.json");
  fs.writeFileSync(reportPath, JSON.stringify(report, null, 2));

  console.log(`Evaluation complete. Success: ${report.success}`);
  console.log(`Report written to: ${reportPath}`);
  // Also print the report JSON so it is visible without bind mounts
  try {
    console.log("\n--- Evaluation Report (inline) ---\n");
    console.log(JSON.stringify(report, null, 2));
    console.log("\n--- End Report ---\n");
  } catch (e) {
    console.warn('Could not print report inline:', e.message);
  }

  // Exit with status code based on the 'After' result
  process.exit(report.success ? 0 : 1);
}

runEvaluation();
