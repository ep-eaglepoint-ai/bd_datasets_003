diff --git a/repository_before/.gitkeep b/repository_before/.gitkeep
deleted file mode 100644
index e69de29..0000000
diff --git a/repository_after/pkg/api/client.go b/repository_after/pkg/api/client.go
new file mode 100644
index 0000000..4df364c
--- /dev/null
+++ b/repository_after/pkg/api/client.go
@@ -0,0 +1,81 @@
+package api
+
+import (
+	"context"
+	"errors"
+	"time"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+// Client provides a client interface to the Raft KV store
+type Client struct {
+	nodes   []*raft.Node
+	timeout time.Duration
+}
+
+// NewClient creates a new client
+func NewClient(nodes []*raft.Node) *Client {
+	return &Client{
+		nodes:   nodes,
+		timeout: 5 * time.Second,
+	}
+}
+
+// Set sets a key-value pair
+func (c *Client) Set(ctx context.Context, key, value string) error {
+	leader := c.findLeader()
+	if leader == nil {
+		return errors.New("no leader available")
+	}
+
+	cmd := raft.Command{
+		Type:  raft.CommandSet,
+		Key:   key,
+		Value: value,
+	}
+
+	_, err := leader.SubmitWithResult(ctx, cmd)
+	return err
+}
+
+// Get retrieves a value by key
+func (c *Client) Get(ctx context.Context, key string) (string, error) {
+	leader := c.findLeader()
+	if leader == nil {
+		return "", errors.New("no leader available")
+	}
+
+	return leader.Read(ctx, key)
+}
+
+// Delete removes a key
+func (c *Client) Delete(ctx context.Context, key string) error {
+	leader := c.findLeader()
+	if leader == nil {
+		return errors.New("no leader available")
+	}
+
+	cmd := raft.Command{
+		Type: raft.CommandDelete,
+		Key:  key,
+	}
+
+	_, err := leader.SubmitWithResult(ctx, cmd)
+	return err
+}
+
+// findLeader finds the current leader node
+func (c *Client) findLeader() *raft.Node {
+	for _, node := range c.nodes {
+		if node.IsLeader() {
+			return node
+		}
+	}
+	return nil
+}
+
+// SetTimeout sets the client timeout
+func (c *Client) SetTimeout(d time.Duration) {
+	c.timeout = d
+}
\ No newline at end of file
diff --git a/repository_after/pkg/kv/store.go b/repository_after/pkg/kv/store.go
new file mode 100644
index 0000000..3d4287c
--- /dev/null
+++ b/repository_after/pkg/kv/store.go
@@ -0,0 +1,103 @@
+package kv
+
+import (
+	"sync"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+// Store implements a simple in-memory key-value store
+type Store struct {
+	mu   sync.RWMutex
+	data map[string]string
+}
+
+// NewStore creates a new KV store
+func NewStore() *Store {
+	return &Store{
+		data: make(map[string]string),
+	}
+}
+
+// Apply applies a command to the state machine
+func (s *Store) Apply(cmd raft.Command) string {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	switch cmd.Type {
+	case raft.CommandSet:
+		s.data[cmd.Key] = cmd.Value
+		return cmd.Value
+	case raft.CommandGet:
+		return s.data[cmd.Key]
+	case raft.CommandDelete:
+		delete(s.data, cmd.Key)
+		return ""
+	case raft.CommandNoop:
+		return ""
+	case raft.CommandAddNode, raft.CommandRemoveNode:
+		// Membership changes are handled by the Raft node
+		return ""
+	default:
+		return ""
+	}
+}
+
+// Get retrieves a value from the store
+func (s *Store) Get(key string) (string, bool) {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+	value, ok := s.data[key]
+	return value, ok
+}
+
+// Set stores a key-value pair
+func (s *Store) Set(key, value string) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+	s.data[key] = value
+}
+
+// Delete removes a key from the store
+func (s *Store) Delete(key string) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+	delete(s.data, key)
+}
+
+// GetSnapshot returns a copy of the current state
+func (s *Store) GetSnapshot() map[string]string {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+
+	snapshot := make(map[string]string)
+	for k, v := range s.data {
+		snapshot[k] = v
+	}
+	return snapshot
+}
+
+// RestoreSnapshot restores state from a snapshot
+func (s *Store) RestoreSnapshot(data map[string]string) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	s.data = make(map[string]string)
+	for k, v := range data {
+		s.data[k] = v
+	}
+}
+
+// Size returns the number of keys in the store
+func (s *Store) Size() int {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+	return len(s.data)
+}
+
+// Clear removes all keys from the store
+func (s *Store) Clear() {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+	s.data = make(map[string]string)
+}
\ No newline at end of file
diff --git a/repository_after/pkg/raft/errors.go b/repository_after/pkg/raft/errors.go
new file mode 100644
index 0000000..7282362
--- /dev/null
+++ b/repository_after/pkg/raft/errors.go
@@ -0,0 +1,11 @@
+package raft
+
+import "errors"
+
+var (
+	ErrNotLeader      = errors.New("not the leader")
+	ErrTimeout        = errors.New("operation timed out")
+	ErrNodeNotFound   = errors.New("node not found")
+	ErrLogCompacted   = errors.New("log has been compacted")
+	ErrSnapshotFailed = errors.New("snapshot operation failed")
+)
\ No newline at end of file
diff --git a/repository_after/pkg/raft/node.go b/repository_after/pkg/raft/node.go
new file mode 100644
index 0000000..c732bf0
--- /dev/null
+++ b/repository_after/pkg/raft/node.go
@@ -0,0 +1,1238 @@
+package raft
+
+import (
+	"context"
+	"log"
+	"math/rand"
+	"sync"
+	"time"
+)
+
+// Node represents a Raft node
+type Node struct {
+	mu sync.RWMutex
+
+	// Node identity and configuration
+	id     string
+	config NodeConfig
+
+	// Persistent state
+	currentTerm uint64
+	votedFor    string
+	log         []LogEntry
+
+	// Volatile state
+	state       NodeState
+	commitIndex uint64
+	lastApplied uint64
+
+	// Leader state
+	nextIndex  map[string]uint64
+	matchIndex map[string]uint64
+
+	// Cluster configuration
+	cluster *ClusterConfig
+
+	// Channels
+	applyCh         chan ApplyMsg
+	stopCh          chan struct{}
+	electionResetCh chan struct{}
+
+	// Pending operations
+	pendingCommands map[uint64]*PendingCommand
+	pendingReads    []*ReadIndex
+	readMu          sync.Mutex
+
+	// Components
+	transport Transport
+	wal       WALInterface
+	stateMachine StateMachineInterface
+
+	// Snapshot state
+	snapshot          *Snapshot
+	snapshotThreshold uint64
+
+	// Leader tracking
+	leaderID        string
+	lastHeartbeat   time.Time
+	electionTimeout time.Duration
+
+	// Read index tracking
+	readIndex     uint64
+	readIndexTerm uint64
+}
+
+// WALInterface defines the interface for write-ahead log
+type WALInterface interface {
+	Save(state *PersistentState) error
+	Load() (*PersistentState, error)
+	SaveSnapshot(snapshot *Snapshot) error
+	LoadSnapshot() (*Snapshot, error)
+	Close() error
+}
+
+// StateMachineInterface defines the interface for the state machine
+type StateMachineInterface interface {
+	Apply(cmd Command) string
+	Get(key string) (string, bool)
+	GetSnapshot() map[string]string
+	RestoreSnapshot(data map[string]string)
+}
+
+// NewNode creates a new Raft node
+func NewNode(config NodeConfig, transport Transport, wal WALInterface, stateMachine StateMachineInterface) *Node {
+	n := &Node{
+		id:              config.ID,
+		config:          config,
+		currentTerm:     0,
+		votedFor:        "",
+		log:             make([]LogEntry, 0),
+		state:           Follower,
+		commitIndex:     0,
+		lastApplied:     0,
+		nextIndex:       make(map[string]uint64),
+		matchIndex:      make(map[string]uint64),
+		cluster:         NewClusterConfig(),
+		applyCh:         make(chan ApplyMsg, 100),
+		stopCh:          make(chan struct{}),
+		electionResetCh: make(chan struct{}, 1),
+		pendingCommands: make(map[uint64]*PendingCommand),
+		pendingReads:    make([]*ReadIndex, 0),
+		transport:       transport,
+		wal:             wal,
+		stateMachine:    stateMachine,
+		snapshotThreshold: config.SnapshotThreshold,
+	}
+
+	// Add self to cluster
+	n.cluster.AddNode(config.ID)
+
+	// Add initial peers
+	for _, peer := range config.Peers {
+		n.cluster.AddNode(peer)
+	}
+
+	// Initialize log with a dummy entry at index 0
+	n.log = append(n.log, LogEntry{Index: 0, Term: 0, Command: Command{Type: CommandNoop}})
+
+	return n
+}
+
+// Start begins the Raft node operation
+func (n *Node) Start() error {
+	// Restore state from WAL
+	if err := n.restore(); err != nil {
+		log.Printf("Node %s: Failed to restore state: %v", n.id, err)
+	}
+
+	// Start the main loop
+	go n.run()
+
+	// Start the apply loop
+	go n.applyLoop()
+
+	return nil
+}
+
+// Stop stops the Raft node
+func (n *Node) Stop() {
+	close(n.stopCh)
+	if n.wal != nil {
+		n.wal.Close()
+	}
+}
+
+// run is the main event loop
+func (n *Node) run() {
+	for {
+		select {
+		case <-n.stopCh:
+			return
+		default:
+		}
+
+		n.mu.RLock()
+		state := n.state
+		n.mu.RUnlock()
+
+		switch state {
+		case Follower:
+			n.runFollower()
+		case Candidate:
+			n.runCandidate()
+		case Leader:
+			n.runLeader()
+		}
+	}
+}
+
+// runFollower runs the follower state
+func (n *Node) runFollower() {
+	timeout := n.randomElectionTimeout()
+	timer := time.NewTimer(timeout)
+	defer timer.Stop()
+
+	for {
+		select {
+		case <-n.stopCh:
+			return
+		case <-timer.C:
+			// Election timeout - become candidate
+			n.mu.Lock()
+			if n.state == Follower {
+				n.becomeCandidate()
+			}
+			n.mu.Unlock()
+			return
+		case <-n.electionResetCh:
+			// Reset election timer
+			if !timer.Stop() {
+				select {
+				case <-timer.C:
+				default:
+				}
+			}
+			timer.Reset(n.randomElectionTimeout())
+		}
+	}
+}
+
+// runCandidate runs the candidate state
+func (n *Node) runCandidate() {
+	n.mu.Lock()
+	n.currentTerm++
+	n.votedFor = n.id
+	currentTerm := n.currentTerm
+	lastLogIndex := n.getLastLogIndex()
+	lastLogTerm := n.getLastLogTerm()
+	n.persist()
+	n.mu.Unlock()
+
+	log.Printf("Node %s: Starting election for term %d", n.id, currentTerm)
+
+	// Vote for self
+	votesReceived := 1
+	voteMu := sync.Mutex{}
+
+	// Request votes from all peers
+	peers := n.cluster.GetNodes()
+	for _, peer := range peers {
+		if peer == n.id {
+			continue
+		}
+
+		go func(peer string) {
+			args := &RequestVoteArgs{
+				Term:         currentTerm,
+				CandidateID:  n.id,
+				LastLogIndex: lastLogIndex,
+				LastLogTerm:  lastLogTerm,
+			}
+
+			reply, err := n.transport.RequestVote(peer, args)
+			if err != nil {
+				return
+			}
+
+			n.mu.Lock()
+			defer n.mu.Unlock()
+
+			if reply.Term > n.currentTerm {
+				n.becomeFollower(reply.Term)
+				return
+			}
+
+			if n.state != Candidate || n.currentTerm != currentTerm {
+				return
+			}
+
+			if reply.VoteGranted {
+				voteMu.Lock()
+				votesReceived++
+				votes := votesReceived
+				voteMu.Unlock()
+
+				majority := n.cluster.Size()/2 + 1
+				if votes >= majority {
+					n.becomeLeader()
+				}
+			}
+		}(peer)
+	}
+
+	// Wait for election timeout
+	timeout := n.randomElectionTimeout()
+	timer := time.NewTimer(timeout)
+	defer timer.Stop()
+
+	select {
+	case <-n.stopCh:
+		return
+	case <-timer.C:
+		n.mu.Lock()
+		if n.state == Candidate {
+			// Election timeout - start new election
+			log.Printf("Node %s: Election timeout, starting new election", n.id)
+		}
+		n.mu.Unlock()
+	case <-n.electionResetCh:
+		// Received heartbeat from leader
+	}
+}
+
+// runLeader runs the leader state
+func (n *Node) runLeader() {
+	// Send initial heartbeats
+	n.sendHeartbeats()
+
+	ticker := time.NewTicker(n.config.HeartbeatInterval)
+	defer ticker.Stop()
+
+	for {
+		select {
+		case <-n.stopCh:
+			return
+		case <-ticker.C:
+			n.mu.RLock()
+			isLeader := n.state == Leader
+			n.mu.RUnlock()
+
+			if !isLeader {
+				return
+			}
+
+			n.sendHeartbeats()
+			n.advanceCommitIndex()
+			n.checkReadIndices()
+		case <-n.electionResetCh:
+			// Ignore in leader state
+		}
+	}
+}
+
+// sendHeartbeats sends heartbeats to all peers
+func (n *Node) sendHeartbeats() {
+	n.mu.RLock()
+	if n.state != Leader {
+		n.mu.RUnlock()
+		return
+	}
+	currentTerm := n.currentTerm
+	commitIndex := n.commitIndex
+	n.mu.RUnlock()
+
+	peers := n.cluster.GetNodes()
+	for _, peer := range peers {
+		if peer == n.id {
+			continue
+		}
+
+		go n.sendAppendEntries(peer, currentTerm, commitIndex)
+	}
+}
+
+// sendAppendEntries sends AppendEntries RPC to a peer
+// sendAppendEntries sends AppendEntries RPC to a peer
+func (n *Node) sendAppendEntries(peer string, term uint64, leaderCommit uint64) {
+	n.mu.RLock()
+	if n.state != Leader || n.currentTerm != term {
+		n.mu.RUnlock()
+		return
+	}
+
+	nextIdx := n.nextIndex[peer]
+	if nextIdx == 0 {
+		nextIdx = n.getLastLogIndex() + 1
+	}
+
+	prevLogIndex := nextIdx - 1
+	prevLogTerm := uint64(0)
+	if prevLogIndex > 0 && int(prevLogIndex) < len(n.log) {
+		prevLogTerm = n.log[prevLogIndex].Term
+	}
+
+	entries := make([]LogEntry, 0)
+	if int(nextIdx) < len(n.log) {
+		entries = append(entries, n.log[nextIdx:]...)
+	}
+
+	args := &AppendEntriesArgs{
+		Term:         term,
+		LeaderID:     n.id,
+		PrevLogIndex: prevLogIndex,
+		PrevLogTerm:  prevLogTerm,
+		Entries:      entries,
+		LeaderCommit: leaderCommit,
+	}
+	n.mu.RUnlock()
+
+	reply, err := n.transport.AppendEntries(peer, args)
+	if err != nil {
+		return
+	}
+
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	if reply.Term > n.currentTerm {
+		n.becomeFollower(reply.Term)
+		return
+	}
+
+	if n.state != Leader || n.currentTerm != term {
+		return
+	}
+
+	if reply.Success {
+		// Update nextIndex and matchIndex
+		newNextIndex := nextIdx + uint64(len(entries))
+		if newNextIndex > n.nextIndex[peer] {
+			n.nextIndex[peer] = newNextIndex
+		}
+		newMatchIndex := newNextIndex - 1
+		if newMatchIndex > n.matchIndex[peer] {
+			n.matchIndex[peer] = newMatchIndex
+		}
+
+		// Try to advance commit index immediately after successful replication
+		n.tryAdvanceCommitIndex()
+	} else {
+		// Decrement nextIndex and retry
+		if reply.ConflictTerm > 0 {
+			lastIndex := uint64(0)
+			for i := len(n.log) - 1; i >= 0; i-- {
+				if n.log[i].Term == reply.ConflictTerm {
+					lastIndex = uint64(i)
+					break
+				}
+			}
+			if lastIndex > 0 {
+				n.nextIndex[peer] = lastIndex + 1
+			} else {
+				n.nextIndex[peer] = reply.ConflictIndex
+			}
+		} else if reply.ConflictIndex > 0 {
+			n.nextIndex[peer] = reply.ConflictIndex
+		} else if n.nextIndex[peer] > 1 {
+			n.nextIndex[peer]--
+		}
+	}
+}
+
+// tryAdvanceCommitIndex tries to advance the commit index (must be called with lock held)
+func (n *Node) tryAdvanceCommitIndex() {
+	if n.state != Leader {
+		return
+	}
+
+	// Find the highest index replicated on a majority
+	for idx := n.commitIndex + 1; idx <= n.getLastLogIndex(); idx++ {
+		if idx == 0 || int(idx) >= len(n.log) {
+			continue
+		}
+
+		// Only commit entries from current term
+		if n.log[idx].Term != n.currentTerm {
+			continue
+		}
+
+		count := 1 // Count self
+		for _, peer := range n.cluster.GetNodes() {
+			if peer == n.id {
+				continue
+			}
+			if n.matchIndex[peer] >= idx {
+				count++
+			}
+		}
+
+		majority := n.cluster.Size()/2 + 1
+		if count >= majority {
+			n.commitIndex = idx
+			log.Printf("Node %s: Committed index %d", n.id, idx)
+
+			// Notify pending commands
+			if pending, ok := n.pendingCommands[idx]; ok {
+				result := CommitResult{
+					Index: idx,
+					Term:  n.log[idx].Term,
+				}
+				select {
+				case pending.ResultCh <- result:
+				default:
+				}
+				delete(n.pendingCommands, idx)
+			}
+		}
+	}
+}
+
+
+
+// advanceCommitIndex advances the commit index if possible
+func (n *Node) advanceCommitIndex() {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	if n.state != Leader {
+		return
+	}
+
+	// Find the highest index replicated on a majority
+	for idx := n.commitIndex + 1; idx <= n.getLastLogIndex(); idx++ {
+		if idx == 0 || int(idx) >= len(n.log) {
+			continue
+		}
+
+		// Only commit entries from current term
+		if n.log[idx].Term != n.currentTerm {
+			continue
+		}
+
+		count := 1 // Count self
+		for _, peer := range n.cluster.GetNodes() {
+			if peer == n.id {
+				continue
+			}
+			if n.matchIndex[peer] >= idx {
+				count++
+			}
+		}
+
+		majority := n.cluster.Size()/2 + 1
+		if count >= majority {
+			n.commitIndex = idx
+			log.Printf("Node %s: Committed index %d", n.id, idx)
+
+			// Notify pending commands
+			if pending, ok := n.pendingCommands[idx]; ok {
+				result := CommitResult{
+					Index: idx,
+					Term:  n.log[idx].Term,
+				}
+				select {
+				case pending.ResultCh <- result:
+				default:
+				}
+				delete(n.pendingCommands, idx)
+			}
+		}
+	}
+}
+
+// HandleRequestVote handles incoming RequestVote RPC
+func (n *Node) HandleRequestVote(args *RequestVoteArgs) *RequestVoteReply {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	reply := &RequestVoteReply{
+		Term:        n.currentTerm,
+		VoteGranted: false,
+	}
+
+	if args.Term < n.currentTerm {
+		return reply
+	}
+
+	if args.Term > n.currentTerm {
+		n.becomeFollower(args.Term)
+	}
+
+	reply.Term = n.currentTerm
+
+	// Check if we can vote for this candidate
+	if (n.votedFor == "" || n.votedFor == args.CandidateID) && n.isLogUpToDate(args.LastLogIndex, args.LastLogTerm) {
+		n.votedFor = args.CandidateID
+		reply.VoteGranted = true
+		n.persist()
+		n.resetElectionTimer()
+		log.Printf("Node %s: Granted vote to %s for term %d", n.id, args.CandidateID, args.Term)
+	}
+
+	return reply
+}
+
+// HandleAppendEntries handles incoming AppendEntries RPC
+func (n *Node) HandleAppendEntries(args *AppendEntriesArgs) *AppendEntriesReply {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	reply := &AppendEntriesReply{
+		Term:    n.currentTerm,
+		Success: false,
+	}
+
+	if args.Term < n.currentTerm {
+		return reply
+	}
+
+	if args.Term > n.currentTerm {
+		n.becomeFollower(args.Term)
+	} else if n.state == Candidate {
+		n.becomeFollower(args.Term)
+	}
+
+	n.leaderID = args.LeaderID
+	n.lastHeartbeat = time.Now()
+	n.resetElectionTimer()
+
+	reply.Term = n.currentTerm
+
+	// Check if log contains entry at prevLogIndex with prevLogTerm
+	if args.PrevLogIndex > 0 {
+		if int(args.PrevLogIndex) >= len(n.log) {
+			reply.ConflictIndex = uint64(len(n.log))
+			reply.ConflictTerm = 0
+			return reply
+		}
+
+		if n.log[args.PrevLogIndex].Term != args.PrevLogTerm {
+			conflictTerm := n.log[args.PrevLogIndex].Term
+			reply.ConflictTerm = conflictTerm
+
+			// Find first index with conflictTerm
+			for i := args.PrevLogIndex; i > 0; i-- {
+				if n.log[i].Term != conflictTerm {
+					reply.ConflictIndex = i + 1
+					break
+				}
+				if i == 1 {
+					reply.ConflictIndex = 1
+				}
+			}
+			return reply
+		}
+	}
+
+	// Append new entries
+	for i, entry := range args.Entries {
+		idx := args.PrevLogIndex + 1 + uint64(i)
+		if int(idx) < len(n.log) {
+			if n.log[idx].Term != entry.Term {
+				// Conflict - truncate and append
+				n.log = n.log[:idx]
+				n.log = append(n.log, entry)
+			}
+		} else {
+			n.log = append(n.log, entry)
+		}
+	}
+
+	if len(args.Entries) > 0 {
+		n.persist()
+	}
+
+	// Update commit index
+	if args.LeaderCommit > n.commitIndex {
+		lastNewIndex := args.PrevLogIndex + uint64(len(args.Entries))
+		if args.LeaderCommit < lastNewIndex {
+			n.commitIndex = args.LeaderCommit
+		} else {
+			n.commitIndex = lastNewIndex
+		}
+	}
+
+	reply.Success = true
+	return reply
+}
+
+// HandleInstallSnapshot handles incoming InstallSnapshot RPC
+func (n *Node) HandleInstallSnapshot(args *InstallSnapshotArgs) *InstallSnapshotReply {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	reply := &InstallSnapshotReply{
+		Term: n.currentTerm,
+	}
+
+	if args.Term < n.currentTerm {
+		return reply
+	}
+
+	if args.Term > n.currentTerm {
+		n.becomeFollower(args.Term)
+	}
+
+	n.leaderID = args.LeaderID
+	n.resetElectionTimer()
+
+	// If existing log entry has same index and term as snapshot's last included entry, retain log entries following it
+	if args.LastIncludedIndex < uint64(len(n.log)) && n.log[args.LastIncludedIndex].Term == args.LastIncludedTerm {
+		n.log = n.log[args.LastIncludedIndex:]
+	} else {
+		// Discard entire log
+		n.log = []LogEntry{{Index: args.LastIncludedIndex, Term: args.LastIncludedTerm, Command: Command{Type: CommandNoop}}}
+	}
+
+	// Update commit and last applied indices
+	if args.LastIncludedIndex > n.commitIndex {
+		n.commitIndex = args.LastIncludedIndex
+	}
+	if args.LastIncludedIndex > n.lastApplied {
+		n.lastApplied = args.LastIncludedIndex
+	}
+
+	// Send snapshot to state machine
+	n.applyCh <- ApplyMsg{
+		SnapshotValid: true,
+		Snapshot:      args.Data,
+		SnapshotTerm:  args.LastIncludedTerm,
+		SnapshotIndex: args.LastIncludedIndex,
+	}
+
+	n.persist()
+	return reply
+}
+
+// Submit submits a command to the Raft cluster
+func (n *Node) Submit(cmd Command) (uint64, uint64, bool) {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	if n.state != Leader {
+		return 0, 0, false
+	}
+
+	entry := LogEntry{
+		Index:   n.getLastLogIndex() + 1,
+		Term:    n.currentTerm,
+		Command: cmd,
+	}
+
+	n.log = append(n.log, entry)
+	n.persist()
+
+	log.Printf("Node %s: Appended entry %d with command %v", n.id, entry.Index, cmd)
+
+	return entry.Index, entry.Term, true
+}
+
+// SubmitWithResult submits a command and waits for it to be committed
+// SubmitWithResult submits a command and waits for it to be committed
+func (n *Node) SubmitWithResult(ctx context.Context, cmd Command) (CommitResult, error) {
+	index, term, isLeader := n.Submit(cmd)
+	if !isLeader {
+		return CommitResult{}, ErrNotLeader
+	}
+
+	resultCh := make(chan CommitResult, 1)
+	pending := &PendingCommand{
+		Index:    index,
+		Term:     term,
+		ResultCh: resultCh,
+	}
+
+	n.mu.Lock()
+	n.pendingCommands[index] = pending
+	n.mu.Unlock()
+
+	select {
+	case result := <-resultCh:
+		if result.Error != nil {
+			return result, result.Error
+		}
+		return result, nil
+	case <-ctx.Done():
+		n.mu.Lock()
+		delete(n.pendingCommands, index)
+		n.mu.Unlock()
+		return CommitResult{}, ctx.Err()
+	}
+}
+
+// Read performs a linearizable read
+func (n *Node) Read(ctx context.Context, key string) (string, error) {
+	n.mu.Lock()
+	if n.state != Leader {
+		n.mu.Unlock()
+		return "", ErrNotLeader
+	}
+
+	// Record current commit index as read index
+	readIndex := n.commitIndex
+	currentTerm := n.currentTerm
+	n.mu.Unlock()
+
+	// Send heartbeats to confirm leadership
+	confirmCh := make(chan bool, 1)
+	go func() {
+		confirmed := n.confirmLeadership(currentTerm)
+		confirmCh <- confirmed
+	}()
+
+	select {
+	case confirmed := <-confirmCh:
+		if !confirmed {
+			return "", ErrNotLeader
+		}
+	case <-ctx.Done():
+		return "", ctx.Err()
+	}
+
+	// Wait for apply index to catch up
+	for {
+		n.mu.RLock()
+		lastApplied := n.lastApplied
+		n.mu.RUnlock()
+
+		if lastApplied >= readIndex {
+			break
+		}
+
+		select {
+		case <-ctx.Done():
+			return "", ctx.Err()
+		case <-time.After(10 * time.Millisecond):
+		}
+	}
+
+	// Read from state machine
+	value, _ := n.stateMachine.Get(key)
+	return value, nil
+}
+
+// confirmLeadership confirms that this node is still the leader
+func (n *Node) confirmLeadership(term uint64) bool {
+	n.mu.RLock()
+	if n.state != Leader || n.currentTerm != term {
+		n.mu.RUnlock()
+		return false
+	}
+	n.mu.RUnlock()
+
+	// Send heartbeats and wait for majority acknowledgment
+	peers := n.cluster.GetNodes()
+	ackCount := 1 // Count self
+	ackMu := sync.Mutex{}
+	done := make(chan struct{})
+
+	for _, peer := range peers {
+		if peer == n.id {
+			continue
+		}
+
+		go func(peer string) {
+			n.mu.RLock()
+			args := &AppendEntriesArgs{
+				Term:         n.currentTerm,
+				LeaderID:     n.id,
+				PrevLogIndex: n.getLastLogIndex(),
+				PrevLogTerm:  n.getLastLogTerm(),
+				Entries:      nil,
+				LeaderCommit: n.commitIndex,
+			}
+			n.mu.RUnlock()
+
+			reply, err := n.transport.AppendEntries(peer, args)
+			if err != nil {
+				return
+			}
+
+			if reply.Success {
+				ackMu.Lock()
+				ackCount++
+				if ackCount >= n.cluster.Size()/2+1 {
+					select {
+					case done <- struct{}{}:
+					default:
+					}
+				}
+				ackMu.Unlock()
+			}
+		}(peer)
+	}
+
+	select {
+	case <-done:
+		return true
+	case <-time.After(n.config.HeartbeatInterval * 2):
+		ackMu.Lock()
+		result := ackCount >= n.cluster.Size()/2+1
+		ackMu.Unlock()
+		return result
+	}
+}
+
+// checkReadIndices checks if any pending reads can be satisfied
+func (n *Node) checkReadIndices() {
+	n.readMu.Lock()
+	defer n.readMu.Unlock()
+
+	n.mu.RLock()
+	lastApplied := n.lastApplied
+	n.mu.RUnlock()
+
+	remaining := make([]*ReadIndex, 0)
+	for _, read := range n.pendingReads {
+		if lastApplied >= read.Index {
+			result := CommitResult{Index: read.Index}
+			select {
+			case read.ResultCh <- result:
+			default:
+			}
+		} else {
+			remaining = append(remaining, read)
+		}
+	}
+	n.pendingReads = remaining
+}
+
+// AddNode adds a new node to the cluster
+func (n *Node) AddNode(nodeID string) error {
+	n.mu.Lock()
+	if n.state != Leader {
+		n.mu.Unlock()
+		return ErrNotLeader
+	}
+	n.mu.Unlock()
+
+	cmd := Command{
+		Type:  CommandAddNode,
+		Key:   nodeID,
+		Value: "",
+	}
+
+	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
+	defer cancel()
+
+	_, err := n.SubmitWithResult(ctx, cmd)
+	if err != nil {
+		return err
+	}
+
+	n.mu.Lock()
+	n.cluster.AddNode(nodeID)
+	n.nextIndex[nodeID] = n.getLastLogIndex() + 1
+	n.matchIndex[nodeID] = 0
+	n.mu.Unlock()
+
+	return nil
+}
+
+// RemoveNode removes a node from the cluster
+func (n *Node) RemoveNode(nodeID string) error {
+	n.mu.Lock()
+	if n.state != Leader {
+		n.mu.Unlock()
+		return ErrNotLeader
+	}
+	n.mu.Unlock()
+
+	cmd := Command{
+		Type:  CommandRemoveNode,
+		Key:   nodeID,
+		Value: "",
+	}
+
+	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
+	defer cancel()
+
+	_, err := n.SubmitWithResult(ctx, cmd)
+	if err != nil {
+		return err
+	}
+
+	n.mu.Lock()
+	n.cluster.RemoveNode(nodeID)
+	delete(n.nextIndex, nodeID)
+	delete(n.matchIndex, nodeID)
+	n.mu.Unlock()
+
+	return nil
+}
+
+// applyLoop applies committed entries to the state machine
+func (n *Node) applyLoop() {
+	for {
+		select {
+		case <-n.stopCh:
+			return
+		default:
+		}
+
+		n.mu.Lock()
+		commitIndex := n.commitIndex
+		lastApplied := n.lastApplied
+		n.mu.Unlock()
+
+		if lastApplied < commitIndex {
+			for i := lastApplied + 1; i <= commitIndex; i++ {
+				n.mu.RLock()
+				if int(i) >= len(n.log) {
+					n.mu.RUnlock()
+					break
+				}
+				entry := n.log[i]
+				n.mu.RUnlock()
+
+				// Apply to state machine
+				result := n.stateMachine.Apply(entry.Command)
+
+				n.applyCh <- ApplyMsg{
+					CommandValid: true,
+					Command:      entry.Command,
+					CommandIndex: entry.Index,
+					CommandTerm:  entry.Term,
+				}
+
+				n.mu.Lock()
+				n.lastApplied = i
+
+				// Notify pending command if this is the leader
+				if n.state == Leader {
+					if pending, ok := n.pendingCommands[i]; ok {
+						commitResult := CommitResult{
+							Index: i,
+							Term:  entry.Term,
+							Value: result,
+						}
+						select {
+						case pending.ResultCh <- commitResult:
+						default:
+						}
+						delete(n.pendingCommands, i)
+					}
+				}
+				n.mu.Unlock()
+
+				// Check if snapshot is needed
+				n.maybeSnapshot()
+			}
+		}
+
+		time.Sleep(10 * time.Millisecond)
+	}
+}
+
+// maybeSnapshot creates a snapshot if needed
+func (n *Node) maybeSnapshot() {
+	n.mu.RLock()
+	lastApplied := n.lastApplied
+	logLen := uint64(len(n.log))
+	n.mu.RUnlock()
+
+	if logLen > n.snapshotThreshold {
+		n.CreateSnapshot(lastApplied)
+	}
+}
+
+// CreateSnapshot creates a snapshot at the given index
+func (n *Node) CreateSnapshot(index uint64) error {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	if index <= 0 || int(index) >= len(n.log) {
+		return nil
+	}
+
+	snapshot := &Snapshot{
+		LastIncludedIndex: index,
+		LastIncludedTerm:  n.log[index].Term,
+		Data:              n.stateMachine.GetSnapshot(),
+	}
+
+	// Trim log
+	n.log = n.log[index:]
+	n.log[0] = LogEntry{
+		Index:   index,
+		Term:    snapshot.LastIncludedTerm,
+		Command: Command{Type: CommandNoop},
+	}
+
+	// Save snapshot
+	if n.wal != nil {
+		if err := n.wal.SaveSnapshot(snapshot); err != nil {
+			return err
+		}
+	}
+
+	n.snapshot = snapshot
+	log.Printf("Node %s: Created snapshot at index %d", n.id, index)
+
+	return nil
+}
+
+// Helper functions
+
+func (n *Node) becomeFollower(term uint64) {
+	log.Printf("Node %s: Becoming follower for term %d", n.id, term)
+	n.state = Follower
+	n.currentTerm = term
+	n.votedFor = ""
+	n.leaderID = ""
+	
+	// Fail all pending commands since we're no longer leader
+	for idx, pending := range n.pendingCommands {
+		result := CommitResult{
+			Index: idx,
+			Error: ErrNotLeader,
+		}
+		select {
+		case pending.ResultCh <- result:
+		default:
+		}
+	}
+	// Clear pending commands
+	n.pendingCommands = make(map[uint64]*PendingCommand)
+	
+	n.persist()
+}
+
+func (n *Node) becomeCandidate() {
+	log.Printf("Node %s: Becoming candidate for term %d", n.id, n.currentTerm+1)
+	n.state = Candidate
+}
+
+func (n *Node) becomeLeader() {
+	log.Printf("Node %s: Becoming leader for term %d", n.id, n.currentTerm)
+	n.state = Leader
+	n.leaderID = n.id
+
+	// Initialize nextIndex and matchIndex
+	lastLogIndex := n.getLastLogIndex()
+	for _, peer := range n.cluster.GetNodes() {
+		if peer != n.id {
+			n.nextIndex[peer] = lastLogIndex + 1
+			n.matchIndex[peer] = 0
+		}
+	}
+
+	// Append a no-op entry to commit entries from previous terms
+	noopEntry := LogEntry{
+		Index:   lastLogIndex + 1,
+		Term:    n.currentTerm,
+		Command: Command{Type: CommandNoop},
+	}
+	n.log = append(n.log, noopEntry)
+	n.persist()
+}
+
+func (n *Node) getLastLogIndex() uint64 {
+	if len(n.log) == 0 {
+		return 0
+	}
+	return uint64(len(n.log) - 1)
+}
+
+func (n *Node) getLastLogTerm() uint64 {
+	if len(n.log) == 0 {
+		return 0
+	}
+	return n.log[len(n.log)-1].Term
+}
+
+func (n *Node) isLogUpToDate(lastLogIndex, lastLogTerm uint64) bool {
+	myLastTerm := n.getLastLogTerm()
+	myLastIndex := n.getLastLogIndex()
+
+	if lastLogTerm != myLastTerm {
+		return lastLogTerm > myLastTerm
+	}
+	return lastLogIndex >= myLastIndex
+}
+
+func (n *Node) randomElectionTimeout() time.Duration {
+	min := int64(n.config.ElectionTimeoutMin)
+	max := int64(n.config.ElectionTimeoutMax)
+	return time.Duration(min + rand.Int63n(max-min))
+}
+
+func (n *Node) resetElectionTimer() {
+	select {
+	case n.electionResetCh <- struct{}{}:
+	default:
+	}
+}
+
+func (n *Node) persist() {
+	if n.wal == nil {
+		return
+	}
+
+	state := &PersistentState{
+		CurrentTerm: n.currentTerm,
+		VotedFor:    n.votedFor,
+		Log:         n.log,
+	}
+
+	if err := n.wal.Save(state); err != nil {
+		log.Printf("Node %s: Failed to persist state: %v", n.id, err)
+	}
+}
+
+func (n *Node) restore() error {
+	if n.wal == nil {
+		return nil
+	}
+
+	// Restore snapshot first
+	snapshot, err := n.wal.LoadSnapshot()
+	if err == nil && snapshot != nil {
+		n.snapshot = snapshot
+		n.stateMachine.RestoreSnapshot(snapshot.Data)
+		n.lastApplied = snapshot.LastIncludedIndex
+		n.commitIndex = snapshot.LastIncludedIndex
+	}
+
+	// Restore persistent state
+	state, err := n.wal.Load()
+	if err != nil {
+		return err
+	}
+
+	if state != nil {
+		n.currentTerm = state.CurrentTerm
+		n.votedFor = state.VotedFor
+		if len(state.Log) > 0 {
+			n.log = state.Log
+		}
+	}
+
+	return nil
+}
+
+// Getters for testing and inspection
+
+func (n *Node) GetState() (uint64, bool) {
+	n.mu.RLock()
+	defer n.mu.RUnlock()
+	return n.currentTerm, n.state == Leader
+}
+
+func (n *Node) GetLeaderID() string {
+	n.mu.RLock()
+	defer n.mu.RUnlock()
+	return n.leaderID
+}
+
+func (n *Node) GetID() string {
+	return n.id
+}
+
+func (n *Node) IsLeader() bool {
+	n.mu.RLock()
+	defer n.mu.RUnlock()
+	return n.state == Leader
+}
+
+func (n *Node) GetCommitIndex() uint64 {
+	n.mu.RLock()
+	defer n.mu.RUnlock()
+	return n.commitIndex
+}
+
+func (n *Node) GetLog() []LogEntry {
+	n.mu.RLock()
+	defer n.mu.RUnlock()
+	logCopy := make([]LogEntry, len(n.log))
+	copy(logCopy, n.log)
+	return logCopy
+}
+
+func (n *Node) GetApplyChan() <-chan ApplyMsg {
+	return n.applyCh
+}
+
+func (n *Node) GetClusterSize() int {
+	return n.cluster.Size()
+}
\ No newline at end of file
diff --git a/repository_after/pkg/raft/types.go b/repository_after/pkg/raft/types.go
new file mode 100644
index 0000000..9e7590f
--- /dev/null
+++ b/repository_after/pkg/raft/types.go
@@ -0,0 +1,241 @@
+package raft
+
+import (
+	"sync"
+	"time"
+)
+
+// NodeState represents the current state of a Raft node
+type NodeState int
+
+const (
+	Follower NodeState = iota
+	Candidate
+	Leader
+)
+
+func (s NodeState) String() string {
+	switch s {
+	case Follower:
+		return "Follower"
+	case Candidate:
+		return "Candidate"
+	case Leader:
+		return "Leader"
+	default:
+		return "Unknown"
+	}
+}
+
+// LogEntry represents a single entry in the Raft log
+type LogEntry struct {
+	Index   uint64
+	Term    uint64
+	Command Command
+}
+
+// Command represents a command to be applied to the state machine
+type Command struct {
+	Type  CommandType
+	Key   string
+	Value string
+}
+
+// CommandType represents the type of command
+type CommandType int
+
+const (
+	CommandSet CommandType = iota
+	CommandGet
+	CommandDelete
+	CommandNoop
+	CommandAddNode
+	CommandRemoveNode
+)
+
+// PersistentState represents state that must be persisted to disk
+type PersistentState struct {
+	CurrentTerm uint64
+	VotedFor    string
+	Log         []LogEntry
+}
+
+// NodeConfig holds the configuration for a Raft node
+type NodeConfig struct {
+	ID                 string
+	Peers              []string
+	ElectionTimeoutMin time.Duration
+	ElectionTimeoutMax time.Duration
+	HeartbeatInterval  time.Duration
+	WALPath            string
+	SnapshotThreshold  uint64
+}
+
+// DefaultConfig returns a default configuration
+func DefaultConfig(id string, peers []string) NodeConfig {
+	return NodeConfig{
+		ID:                 id,
+		Peers:              peers,
+		ElectionTimeoutMin: 150 * time.Millisecond,
+		ElectionTimeoutMax: 300 * time.Millisecond,
+		HeartbeatInterval:  50 * time.Millisecond,
+		WALPath:            "/tmp/raft-wal-" + id,
+		SnapshotThreshold:  1000,
+	}
+}
+
+// Snapshot represents a snapshot of the state machine
+type Snapshot struct {
+	LastIncludedIndex uint64
+	LastIncludedTerm  uint64
+	Data              map[string]string
+}
+
+// RequestVoteArgs represents arguments for RequestVote RPC
+type RequestVoteArgs struct {
+	Term         uint64
+	CandidateID  string
+	LastLogIndex uint64
+	LastLogTerm  uint64
+}
+
+// RequestVoteReply represents reply for RequestVote RPC
+type RequestVoteReply struct {
+	Term        uint64
+	VoteGranted bool
+}
+
+// AppendEntriesArgs represents arguments for AppendEntries RPC
+type AppendEntriesArgs struct {
+	Term         uint64
+	LeaderID     string
+	PrevLogIndex uint64
+	PrevLogTerm  uint64
+	Entries      []LogEntry
+	LeaderCommit uint64
+}
+
+// AppendEntriesReply represents reply for AppendEntries RPC
+type AppendEntriesReply struct {
+	Term          uint64
+	Success       bool
+	ConflictIndex uint64
+	ConflictTerm  uint64
+}
+
+// InstallSnapshotArgs represents arguments for InstallSnapshot RPC
+type InstallSnapshotArgs struct {
+	Term              uint64
+	LeaderID          string
+	LastIncludedIndex uint64
+	LastIncludedTerm  uint64
+	Data              []byte
+}
+
+// InstallSnapshotReply represents reply for InstallSnapshot RPC
+type InstallSnapshotReply struct {
+	Term uint64
+}
+
+// ApplyMsg represents a message sent to the state machine
+type ApplyMsg struct {
+	CommandValid bool
+	Command      Command
+	CommandIndex uint64
+	CommandTerm  uint64
+
+	SnapshotValid bool
+	Snapshot      []byte
+	SnapshotTerm  uint64
+	SnapshotIndex uint64
+}
+
+// Transport defines the interface for node-to-node communication
+type Transport interface {
+	RequestVote(target string, args *RequestVoteArgs) (*RequestVoteReply, error)
+	AppendEntries(target string, args *AppendEntriesArgs) (*AppendEntriesReply, error)
+	InstallSnapshot(target string, args *InstallSnapshotArgs) (*InstallSnapshotReply, error)
+}
+
+// CommitResult represents the result of committing a command
+type CommitResult struct {
+	Index uint64
+	Term  uint64
+	Value string
+	Error error
+}
+
+// PendingCommand represents a command waiting to be committed
+type PendingCommand struct {
+	Index    uint64
+	Term     uint64
+	ResultCh chan CommitResult
+}
+
+// ReadIndex represents a pending read operation
+type ReadIndex struct {
+	Index    uint64
+	ResultCh chan CommitResult
+}
+
+// MembershipChange represents a cluster membership change
+type MembershipChange struct {
+	Type   MembershipChangeType
+	NodeID string
+}
+
+type MembershipChangeType int
+
+const (
+	AddNode MembershipChangeType = iota
+	RemoveNode
+)
+
+// ClusterConfig represents the current cluster configuration
+type ClusterConfig struct {
+	mu      sync.RWMutex
+	Nodes   map[string]bool
+	Version uint64
+}
+
+func NewClusterConfig() *ClusterConfig {
+	return &ClusterConfig{
+		Nodes: make(map[string]bool),
+	}
+}
+
+func (c *ClusterConfig) AddNode(nodeID string) {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	c.Nodes[nodeID] = true
+	c.Version++
+}
+
+func (c *ClusterConfig) RemoveNode(nodeID string) {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	delete(c.Nodes, nodeID)
+	c.Version++
+}
+
+func (c *ClusterConfig) GetNodes() []string {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	nodes := make([]string, 0, len(c.Nodes))
+	for node := range c.Nodes {
+		nodes = append(nodes, node)
+	}
+	return nodes
+}
+
+func (c *ClusterConfig) HasNode(nodeID string) bool {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	return c.Nodes[nodeID]
+}
+
+func (c *ClusterConfig) Size() int {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	return len(c.Nodes)
+}
\ No newline at end of file
diff --git a/repository_after/pkg/rpc/transport.go b/repository_after/pkg/rpc/transport.go
new file mode 100644
index 0000000..796993b
--- /dev/null
+++ b/repository_after/pkg/rpc/transport.go
@@ -0,0 +1,164 @@
+package rpc
+
+import (
+	"sync"
+	"time"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+// LocalTransport implements in-memory transport for testing
+type LocalTransport struct {
+	mu       sync.RWMutex
+	nodes    map[string]*raft.Node
+	disabled map[string]map[string]bool // disabled[from][to] = true if connection is disabled
+	latency  time.Duration
+}
+
+// NewLocalTransport creates a new local transport for testing
+func NewLocalTransport() *LocalTransport {
+	return &LocalTransport{
+		nodes:    make(map[string]*raft.Node),
+		disabled: make(map[string]map[string]bool),
+	}
+}
+
+// Register registers a node with the transport
+func (t *LocalTransport) Register(id string, node *raft.Node) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	t.nodes[id] = node
+	t.disabled[id] = make(map[string]bool)
+}
+
+// SetLatency sets artificial latency for all RPCs
+func (t *LocalTransport) SetLatency(d time.Duration) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	t.latency = d
+}
+
+// Disconnect simulates network disconnect between two nodes
+func (t *LocalTransport) Disconnect(from, to string) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	if t.disabled[from] == nil {
+		t.disabled[from] = make(map[string]bool)
+	}
+	t.disabled[from][to] = true
+}
+
+// Connect restores network connection between two nodes
+func (t *LocalTransport) Connect(from, to string) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	if t.disabled[from] != nil {
+		delete(t.disabled[from], to)
+	}
+}
+
+// Partition isolates a node from the rest of the cluster
+func (t *LocalTransport) Partition(nodeID string) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+
+	for id := range t.nodes {
+		if id != nodeID {
+			if t.disabled[nodeID] == nil {
+				t.disabled[nodeID] = make(map[string]bool)
+			}
+			if t.disabled[id] == nil {
+				t.disabled[id] = make(map[string]bool)
+			}
+			t.disabled[nodeID][id] = true
+			t.disabled[id][nodeID] = true
+		}
+	}
+}
+
+// Heal restores all network connections for a node
+func (t *LocalTransport) Heal(nodeID string) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+
+	t.disabled[nodeID] = make(map[string]bool)
+	for id := range t.nodes {
+		if t.disabled[id] != nil {
+			delete(t.disabled[id], nodeID)
+		}
+	}
+}
+
+// HealAll restores all network connections
+func (t *LocalTransport) HealAll() {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	t.disabled = make(map[string]map[string]bool)
+}
+
+func (t *LocalTransport) isConnected(from, to string) bool {
+	if t.disabled[from] == nil {
+		return true
+	}
+	return !t.disabled[from][to]
+}
+
+// RequestVote sends a RequestVote RPC
+func (t *LocalTransport) RequestVote(target string, args *raft.RequestVoteArgs) (*raft.RequestVoteReply, error) {
+	t.mu.RLock()
+	node, ok := t.nodes[target]
+	connected := t.isConnected(args.CandidateID, target)
+	latency := t.latency
+	t.mu.RUnlock()
+
+	if !ok || !connected {
+		return nil, raft.ErrNodeNotFound
+	}
+
+	if latency > 0 {
+		time.Sleep(latency)
+	}
+
+	reply := node.HandleRequestVote(args)
+	return reply, nil
+}
+
+// AppendEntries sends an AppendEntries RPC
+func (t *LocalTransport) AppendEntries(target string, args *raft.AppendEntriesArgs) (*raft.AppendEntriesReply, error) {
+	t.mu.RLock()
+	node, ok := t.nodes[target]
+	connected := t.isConnected(args.LeaderID, target)
+	latency := t.latency
+	t.mu.RUnlock()
+
+	if !ok || !connected {
+		return nil, raft.ErrNodeNotFound
+	}
+
+	if latency > 0 {
+		time.Sleep(latency)
+	}
+
+	reply := node.HandleAppendEntries(args)
+	return reply, nil
+}
+
+// InstallSnapshot sends an InstallSnapshot RPC
+func (t *LocalTransport) InstallSnapshot(target string, args *raft.InstallSnapshotArgs) (*raft.InstallSnapshotReply, error) {
+	t.mu.RLock()
+	node, ok := t.nodes[target]
+	connected := t.isConnected(args.LeaderID, target)
+	latency := t.latency
+	t.mu.RUnlock()
+
+	if !ok || !connected {
+		return nil, raft.ErrNodeNotFound
+	}
+
+	if latency > 0 {
+		time.Sleep(latency)
+	}
+
+	reply := node.HandleInstallSnapshot(args)
+	return reply, nil
+}
\ No newline at end of file
diff --git a/repository_after/pkg/testing/cluster.go b/repository_after/pkg/testing/cluster.go
new file mode 100644
index 0000000..c45fe40
--- /dev/null
+++ b/repository_after/pkg/testing/cluster.go
@@ -0,0 +1,241 @@
+package testing
+
+import (
+	"context"
+	"fmt"
+	"math/rand"
+	"os"
+	"time"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/kv"
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/rpc"
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/wal"
+)
+
+// TestCluster represents a test cluster
+type TestCluster struct {
+	Nodes     []*raft.Node
+	Stores    []*kv.Store
+	Transport *rpc.LocalTransport
+	WALs      []*wal.WAL
+	walDirs   []string
+}
+
+// NewTestCluster creates a new test cluster
+func NewTestCluster(size int) (*TestCluster, error) {
+	transport := rpc.NewLocalTransport()
+
+	uniqueID := rand.Int63()
+
+	nodeIDs := make([]string, size)
+	for i := 0; i < size; i++ {
+		nodeIDs[i] = fmt.Sprintf("node-%d", i)
+	}
+
+	cluster := &TestCluster{
+		Nodes:     make([]*raft.Node, size),
+		Stores:    make([]*kv.Store, size),
+		Transport: transport,
+		WALs:      make([]*wal.WAL, size),
+		walDirs:   make([]string, size),
+	}
+
+	for i := 0; i < size; i++ {
+		peers := make([]string, 0, size-1)
+		for j := 0; j < size; j++ {
+			if i != j {
+				peers = append(peers, nodeIDs[j])
+			}
+		}
+
+		walDir := fmt.Sprintf("/tmp/raft-test-wal-%d-%d-%d", os.Getpid(), uniqueID, i)
+		cluster.walDirs[i] = walDir
+		os.RemoveAll(walDir)
+
+		walInstance, err := wal.NewWAL(walDir)
+		if err != nil {
+			cluster.Cleanup()
+			return nil, err
+		}
+		cluster.WALs[i] = walInstance
+
+		store := kv.NewStore()
+		cluster.Stores[i] = store
+
+		// Fast heartbeats, long election timeout for stability
+		config := raft.NodeConfig{
+			ID:                 nodeIDs[i],
+			Peers:              peers,
+			ElectionTimeoutMin: 500 * time.Millisecond,
+			ElectionTimeoutMax: 1000 * time.Millisecond,
+			HeartbeatInterval:  50 * time.Millisecond,
+			WALPath:            walDir,
+			SnapshotThreshold:  100,
+		}
+
+		node := raft.NewNode(config, transport, walInstance, store)
+		cluster.Nodes[i] = node
+		transport.Register(nodeIDs[i], node)
+	}
+
+	return cluster, nil
+}
+
+// Start starts all nodes in the cluster
+func (c *TestCluster) Start() error {
+	for _, node := range c.Nodes {
+		if err := node.Start(); err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+// Stop stops all nodes in the cluster
+func (c *TestCluster) Stop() {
+	for _, node := range c.Nodes {
+		if node != nil {
+			node.Stop()
+		}
+	}
+}
+
+// Cleanup removes all temporary files
+func (c *TestCluster) Cleanup() {
+	c.Stop()
+	time.Sleep(100 * time.Millisecond)
+	for _, dir := range c.walDirs {
+		os.RemoveAll(dir)
+	}
+}
+
+// WaitForLeader waits for a leader to be elected
+func (c *TestCluster) WaitForLeader(timeout time.Duration) (*raft.Node, error) {
+	deadline := time.Now().Add(timeout)
+	for time.Now().Before(deadline) {
+		for _, node := range c.Nodes {
+			if node.IsLeader() {
+				return node, nil
+			}
+		}
+		time.Sleep(50 * time.Millisecond)
+	}
+	return nil, fmt.Errorf("no leader elected within timeout")
+}
+
+// WaitForStableLeader waits for a leader that has committed at least one entry
+func (c *TestCluster) WaitForStableLeader(timeout time.Duration) (*raft.Node, error) {
+	deadline := time.Now().Add(timeout)
+
+	for time.Now().Before(deadline) {
+		for _, node := range c.Nodes {
+			if node.IsLeader() && node.GetCommitIndex() > 0 {
+				// Found a leader that has committed - wait a bit to ensure stability
+				time.Sleep(200 * time.Millisecond)
+				if node.IsLeader() {
+					return node, nil
+				}
+			}
+		}
+		time.Sleep(100 * time.Millisecond)
+	}
+	return nil, fmt.Errorf("no stable leader elected within timeout")
+}
+
+// GetLeader returns the current leader
+func (c *TestCluster) GetLeader() *raft.Node {
+	for _, node := range c.Nodes {
+		if node.IsLeader() {
+			return node
+		}
+	}
+	return nil
+}
+
+// GetLeaderExcluding returns the current leader excluding a specific node
+func (c *TestCluster) GetLeaderExcluding(excludeID string) *raft.Node {
+	for _, node := range c.Nodes {
+		if node.GetID() != excludeID && node.IsLeader() {
+			return node
+		}
+	}
+	return nil
+}
+
+// PartitionLeader partitions the current leader from the cluster
+func (c *TestCluster) PartitionLeader() *raft.Node {
+	leader := c.GetLeader()
+	if leader != nil {
+		c.Transport.Partition(leader.GetID())
+	}
+	return leader
+}
+
+// HealPartition heals all network partitions
+func (c *TestCluster) HealPartition() {
+	c.Transport.HealAll()
+}
+
+// SubmitCommand submits a command with retry logic
+func (c *TestCluster) SubmitCommand(cmd raft.Command, timeout time.Duration) error {
+	return c.SubmitCommandExcluding(cmd, timeout, "")
+}
+
+// SubmitCommandExcluding submits a command, excluding a specific node from being used as leader
+func (c *TestCluster) SubmitCommandExcluding(cmd raft.Command, timeout time.Duration, excludeID string) error {
+	deadline := time.Now().Add(timeout)
+
+	for time.Now().Before(deadline) {
+		var leader *raft.Node
+		if excludeID != "" {
+			leader = c.GetLeaderExcluding(excludeID)
+		} else {
+			leader = c.GetLeader()
+		}
+
+		if leader == nil {
+			time.Sleep(100 * time.Millisecond)
+			continue
+		}
+
+		ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
+		_, err := leader.SubmitWithResult(ctx, cmd)
+		cancel()
+
+		if err == nil {
+			return nil
+		}
+
+		time.Sleep(100 * time.Millisecond)
+	}
+
+	return fmt.Errorf("timeout submitting command")
+}
+
+// SubmitToNode submits a command directly to a specific node
+func (c *TestCluster) SubmitToNode(node *raft.Node, cmd raft.Command, timeout time.Duration) error {
+	ctx, cancel := context.WithTimeout(context.Background(), timeout)
+	defer cancel()
+
+	_, err := node.SubmitWithResult(ctx, cmd)
+	return err
+}
+
+// WaitForNewLeader waits for a new leader different from the specified node
+func (c *TestCluster) WaitForNewLeader(excludeID string, timeout time.Duration) (*raft.Node, error) {
+	deadline := time.Now().Add(timeout)
+	for time.Now().Before(deadline) {
+		for _, node := range c.Nodes {
+			if node.GetID() != excludeID && node.IsLeader() {
+				// Wait a bit to ensure it's stable
+				time.Sleep(300 * time.Millisecond)
+				if node.IsLeader() {
+					return node, nil
+				}
+			}
+		}
+		time.Sleep(50 * time.Millisecond)
+	}
+	return nil, fmt.Errorf("no new leader elected within timeout")
+}
\ No newline at end of file
diff --git a/repository_after/pkg/wal/wal.go b/repository_after/pkg/wal/wal.go
new file mode 100644
index 0000000..bcb4e70
--- /dev/null
+++ b/repository_after/pkg/wal/wal.go
@@ -0,0 +1,166 @@
+package wal
+
+import (
+	"encoding/json"
+	"fmt"
+	"os"
+	"path/filepath"
+	"sync"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+// WAL implements a write-ahead log for Raft persistence
+type WAL struct {
+	mu           sync.Mutex
+	dir          string
+	stateFile    string
+	snapshotFile string
+	closed       bool
+}
+
+// NewWAL creates a new WAL instance
+func NewWAL(dir string) (*WAL, error) {
+	if err := os.MkdirAll(dir, 0755); err != nil {
+		return nil, fmt.Errorf("failed to create WAL directory: %w", err)
+	}
+
+	return &WAL{
+		dir:          dir,
+		stateFile:    filepath.Join(dir, "state.json"),
+		snapshotFile: filepath.Join(dir, "snapshot.json"),
+		closed:       false,
+	}, nil
+}
+
+// ensureDir ensures the WAL directory exists
+func (w *WAL) ensureDir() error {
+	if _, err := os.Stat(w.dir); os.IsNotExist(err) {
+		return os.MkdirAll(w.dir, 0755)
+	}
+	return nil
+}
+
+// Save persists the current Raft state
+func (w *WAL) Save(state *raft.PersistentState) error {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+
+	if w.closed {
+		return nil
+	}
+
+	// Ensure directory exists
+	if err := w.ensureDir(); err != nil {
+		return fmt.Errorf("failed to ensure WAL directory: %w", err)
+	}
+
+	data, err := json.Marshal(state)
+	if err != nil {
+		return fmt.Errorf("failed to marshal state: %w", err)
+	}
+
+	// Write to temporary file first
+	tmpFile := w.stateFile + ".tmp"
+	if err := os.WriteFile(tmpFile, data, 0644); err != nil {
+		return fmt.Errorf("failed to write state file: %w", err)
+	}
+
+	// Atomic rename
+	if err := os.Rename(tmpFile, w.stateFile); err != nil {
+		return fmt.Errorf("failed to rename state file: %w", err)
+	}
+
+	return nil
+}
+
+// Load restores the Raft state from disk
+func (w *WAL) Load() (*raft.PersistentState, error) {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+
+	data, err := os.ReadFile(w.stateFile)
+	if err != nil {
+		if os.IsNotExist(err) {
+			return nil, nil
+		}
+		return nil, fmt.Errorf("failed to read state file: %w", err)
+	}
+
+	var state raft.PersistentState
+	if err := json.Unmarshal(data, &state); err != nil {
+		return nil, fmt.Errorf("failed to unmarshal state: %w", err)
+	}
+
+	return &state, nil
+}
+
+// SaveSnapshot persists a snapshot
+func (w *WAL) SaveSnapshot(snapshot *raft.Snapshot) error {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+
+	if w.closed {
+		return nil
+	}
+
+	// Ensure directory exists
+	if err := w.ensureDir(); err != nil {
+		return fmt.Errorf("failed to ensure WAL directory: %w", err)
+	}
+
+	data, err := json.Marshal(snapshot)
+	if err != nil {
+		return fmt.Errorf("failed to marshal snapshot: %w", err)
+	}
+
+	tmpFile := w.snapshotFile + ".tmp"
+	if err := os.WriteFile(tmpFile, data, 0644); err != nil {
+		return fmt.Errorf("failed to write snapshot file: %w", err)
+	}
+
+	if err := os.Rename(tmpFile, w.snapshotFile); err != nil {
+		return fmt.Errorf("failed to rename snapshot file: %w", err)
+	}
+
+	return nil
+}
+
+// LoadSnapshot restores a snapshot from disk
+func (w *WAL) LoadSnapshot() (*raft.Snapshot, error) {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+
+	data, err := os.ReadFile(w.snapshotFile)
+	if err != nil {
+		if os.IsNotExist(err) {
+			return nil, nil
+		}
+		return nil, fmt.Errorf("failed to read snapshot file: %w", err)
+	}
+
+	var snapshot raft.Snapshot
+	if err := json.Unmarshal(data, &snapshot); err != nil {
+		return nil, fmt.Errorf("failed to unmarshal snapshot: %w", err)
+	}
+
+	return &snapshot, nil
+}
+
+// Close closes the WAL
+func (w *WAL) Close() error {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+	w.closed = true
+	return nil
+}
+
+// Clear removes all WAL files (for testing)
+func (w *WAL) Clear() error {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+
+	os.Remove(w.stateFile)
+	os.Remove(w.snapshotFile)
+	return nil
+}
\ No newline at end of file
