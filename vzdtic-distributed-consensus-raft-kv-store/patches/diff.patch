diff --git a/repository_before/.gitkeep b/repository_before/.gitkeep
deleted file mode 100644
index e69de29..0000000
diff --git a/repository_after/cmd/server/main.go b/repository_after/cmd/server/main.go
new file mode 100644
index 0000000..961efe6
--- /dev/null
+++ b/repository_after/cmd/server/main.go
@@ -0,0 +1,143 @@
+package main
+
+import (
+	"context"
+	"flag"
+	"fmt"
+	"log"
+	"net/http"
+	"os"
+	"os/signal"
+	"strings"
+	"syscall"
+	"time"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/api"
+	grpctransport "github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/grpc"
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/kv"
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/wal"
+)
+
+func main() {
+	nodeID := flag.String("id", "", "Node ID")
+	addr := flag.String("addr", "", "gRPC listen address (e.g., localhost:5000)")
+	httpAddr := flag.String("http", "", "HTTP API listen address (e.g., localhost:8000)")
+	peers := flag.String("peers", "", "Comma-separated list of peer addresses (id1=addr1,id2=addr2)")
+	walDir := flag.String("wal", "", "WAL directory path")
+	flag.Parse()
+
+	if *nodeID == "" || *addr == "" || *httpAddr == "" {
+		flag.Usage()
+		os.Exit(1)
+	}
+
+	// Parse and validate peer addresses
+	peerAddrs := make(map[string]string)
+	peerIDs := make([]string, 0)
+	seenPeers := make(map[string]bool)
+
+	if *peers != "" {
+		for _, peer := range strings.Split(*peers, ",") {
+			peer = strings.TrimSpace(peer)
+			if peer == "" {
+				continue
+			}
+
+			parts := strings.SplitN(peer, "=", 2)
+			if len(parts) != 2 {
+				log.Fatalf("Invalid peer format: %s (expected id=addr)", peer)
+			}
+
+			peerID := strings.TrimSpace(parts[0])
+			peerAddr := strings.TrimSpace(parts[1])
+
+			if peerID == "" || peerAddr == "" {
+				log.Fatalf("Invalid peer: empty id or address in '%s'", peer)
+			}
+
+			if seenPeers[peerID] {
+				log.Fatalf("Duplicate peer ID: %s", peerID)
+			}
+			seenPeers[peerID] = true
+
+			peerAddrs[peerID] = peerAddr
+			if peerID != *nodeID {
+				peerIDs = append(peerIDs, peerID)
+			}
+		}
+	}
+	peerAddrs[*nodeID] = *addr
+
+	walPath := *walDir
+	if walPath == "" {
+		walPath = fmt.Sprintf("/tmp/raft-wal-%s", *nodeID)
+	}
+
+	log.Printf("Starting Raft node %s", *nodeID)
+	log.Printf("gRPC address: %s", *addr)
+	log.Printf("HTTP address: %s", *httpAddr)
+	log.Printf("Peers: %v", peerIDs)
+	log.Printf("WAL path: %s", walPath)
+
+	walInstance, err := wal.NewWAL(walPath)
+	if err != nil {
+		log.Fatalf("Failed to create WAL: %v", err)
+	}
+
+	store := kv.NewStore()
+
+	transport := grpctransport.NewGRPCTransport(*addr, peerAddrs)
+
+	config := raft.NodeConfig{
+		ID:                 *nodeID,
+		Peers:              peerIDs,
+		ElectionTimeoutMin: 500 * time.Millisecond,
+		ElectionTimeoutMax: 1000 * time.Millisecond,
+		HeartbeatInterval:  50 * time.Millisecond,
+		WALPath:            walPath,
+		SnapshotThreshold:  1000,
+	}
+
+	node := raft.NewNode(config, transport, walInstance, store)
+
+	// Set node BEFORE starting transport
+	transport.SetNode(node)
+
+	if err := transport.Start(); err != nil {
+		log.Fatalf("Failed to start transport: %v", err)
+	}
+
+	if err := node.Start(); err != nil {
+		log.Fatalf("Failed to start node: %v", err)
+	}
+
+	apiServer := &http.Server{
+		Addr:         *httpAddr,
+		Handler:      api.NewHTTPHandler(node, store),
+		ReadTimeout:  10 * time.Second,
+		WriteTimeout: 10 * time.Second,
+	}
+
+	go func() {
+		log.Printf("HTTP API listening on %s", *httpAddr)
+		if err := apiServer.ListenAndServe(); err != nil && err != http.ErrServerClosed {
+			log.Fatalf("HTTP server error: %v", err)
+		}
+	}()
+
+	sigCh := make(chan os.Signal, 1)
+	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)
+	<-sigCh
+
+	log.Println("Shutting down...")
+
+	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
+	defer cancel()
+
+	apiServer.Shutdown(ctx)
+	node.Stop()
+	transport.Stop()
+
+	log.Println("Shutdown complete")
+}
\ No newline at end of file
diff --git a/repository_after/pkg/api/client.go b/repository_after/pkg/api/client.go
new file mode 100644
index 0000000..4df364c
--- /dev/null
+++ b/repository_after/pkg/api/client.go
@@ -0,0 +1,81 @@
+package api
+
+import (
+	"context"
+	"errors"
+	"time"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+// Client provides a client interface to the Raft KV store
+type Client struct {
+	nodes   []*raft.Node
+	timeout time.Duration
+}
+
+// NewClient creates a new client
+func NewClient(nodes []*raft.Node) *Client {
+	return &Client{
+		nodes:   nodes,
+		timeout: 5 * time.Second,
+	}
+}
+
+// Set sets a key-value pair
+func (c *Client) Set(ctx context.Context, key, value string) error {
+	leader := c.findLeader()
+	if leader == nil {
+		return errors.New("no leader available")
+	}
+
+	cmd := raft.Command{
+		Type:  raft.CommandSet,
+		Key:   key,
+		Value: value,
+	}
+
+	_, err := leader.SubmitWithResult(ctx, cmd)
+	return err
+}
+
+// Get retrieves a value by key
+func (c *Client) Get(ctx context.Context, key string) (string, error) {
+	leader := c.findLeader()
+	if leader == nil {
+		return "", errors.New("no leader available")
+	}
+
+	return leader.Read(ctx, key)
+}
+
+// Delete removes a key
+func (c *Client) Delete(ctx context.Context, key string) error {
+	leader := c.findLeader()
+	if leader == nil {
+		return errors.New("no leader available")
+	}
+
+	cmd := raft.Command{
+		Type: raft.CommandDelete,
+		Key:  key,
+	}
+
+	_, err := leader.SubmitWithResult(ctx, cmd)
+	return err
+}
+
+// findLeader finds the current leader node
+func (c *Client) findLeader() *raft.Node {
+	for _, node := range c.nodes {
+		if node.IsLeader() {
+			return node
+		}
+	}
+	return nil
+}
+
+// SetTimeout sets the client timeout
+func (c *Client) SetTimeout(d time.Duration) {
+	c.timeout = d
+}
\ No newline at end of file
diff --git a/repository_after/pkg/api/http.go b/repository_after/pkg/api/http.go
new file mode 100644
index 0000000..242bac8
--- /dev/null
+++ b/repository_after/pkg/api/http.go
@@ -0,0 +1,171 @@
+package api
+
+import (
+	"context"
+	"encoding/json"
+	"net/http"
+	"strings"
+	"time"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/kv"
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+type HTTPHandler struct {
+	node  *raft.Node
+	store *kv.Store
+	mux   *http.ServeMux
+}
+
+func NewHTTPHandler(node *raft.Node, store *kv.Store) *HTTPHandler {
+	h := &HTTPHandler{
+		node:  node,
+		store: store,
+		mux:   http.NewServeMux(),
+	}
+
+	h.mux.HandleFunc("/kv/", h.handleKV)
+	h.mux.HandleFunc("/status", h.handleStatus)
+
+	return h
+}
+
+func (h *HTTPHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {
+	h.mux.ServeHTTP(w, r)
+}
+
+func (h *HTTPHandler) handleKV(w http.ResponseWriter, r *http.Request) {
+	key := strings.TrimPrefix(r.URL.Path, "/kv/")
+	if key == "" {
+		http.Error(w, "key required", http.StatusBadRequest)
+		return
+	}
+
+	switch r.Method {
+	case http.MethodGet:
+		ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
+		defer cancel()
+
+		// First check if we're the leader
+		if !h.node.IsLeader() {
+			h.respondNotLeader(w)
+			return
+		}
+
+		value, err := h.node.Read(ctx, key)
+		if err != nil {
+			if err == raft.ErrNotLeader {
+				h.respondNotLeader(w)
+				return
+			}
+			if err == raft.ErrTimeout || err == context.DeadlineExceeded {
+				http.Error(w, "request timeout", http.StatusGatewayTimeout)
+				return
+			}
+			http.Error(w, err.Error(), http.StatusInternalServerError)
+			return
+		}
+
+		// Check if key exists (value could be empty string)
+		exists := h.store.Exists(key)
+		if !exists {
+			http.Error(w, "key not found", http.StatusNotFound)
+			return
+		}
+
+		w.Header().Set("Content-Type", "application/json")
+		json.NewEncoder(w).Encode(map[string]string{"value": value})
+
+	case http.MethodPut, http.MethodPost:
+		var req struct {
+			Value string `json:"value"`
+		}
+		if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
+			http.Error(w, "invalid request body: "+err.Error(), http.StatusBadRequest)
+			return
+		}
+
+		ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
+		defer cancel()
+
+		cmd := raft.Command{
+			Type:  raft.CommandSet,
+			Key:   key,
+			Value: req.Value,
+		}
+
+		_, err := h.node.SubmitWithResult(ctx, cmd)
+		if err != nil {
+			if err == raft.ErrNotLeader {
+				h.respondNotLeader(w)
+				return
+			}
+			if err == context.DeadlineExceeded {
+				http.Error(w, "request timeout", http.StatusGatewayTimeout)
+				return
+			}
+			http.Error(w, err.Error(), http.StatusInternalServerError)
+			return
+		}
+
+		w.Header().Set("Content-Type", "application/json")
+		w.WriteHeader(http.StatusOK)
+		json.NewEncoder(w).Encode(map[string]string{"status": "ok"})
+
+	case http.MethodDelete:
+		ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
+		defer cancel()
+
+		cmd := raft.Command{
+			Type: raft.CommandDelete,
+			Key:  key,
+		}
+
+		_, err := h.node.SubmitWithResult(ctx, cmd)
+		if err != nil {
+			if err == raft.ErrNotLeader {
+				h.respondNotLeader(w)
+				return
+			}
+			if err == context.DeadlineExceeded {
+				http.Error(w, "request timeout", http.StatusGatewayTimeout)
+				return
+			}
+			http.Error(w, err.Error(), http.StatusInternalServerError)
+			return
+		}
+
+		w.Header().Set("Content-Type", "application/json")
+		w.WriteHeader(http.StatusOK)
+		json.NewEncoder(w).Encode(map[string]string{"status": "ok"})
+
+	default:
+		http.Error(w, "method not allowed", http.StatusMethodNotAllowed)
+	}
+}
+
+func (h *HTTPHandler) respondNotLeader(w http.ResponseWriter) {
+	leaderID := h.node.GetLeaderID()
+	w.Header().Set("Content-Type", "application/json")
+	w.WriteHeader(http.StatusServiceUnavailable)
+	json.NewEncoder(w).Encode(map[string]interface{}{
+		"error":     "not leader",
+		"leader_id": leaderID,
+	})
+}
+
+func (h *HTTPHandler) handleStatus(w http.ResponseWriter, r *http.Request) {
+	term, isLeader := h.node.GetState()
+
+	status := map[string]interface{}{
+		"id":           h.node.GetID(),
+		"term":         term,
+		"is_leader":    isLeader,
+		"leader_id":    h.node.GetLeaderID(),
+		"commit_index": h.node.GetCommitIndex(),
+		"cluster_size": h.node.GetClusterSize(),
+	}
+
+	w.Header().Set("Content-Type", "application/json")
+	json.NewEncoder(w).Encode(status)
+}
\ No newline at end of file
diff --git a/repository_after/pkg/grpc/proto/raft.pb.go b/repository_after/pkg/grpc/proto/raft.pb.go
new file mode 100644
index 0000000..35446c9
--- /dev/null
+++ b/repository_after/pkg/grpc/proto/raft.pb.go
@@ -0,0 +1,523 @@
+// Code generated by protoc-gen-go. DO NOT EDIT.
+// source: raft.proto
+
+package proto
+
+import (
+	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
+	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
+	reflect "reflect"
+	sync "sync"
+)
+
+const (
+	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
+	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
+)
+
+type LogEntry struct {
+	state         protoimpl.MessageState
+	sizeCache     protoimpl.SizeCache
+	unknownFields protoimpl.UnknownFields
+
+	Index   uint64   `protobuf:"varint,1,opt,name=index,proto3" json:"index,omitempty"`
+	Term    uint64   `protobuf:"varint,2,opt,name=term,proto3" json:"term,omitempty"`
+	Command *Command `protobuf:"bytes,3,opt,name=command,proto3" json:"command,omitempty"`
+}
+
+func (x *LogEntry) Reset() {
+	*x = LogEntry{}
+}
+
+func (x *LogEntry) String() string {
+	return protoimpl.X.MessageStringOf(x)
+}
+
+func (*LogEntry) ProtoMessage() {}
+
+func (x *LogEntry) ProtoReflect() protoreflect.Message {
+	mi := &file_raft_proto_msgTypes[0]
+	if protoimpl.UnsafeEnabled && x != nil {
+		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
+		if ms.LoadMessageInfo() == nil {
+			ms.StoreMessageInfo(mi)
+		}
+		return ms
+	}
+	return mi.MessageOf(x)
+}
+
+func (x *LogEntry) GetIndex() uint64 {
+	if x != nil {
+		return x.Index
+	}
+	return 0
+}
+
+func (x *LogEntry) GetTerm() uint64 {
+	if x != nil {
+		return x.Term
+	}
+	return 0
+}
+
+func (x *LogEntry) GetCommand() *Command {
+	if x != nil {
+		return x.Command
+	}
+	return nil
+}
+
+type Command struct {
+	state         protoimpl.MessageState
+	sizeCache     protoimpl.SizeCache
+	unknownFields protoimpl.UnknownFields
+
+	Type  int32  `protobuf:"varint,1,opt,name=type,proto3" json:"type,omitempty"`
+	Key   string `protobuf:"bytes,2,opt,name=key,proto3" json:"key,omitempty"`
+	Value string `protobuf:"bytes,3,opt,name=value,proto3" json:"value,omitempty"`
+}
+
+func (x *Command) Reset() {
+	*x = Command{}
+}
+
+func (x *Command) String() string {
+	return protoimpl.X.MessageStringOf(x)
+}
+
+func (*Command) ProtoMessage() {}
+
+func (x *Command) ProtoReflect() protoreflect.Message {
+	mi := &file_raft_proto_msgTypes[1]
+	if protoimpl.UnsafeEnabled && x != nil {
+		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
+		if ms.LoadMessageInfo() == nil {
+			ms.StoreMessageInfo(mi)
+		}
+		return ms
+	}
+	return mi.MessageOf(x)
+}
+
+func (x *Command) GetType() int32 {
+	if x != nil {
+		return x.Type
+	}
+	return 0
+}
+
+func (x *Command) GetKey() string {
+	if x != nil {
+		return x.Key
+	}
+	return ""
+}
+
+func (x *Command) GetValue() string {
+	if x != nil {
+		return x.Value
+	}
+	return ""
+}
+
+type RequestVoteRequest struct {
+	state         protoimpl.MessageState
+	sizeCache     protoimpl.SizeCache
+	unknownFields protoimpl.UnknownFields
+
+	Term         uint64 `protobuf:"varint,1,opt,name=term,proto3" json:"term,omitempty"`
+	CandidateId  string `protobuf:"bytes,2,opt,name=candidate_id,json=candidateId,proto3" json:"candidate_id,omitempty"`
+	LastLogIndex uint64 `protobuf:"varint,3,opt,name=last_log_index,json=lastLogIndex,proto3" json:"last_log_index,omitempty"`
+	LastLogTerm  uint64 `protobuf:"varint,4,opt,name=last_log_term,json=lastLogTerm,proto3" json:"last_log_term,omitempty"`
+}
+
+func (x *RequestVoteRequest) Reset() {
+	*x = RequestVoteRequest{}
+}
+
+func (x *RequestVoteRequest) String() string {
+	return protoimpl.X.MessageStringOf(x)
+}
+
+func (*RequestVoteRequest) ProtoMessage() {}
+
+func (x *RequestVoteRequest) ProtoReflect() protoreflect.Message {
+	mi := &file_raft_proto_msgTypes[2]
+	if protoimpl.UnsafeEnabled && x != nil {
+		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
+		if ms.LoadMessageInfo() == nil {
+			ms.StoreMessageInfo(mi)
+		}
+		return ms
+	}
+	return mi.MessageOf(x)
+}
+
+func (x *RequestVoteRequest) GetTerm() uint64 {
+	if x != nil {
+		return x.Term
+	}
+	return 0
+}
+
+func (x *RequestVoteRequest) GetCandidateId() string {
+	if x != nil {
+		return x.CandidateId
+	}
+	return ""
+}
+
+func (x *RequestVoteRequest) GetLastLogIndex() uint64 {
+	if x != nil {
+		return x.LastLogIndex
+	}
+	return 0
+}
+
+func (x *RequestVoteRequest) GetLastLogTerm() uint64 {
+	if x != nil {
+		return x.LastLogTerm
+	}
+	return 0
+}
+
+type RequestVoteResponse struct {
+	state         protoimpl.MessageState
+	sizeCache     protoimpl.SizeCache
+	unknownFields protoimpl.UnknownFields
+
+	Term        uint64 `protobuf:"varint,1,opt,name=term,proto3" json:"term,omitempty"`
+	VoteGranted bool   `protobuf:"varint,2,opt,name=vote_granted,json=voteGranted,proto3" json:"vote_granted,omitempty"`
+}
+
+func (x *RequestVoteResponse) Reset() {
+	*x = RequestVoteResponse{}
+}
+
+func (x *RequestVoteResponse) String() string {
+	return protoimpl.X.MessageStringOf(x)
+}
+
+func (*RequestVoteResponse) ProtoMessage() {}
+
+func (x *RequestVoteResponse) ProtoReflect() protoreflect.Message {
+	mi := &file_raft_proto_msgTypes[3]
+	if protoimpl.UnsafeEnabled && x != nil {
+		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
+		if ms.LoadMessageInfo() == nil {
+			ms.StoreMessageInfo(mi)
+		}
+		return ms
+	}
+	return mi.MessageOf(x)
+}
+
+func (x *RequestVoteResponse) GetTerm() uint64 {
+	if x != nil {
+		return x.Term
+	}
+	return 0
+}
+
+func (x *RequestVoteResponse) GetVoteGranted() bool {
+	if x != nil {
+		return x.VoteGranted
+	}
+	return false
+}
+
+type AppendEntriesRequest struct {
+	state         protoimpl.MessageState
+	sizeCache     protoimpl.SizeCache
+	unknownFields protoimpl.UnknownFields
+
+	Term         uint64      `protobuf:"varint,1,opt,name=term,proto3" json:"term,omitempty"`
+	LeaderId     string      `protobuf:"bytes,2,opt,name=leader_id,json=leaderId,proto3" json:"leader_id,omitempty"`
+	PrevLogIndex uint64      `protobuf:"varint,3,opt,name=prev_log_index,json=prevLogIndex,proto3" json:"prev_log_index,omitempty"`
+	PrevLogTerm  uint64      `protobuf:"varint,4,opt,name=prev_log_term,json=prevLogTerm,proto3" json:"prev_log_term,omitempty"`
+	Entries      []*LogEntry `protobuf:"bytes,5,rep,name=entries,proto3" json:"entries,omitempty"`
+	LeaderCommit uint64      `protobuf:"varint,6,opt,name=leader_commit,json=leaderCommit,proto3" json:"leader_commit,omitempty"`
+}
+
+func (x *AppendEntriesRequest) Reset() {
+	*x = AppendEntriesRequest{}
+}
+
+func (x *AppendEntriesRequest) String() string {
+	return protoimpl.X.MessageStringOf(x)
+}
+
+func (*AppendEntriesRequest) ProtoMessage() {}
+
+func (x *AppendEntriesRequest) ProtoReflect() protoreflect.Message {
+	mi := &file_raft_proto_msgTypes[4]
+	if protoimpl.UnsafeEnabled && x != nil {
+		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
+		if ms.LoadMessageInfo() == nil {
+			ms.StoreMessageInfo(mi)
+		}
+		return ms
+	}
+	return mi.MessageOf(x)
+}
+
+func (x *AppendEntriesRequest) GetTerm() uint64 {
+	if x != nil {
+		return x.Term
+	}
+	return 0
+}
+
+func (x *AppendEntriesRequest) GetLeaderId() string {
+	if x != nil {
+		return x.LeaderId
+	}
+	return ""
+}
+
+func (x *AppendEntriesRequest) GetPrevLogIndex() uint64 {
+	if x != nil {
+		return x.PrevLogIndex
+	}
+	return 0
+}
+
+func (x *AppendEntriesRequest) GetPrevLogTerm() uint64 {
+	if x != nil {
+		return x.PrevLogTerm
+	}
+	return 0
+}
+
+func (x *AppendEntriesRequest) GetEntries() []*LogEntry {
+	if x != nil {
+		return x.Entries
+	}
+	return nil
+}
+
+func (x *AppendEntriesRequest) GetLeaderCommit() uint64 {
+	if x != nil {
+		return x.LeaderCommit
+	}
+	return 0
+}
+
+type AppendEntriesResponse struct {
+	state         protoimpl.MessageState
+	sizeCache     protoimpl.SizeCache
+	unknownFields protoimpl.UnknownFields
+
+	Term          uint64 `protobuf:"varint,1,opt,name=term,proto3" json:"term,omitempty"`
+	Success       bool   `protobuf:"varint,2,opt,name=success,proto3" json:"success,omitempty"`
+	ConflictIndex uint64 `protobuf:"varint,3,opt,name=conflict_index,json=conflictIndex,proto3" json:"conflict_index,omitempty"`
+	ConflictTerm  uint64 `protobuf:"varint,4,opt,name=conflict_term,json=conflictTerm,proto3" json:"conflict_term,omitempty"`
+}
+
+func (x *AppendEntriesResponse) Reset() {
+	*x = AppendEntriesResponse{}
+}
+
+func (x *AppendEntriesResponse) String() string {
+	return protoimpl.X.MessageStringOf(x)
+}
+
+func (*AppendEntriesResponse) ProtoMessage() {}
+
+func (x *AppendEntriesResponse) ProtoReflect() protoreflect.Message {
+	mi := &file_raft_proto_msgTypes[5]
+	if protoimpl.UnsafeEnabled && x != nil {
+		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
+		if ms.LoadMessageInfo() == nil {
+			ms.StoreMessageInfo(mi)
+		}
+		return ms
+	}
+	return mi.MessageOf(x)
+}
+
+func (x *AppendEntriesResponse) GetTerm() uint64 {
+	if x != nil {
+		return x.Term
+	}
+	return 0
+}
+
+func (x *AppendEntriesResponse) GetSuccess() bool {
+	if x != nil {
+		return x.Success
+	}
+	return false
+}
+
+func (x *AppendEntriesResponse) GetConflictIndex() uint64 {
+	if x != nil {
+		return x.ConflictIndex
+	}
+	return 0
+}
+
+func (x *AppendEntriesResponse) GetConflictTerm() uint64 {
+	if x != nil {
+		return x.ConflictTerm
+	}
+	return 0
+}
+
+type InstallSnapshotRequest struct {
+	state         protoimpl.MessageState
+	sizeCache     protoimpl.SizeCache
+	unknownFields protoimpl.UnknownFields
+
+	Term              uint64 `protobuf:"varint,1,opt,name=term,proto3" json:"term,omitempty"`
+	LeaderId          string `protobuf:"bytes,2,opt,name=leader_id,json=leaderId,proto3" json:"leader_id,omitempty"`
+	LastIncludedIndex uint64 `protobuf:"varint,3,opt,name=last_included_index,json=lastIncludedIndex,proto3" json:"last_included_index,omitempty"`
+	LastIncludedTerm  uint64 `protobuf:"varint,4,opt,name=last_included_term,json=lastIncludedTerm,proto3" json:"last_included_term,omitempty"`
+	Data              []byte `protobuf:"bytes,5,opt,name=data,proto3" json:"data,omitempty"`
+}
+
+func (x *InstallSnapshotRequest) Reset() {
+	*x = InstallSnapshotRequest{}
+}
+
+func (x *InstallSnapshotRequest) String() string {
+	return protoimpl.X.MessageStringOf(x)
+}
+
+func (*InstallSnapshotRequest) ProtoMessage() {}
+
+func (x *InstallSnapshotRequest) ProtoReflect() protoreflect.Message {
+	mi := &file_raft_proto_msgTypes[6]
+	if protoimpl.UnsafeEnabled && x != nil {
+		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
+		if ms.LoadMessageInfo() == nil {
+			ms.StoreMessageInfo(mi)
+		}
+		return ms
+	}
+	return mi.MessageOf(x)
+}
+
+func (x *InstallSnapshotRequest) GetTerm() uint64 {
+	if x != nil {
+		return x.Term
+	}
+	return 0
+}
+
+func (x *InstallSnapshotRequest) GetLeaderId() string {
+	if x != nil {
+		return x.LeaderId
+	}
+	return ""
+}
+
+func (x *InstallSnapshotRequest) GetLastIncludedIndex() uint64 {
+	if x != nil {
+		return x.LastIncludedIndex
+	}
+	return 0
+}
+
+func (x *InstallSnapshotRequest) GetLastIncludedTerm() uint64 {
+	if x != nil {
+		return x.LastIncludedTerm
+	}
+	return 0
+}
+
+func (x *InstallSnapshotRequest) GetData() []byte {
+	if x != nil {
+		return x.Data
+	}
+	return nil
+}
+
+type InstallSnapshotResponse struct {
+	state         protoimpl.MessageState
+	sizeCache     protoimpl.SizeCache
+	unknownFields protoimpl.UnknownFields
+
+	Term uint64 `protobuf:"varint,1,opt,name=term,proto3" json:"term,omitempty"`
+}
+
+func (x *InstallSnapshotResponse) Reset() {
+	*x = InstallSnapshotResponse{}
+}
+
+func (x *InstallSnapshotResponse) String() string {
+	return protoimpl.X.MessageStringOf(x)
+}
+
+func (*InstallSnapshotResponse) ProtoMessage() {}
+
+func (x *InstallSnapshotResponse) ProtoReflect() protoreflect.Message {
+	mi := &file_raft_proto_msgTypes[7]
+	if protoimpl.UnsafeEnabled && x != nil {
+		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
+		if ms.LoadMessageInfo() == nil {
+			ms.StoreMessageInfo(mi)
+		}
+		return ms
+	}
+	return mi.MessageOf(x)
+}
+
+func (x *InstallSnapshotResponse) GetTerm() uint64 {
+	if x != nil {
+		return x.Term
+	}
+	return 0
+}
+
+var file_raft_proto_msgTypes = make([]protoimpl.MessageInfo, 8)
+var file_raft_proto_goTypes = []interface{}{
+	(*LogEntry)(nil),                // 0
+	(*Command)(nil),                 // 1
+	(*RequestVoteRequest)(nil),      // 2
+	(*RequestVoteResponse)(nil),     // 3
+	(*AppendEntriesRequest)(nil),    // 4
+	(*AppendEntriesResponse)(nil),   // 5
+	(*InstallSnapshotRequest)(nil),  // 6
+	(*InstallSnapshotResponse)(nil), // 7
+}
+var file_raft_proto_depIdxs = []int32{
+	1, // 0: raft.LogEntry.command:type_name -> raft.Command
+	0, // 1: raft.AppendEntriesRequest.entries:type_name -> raft.LogEntry
+	2, // 2: raft.RaftService.RequestVote:input_type -> raft.RequestVoteRequest
+	4, // 3: raft.RaftService.AppendEntries:input_type -> raft.AppendEntriesRequest
+	6, // 4: raft.RaftService.InstallSnapshot:input_type -> raft.InstallSnapshotRequest
+	3, // 5: raft.RaftService.RequestVote:output_type -> raft.RequestVoteResponse
+	5, // 6: raft.RaftService.AppendEntries:output_type -> raft.AppendEntriesResponse
+	7, // 7: raft.RaftService.InstallSnapshot:output_type -> raft.InstallSnapshotResponse
+	5, // [5:8] is the sub-list for method output_type
+	2, // [2:5] is the sub-list for method input_type
+	2, // [2:2] is the sub-list for extension type_name
+	2, // [2:2] is the sub-list for extension extendee
+	0, // [0:2] is the sub-list for field type_name
+}
+
+func init() { file_raft_proto_init() }
+
+func file_raft_proto_init() {
+	if file_raft_proto_msgTypes[0].Exporter != nil {
+		return
+	}
+	type x struct{}
+	out := protoimpl.TypeBuilder{
+		File: protoimpl.DescBuilder{
+			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
+			RawDescriptor: nil,
+			NumEnums:      0,
+			NumMessages:   8,
+			NumExtensions: 0,
+			NumServices:   1,
+		},
+		GoTypes:           file_raft_proto_goTypes,
+		DependencyIndexes: file_raft_proto_depIdxs,
+		MessageInfos:      file_raft_proto_msgTypes,
+	}.Build()
+	_ = out.File
+	file_raft_proto_msgTypes = out.Messages
+}
\ No newline at end of file
diff --git a/repository_after/pkg/grpc/proto/raft.proto b/repository_after/pkg/grpc/proto/raft.proto
new file mode 100644
index 0000000..fc31e00
--- /dev/null
+++ b/repository_after/pkg/grpc/proto/raft.proto
@@ -0,0 +1,63 @@
+syntax = "proto3";
+
+package raft;
+
+option go_package = "github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/grpc/proto";
+
+service RaftService {
+  rpc RequestVote(RequestVoteRequest) returns (RequestVoteResponse);
+  rpc AppendEntries(AppendEntriesRequest) returns (AppendEntriesResponse);
+  rpc InstallSnapshot(InstallSnapshotRequest) returns (InstallSnapshotResponse);
+}
+
+message LogEntry {
+  uint64 index = 1;
+  uint64 term = 2;
+  Command command = 3;
+}
+
+message Command {
+  int32 type = 1;
+  string key = 2;
+  string value = 3;
+}
+
+message RequestVoteRequest {
+  uint64 term = 1;
+  string candidate_id = 2;
+  uint64 last_log_index = 3;
+  uint64 last_log_term = 4;
+}
+
+message RequestVoteResponse {
+  uint64 term = 1;
+  bool vote_granted = 2;
+}
+
+message AppendEntriesRequest {
+  uint64 term = 1;
+  string leader_id = 2;
+  uint64 prev_log_index = 3;
+  uint64 prev_log_term = 4;
+  repeated LogEntry entries = 5;
+  uint64 leader_commit = 6;
+}
+
+message AppendEntriesResponse {
+  uint64 term = 1;
+  bool success = 2;
+  uint64 conflict_index = 3;
+  uint64 conflict_term = 4;
+}
+
+message InstallSnapshotRequest {
+  uint64 term = 1;
+  string leader_id = 2;
+  uint64 last_included_index = 3;
+  uint64 last_included_term = 4;
+  bytes data = 5;
+}
+
+message InstallSnapshotResponse {
+  uint64 term = 1;
+}
\ No newline at end of file
diff --git a/repository_after/pkg/grpc/proto/raft_grpc.pb.go b/repository_after/pkg/grpc/proto/raft_grpc.pb.go
new file mode 100644
index 0000000..86c9dd2
--- /dev/null
+++ b/repository_after/pkg/grpc/proto/raft_grpc.pb.go
@@ -0,0 +1,159 @@
+// Code generated by protoc-gen-go-grpc. DO NOT EDIT.
+
+package proto
+
+import (
+	context "context"
+	grpc "google.golang.org/grpc"
+	codes "google.golang.org/grpc/codes"
+	status "google.golang.org/grpc/status"
+)
+
+// RaftServiceClient is the client API for RaftService service.
+type RaftServiceClient interface {
+	RequestVote(ctx context.Context, in *RequestVoteRequest, opts ...grpc.CallOption) (*RequestVoteResponse, error)
+	AppendEntries(ctx context.Context, in *AppendEntriesRequest, opts ...grpc.CallOption) (*AppendEntriesResponse, error)
+	InstallSnapshot(ctx context.Context, in *InstallSnapshotRequest, opts ...grpc.CallOption) (*InstallSnapshotResponse, error)
+}
+
+type raftServiceClient struct {
+	cc grpc.ClientConnInterface
+}
+
+func NewRaftServiceClient(cc grpc.ClientConnInterface) RaftServiceClient {
+	return &raftServiceClient{cc}
+}
+
+func (c *raftServiceClient) RequestVote(ctx context.Context, in *RequestVoteRequest, opts ...grpc.CallOption) (*RequestVoteResponse, error) {
+	out := new(RequestVoteResponse)
+	err := c.cc.Invoke(ctx, "/raft.RaftService/RequestVote", in, out, opts...)
+	if err != nil {
+		return nil, err
+	}
+	return out, nil
+}
+
+func (c *raftServiceClient) AppendEntries(ctx context.Context, in *AppendEntriesRequest, opts ...grpc.CallOption) (*AppendEntriesResponse, error) {
+	out := new(AppendEntriesResponse)
+	err := c.cc.Invoke(ctx, "/raft.RaftService/AppendEntries", in, out, opts...)
+	if err != nil {
+		return nil, err
+	}
+	return out, nil
+}
+
+func (c *raftServiceClient) InstallSnapshot(ctx context.Context, in *InstallSnapshotRequest, opts ...grpc.CallOption) (*InstallSnapshotResponse, error) {
+	out := new(InstallSnapshotResponse)
+	err := c.cc.Invoke(ctx, "/raft.RaftService/InstallSnapshot", in, out, opts...)
+	if err != nil {
+		return nil, err
+	}
+	return out, nil
+}
+
+// RaftServiceServer is the server API for RaftService service.
+type RaftServiceServer interface {
+	RequestVote(context.Context, *RequestVoteRequest) (*RequestVoteResponse, error)
+	AppendEntries(context.Context, *AppendEntriesRequest) (*AppendEntriesResponse, error)
+	InstallSnapshot(context.Context, *InstallSnapshotRequest) (*InstallSnapshotResponse, error)
+	mustEmbedUnimplementedRaftServiceServer()
+}
+
+// UnimplementedRaftServiceServer must be embedded to have forward compatible implementations.
+type UnimplementedRaftServiceServer struct {
+}
+
+func (UnimplementedRaftServiceServer) RequestVote(context.Context, *RequestVoteRequest) (*RequestVoteResponse, error) {
+	return nil, status.Errorf(codes.Unimplemented, "method RequestVote not implemented")
+}
+func (UnimplementedRaftServiceServer) AppendEntries(context.Context, *AppendEntriesRequest) (*AppendEntriesResponse, error) {
+	return nil, status.Errorf(codes.Unimplemented, "method AppendEntries not implemented")
+}
+func (UnimplementedRaftServiceServer) InstallSnapshot(context.Context, *InstallSnapshotRequest) (*InstallSnapshotResponse, error) {
+	return nil, status.Errorf(codes.Unimplemented, "method InstallSnapshot not implemented")
+}
+func (UnimplementedRaftServiceServer) mustEmbedUnimplementedRaftServiceServer() {}
+
+// UnsafeRaftServiceServer may be embedded to opt out of forward compatibility for this service.
+type UnsafeRaftServiceServer interface {
+	mustEmbedUnimplementedRaftServiceServer()
+}
+
+func RegisterRaftServiceServer(s grpc.ServiceRegistrar, srv RaftServiceServer) {
+	s.RegisterService(&RaftService_ServiceDesc, srv)
+}
+
+func _RaftService_RequestVote_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
+	in := new(RequestVoteRequest)
+	if err := dec(in); err != nil {
+		return nil, err
+	}
+	if interceptor == nil {
+		return srv.(RaftServiceServer).RequestVote(ctx, in)
+	}
+	info := &grpc.UnaryServerInfo{
+		Server:     srv,
+		FullMethod: "/raft.RaftService/RequestVote",
+	}
+	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
+		return srv.(RaftServiceServer).RequestVote(ctx, req.(*RequestVoteRequest))
+	}
+	return interceptor(ctx, in, info, handler)
+}
+
+func _RaftService_AppendEntries_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
+	in := new(AppendEntriesRequest)
+	if err := dec(in); err != nil {
+		return nil, err
+	}
+	if interceptor == nil {
+		return srv.(RaftServiceServer).AppendEntries(ctx, in)
+	}
+	info := &grpc.UnaryServerInfo{
+		Server:     srv,
+		FullMethod: "/raft.RaftService/AppendEntries",
+	}
+	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
+		return srv.(RaftServiceServer).AppendEntries(ctx, req.(*AppendEntriesRequest))
+	}
+	return interceptor(ctx, in, info, handler)
+}
+
+func _RaftService_InstallSnapshot_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
+	in := new(InstallSnapshotRequest)
+	if err := dec(in); err != nil {
+		return nil, err
+	}
+	if interceptor == nil {
+		return srv.(RaftServiceServer).InstallSnapshot(ctx, in)
+	}
+	info := &grpc.UnaryServerInfo{
+		Server:     srv,
+		FullMethod: "/raft.RaftService/InstallSnapshot",
+	}
+	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
+		return srv.(RaftServiceServer).InstallSnapshot(ctx, req.(*InstallSnapshotRequest))
+	}
+	return interceptor(ctx, in, info, handler)
+}
+
+var RaftService_ServiceDesc = grpc.ServiceDesc{
+	ServiceName: "raft.RaftService",
+	HandlerType: (*RaftServiceServer)(nil),
+	Methods: []grpc.MethodDesc{
+		{
+			MethodName: "RequestVote",
+			Handler:    _RaftService_RequestVote_Handler,
+		},
+		{
+			MethodName: "AppendEntries",
+			Handler:    _RaftService_AppendEntries_Handler,
+		},
+		{
+			MethodName: "InstallSnapshot",
+			Handler:    _RaftService_InstallSnapshot_Handler,
+		},
+	},
+	Streams:  []grpc.StreamDesc{},
+	Metadata: "raft.proto",
+}
\ No newline at end of file
diff --git a/repository_after/pkg/grpc/transport.go b/repository_after/pkg/grpc/transport.go
new file mode 100644
index 0000000..5484baf
--- /dev/null
+++ b/repository_after/pkg/grpc/transport.go
@@ -0,0 +1,313 @@
+package grpc
+
+import (
+	"context"
+	"fmt"
+	"net"
+	"sync"
+	"time"
+
+	"google.golang.org/grpc"
+	"google.golang.org/grpc/credentials/insecure"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/grpc/proto"
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+// GRPCTransport implements network transport using gRPC
+type GRPCTransport struct {
+	mu          sync.RWMutex
+	localAddr   string
+	node        *raft.Node
+	server      *grpc.Server
+	listener    net.Listener
+	connections map[string]*grpc.ClientConn
+	clients     map[string]proto.RaftServiceClient
+	peerAddrs   map[string]string
+	timeout     time.Duration
+}
+
+// raftServer implements the gRPC service interface
+type raftServer struct {
+	proto.UnimplementedRaftServiceServer
+	transport *GRPCTransport
+}
+
+func NewGRPCTransport(addr string, peerAddrs map[string]string) *GRPCTransport {
+	return &GRPCTransport{
+		localAddr:   addr,
+		connections: make(map[string]*grpc.ClientConn),
+		clients:     make(map[string]proto.RaftServiceClient),
+		peerAddrs:   peerAddrs,
+		timeout:     5 * time.Second,
+	}
+}
+
+func (t *GRPCTransport) Start() error {
+	listener, err := net.Listen("tcp", t.localAddr)
+	if err != nil {
+		return fmt.Errorf("failed to listen: %w", err)
+	}
+	t.listener = listener
+
+	t.server = grpc.NewServer()
+	proto.RegisterRaftServiceServer(t.server, &raftServer{transport: t})
+
+	go func() {
+		if err := t.server.Serve(listener); err != nil {
+			fmt.Printf("gRPC server error: %v\n", err)
+		}
+	}()
+
+	return nil
+}
+
+func (t *GRPCTransport) Stop() {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+
+	for _, conn := range t.connections {
+		conn.Close()
+	}
+
+	if t.server != nil {
+		t.server.GracefulStop()
+	}
+	if t.listener != nil {
+		t.listener.Close()
+	}
+}
+
+func (t *GRPCTransport) SetNode(node *raft.Node) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	t.node = node
+}
+
+func (t *GRPCTransport) getClient(target string) (proto.RaftServiceClient, error) {
+	t.mu.RLock()
+	if client, ok := t.clients[target]; ok {
+		t.mu.RUnlock()
+		return client, nil
+	}
+	t.mu.RUnlock()
+
+	t.mu.Lock()
+	defer t.mu.Unlock()
+
+	if client, ok := t.clients[target]; ok {
+		return client, nil
+	}
+
+	addr, ok := t.peerAddrs[target]
+	if !ok {
+		return nil, fmt.Errorf("unknown peer: %s", target)
+	}
+
+	ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
+	defer cancel()
+
+	conn, err := grpc.DialContext(ctx, addr,
+		grpc.WithTransportCredentials(insecure.NewCredentials()),
+		grpc.WithBlock(),
+	)
+	if err != nil {
+		return nil, fmt.Errorf("failed to connect to %s: %w", addr, err)
+	}
+
+	client := proto.NewRaftServiceClient(conn)
+	t.connections[target] = conn
+	t.clients[target] = client
+	return client, nil
+}
+
+// Transport interface implementations (client-side)
+
+func (t *GRPCTransport) RequestVote(target string, args *raft.RequestVoteArgs) (*raft.RequestVoteReply, error) {
+	client, err := t.getClient(target)
+	if err != nil {
+		return nil, err
+	}
+
+	ctx, cancel := context.WithTimeout(context.Background(), t.timeout)
+	defer cancel()
+
+	req := &proto.RequestVoteRequest{
+		Term:         args.Term,
+		CandidateId:  args.CandidateID,
+		LastLogIndex: args.LastLogIndex,
+		LastLogTerm:  args.LastLogTerm,
+	}
+
+	resp, err := client.RequestVote(ctx, req)
+	if err != nil {
+		return nil, err
+	}
+
+	return &raft.RequestVoteReply{
+		Term:        resp.Term,
+		VoteGranted: resp.VoteGranted,
+	}, nil
+}
+
+func (t *GRPCTransport) AppendEntries(target string, args *raft.AppendEntriesArgs) (*raft.AppendEntriesReply, error) {
+	client, err := t.getClient(target)
+	if err != nil {
+		return nil, err
+	}
+
+	ctx, cancel := context.WithTimeout(context.Background(), t.timeout)
+	defer cancel()
+
+	entries := make([]*proto.LogEntry, len(args.Entries))
+	for i, entry := range args.Entries {
+		entries[i] = &proto.LogEntry{
+			Index: entry.Index,
+			Term:  entry.Term,
+			Command: &proto.Command{
+				Type:  int32(entry.Command.Type),
+				Key:   entry.Command.Key,
+				Value: entry.Command.Value,
+			},
+		}
+	}
+
+	req := &proto.AppendEntriesRequest{
+		Term:         args.Term,
+		LeaderId:     args.LeaderID,
+		PrevLogIndex: args.PrevLogIndex,
+		PrevLogTerm:  args.PrevLogTerm,
+		Entries:      entries,
+		LeaderCommit: args.LeaderCommit,
+	}
+
+	resp, err := client.AppendEntries(ctx, req)
+	if err != nil {
+		return nil, err
+	}
+
+	return &raft.AppendEntriesReply{
+		Term:          resp.Term,
+		Success:       resp.Success,
+		ConflictIndex: resp.ConflictIndex,
+		ConflictTerm:  resp.ConflictTerm,
+	}, nil
+}
+
+func (t *GRPCTransport) InstallSnapshot(target string, args *raft.InstallSnapshotArgs) (*raft.InstallSnapshotReply, error) {
+	client, err := t.getClient(target)
+	if err != nil {
+		return nil, err
+	}
+
+	ctx, cancel := context.WithTimeout(context.Background(), t.timeout*2)
+	defer cancel()
+
+	req := &proto.InstallSnapshotRequest{
+		Term:              args.Term,
+		LeaderId:          args.LeaderID,
+		LastIncludedIndex: args.LastIncludedIndex,
+		LastIncludedTerm:  args.LastIncludedTerm,
+		Data:              args.Data,
+	}
+
+	resp, err := client.InstallSnapshot(ctx, req)
+	if err != nil {
+		return nil, err
+	}
+
+	return &raft.InstallSnapshotReply{
+		Term: resp.Term,
+	}, nil
+}
+
+// gRPC service implementation (server-side)
+
+func (s *raftServer) RequestVote(ctx context.Context, req *proto.RequestVoteRequest) (*proto.RequestVoteResponse, error) {
+	s.transport.mu.RLock()
+	node := s.transport.node
+	s.transport.mu.RUnlock()
+
+	if node == nil {
+		return nil, fmt.Errorf("node not set")
+	}
+
+	args := &raft.RequestVoteArgs{
+		Term:         req.Term,
+		CandidateID:  req.CandidateId,
+		LastLogIndex: req.LastLogIndex,
+		LastLogTerm:  req.LastLogTerm,
+	}
+
+	reply := node.HandleRequestVote(args)
+
+	return &proto.RequestVoteResponse{
+		Term:        reply.Term,
+		VoteGranted: reply.VoteGranted,
+	}, nil
+}
+
+func (s *raftServer) AppendEntries(ctx context.Context, req *proto.AppendEntriesRequest) (*proto.AppendEntriesResponse, error) {
+	s.transport.mu.RLock()
+	node := s.transport.node
+	s.transport.mu.RUnlock()
+
+	if node == nil {
+		return nil, fmt.Errorf("node not set")
+	}
+
+	entries := make([]raft.LogEntry, len(req.Entries))
+	for i, entry := range req.Entries {
+		entries[i] = raft.LogEntry{
+			Index: entry.Index,
+			Term:  entry.Term,
+			Command: raft.Command{
+				Type:  raft.CommandType(entry.Command.Type),
+				Key:   entry.Command.Key,
+				Value: entry.Command.Value,
+			},
+		}
+	}
+
+	args := &raft.AppendEntriesArgs{
+		Term:         req.Term,
+		LeaderID:     req.LeaderId,
+		PrevLogIndex: req.PrevLogIndex,
+		PrevLogTerm:  req.PrevLogTerm,
+		Entries:      entries,
+		LeaderCommit: req.LeaderCommit,
+	}
+
+	reply := node.HandleAppendEntries(args)
+
+	return &proto.AppendEntriesResponse{
+		Term:          reply.Term,
+		Success:       reply.Success,
+		ConflictIndex: reply.ConflictIndex,
+		ConflictTerm:  reply.ConflictTerm,
+	}, nil
+}
+
+func (s *raftServer) InstallSnapshot(ctx context.Context, req *proto.InstallSnapshotRequest) (*proto.InstallSnapshotResponse, error) {
+	s.transport.mu.RLock()
+	node := s.transport.node
+	s.transport.mu.RUnlock()
+
+	if node == nil {
+		return nil, fmt.Errorf("node not set")
+	}
+
+	args := &raft.InstallSnapshotArgs{
+		Term:              req.Term,
+		LeaderID:          req.LeaderId,
+		LastIncludedIndex: req.LastIncludedIndex,
+		LastIncludedTerm:  req.LastIncludedTerm,
+		Data:              req.Data,
+	}
+
+	reply := node.HandleInstallSnapshot(args)
+
+	return &proto.InstallSnapshotResponse{
+		Term: reply.Term,
+	}, nil
+}
\ No newline at end of file
diff --git a/repository_after/pkg/kv/store.go b/repository_after/pkg/kv/store.go
new file mode 100644
index 0000000..f3c12cf
--- /dev/null
+++ b/repository_after/pkg/kv/store.go
@@ -0,0 +1,121 @@
+package kv
+
+import (
+	"sync"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+// Store implements a simple in-memory key-value store
+type Store struct {
+	mu   sync.RWMutex
+	data map[string]string
+}
+
+// NewStore creates a new KV store
+func NewStore() *Store {
+	return &Store{
+		data: make(map[string]string),
+	}
+}
+
+// Apply applies a command to the state machine
+func (s *Store) Apply(cmd raft.Command) string {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	switch cmd.Type {
+	case raft.CommandSet:
+		s.data[cmd.Key] = cmd.Value
+		return cmd.Value
+	case raft.CommandDelete:
+		delete(s.data, cmd.Key)
+		return ""
+	case raft.CommandNoop:
+		return ""
+	case raft.CommandAddNode, raft.CommandRemoveNode:
+		// Membership changes are handled by the Raft node
+		return ""
+	default:
+		return ""
+	}
+}
+
+// Get retrieves a value from the store
+func (s *Store) Get(key string) (string, bool) {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+	value, ok := s.data[key]
+	return value, ok
+}
+
+// Exists checks if a key exists in the store
+func (s *Store) Exists(key string) bool {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+	_, ok := s.data[key]
+	return ok
+}
+
+// Set stores a key-value pair
+func (s *Store) Set(key, value string) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+	s.data[key] = value
+}
+
+// Delete removes a key from the store
+func (s *Store) Delete(key string) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+	delete(s.data, key)
+}
+
+// GetSnapshot returns a copy of the current state
+func (s *Store) GetSnapshot() map[string]string {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+
+	snapshot := make(map[string]string, len(s.data))
+	for k, v := range s.data {
+		snapshot[k] = v
+	}
+	return snapshot
+}
+
+// RestoreSnapshot restores state from a snapshot
+func (s *Store) RestoreSnapshot(data map[string]string) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	s.data = make(map[string]string, len(data))
+	for k, v := range data {
+		s.data[k] = v
+	}
+}
+
+// Size returns the number of keys in the store
+func (s *Store) Size() int {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+	return len(s.data)
+}
+
+// Clear removes all keys from the store
+func (s *Store) Clear() {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+	s.data = make(map[string]string)
+}
+
+// Keys returns all keys in the store
+func (s *Store) Keys() []string {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+
+	keys := make([]string, 0, len(s.data))
+	for k := range s.data {
+		keys = append(keys, k)
+	}
+	return keys
+}
\ No newline at end of file
diff --git a/repository_after/pkg/raft/errors.go b/repository_after/pkg/raft/errors.go
new file mode 100644
index 0000000..f527f1c
--- /dev/null
+++ b/repository_after/pkg/raft/errors.go
@@ -0,0 +1,15 @@
+package raft
+
+import "errors"
+
+var (
+	ErrNotLeader               = errors.New("not the leader")
+	ErrTimeout                 = errors.New("operation timed out")
+	ErrNodeNotFound            = errors.New("node not found")
+	ErrLogCompacted            = errors.New("log has been compacted")
+	ErrSnapshotFailed          = errors.New("snapshot operation failed")
+	ErrMembershipChangePending = errors.New("membership change already in progress")
+	ErrNodeStopped             = errors.New("node has been stopped")
+	ErrNodeAlreadyExists       = errors.New("node already exists in cluster")
+	ErrCannotRemoveLastNode    = errors.New("cannot remove the last node from cluster")
+)
\ No newline at end of file
diff --git a/repository_after/pkg/raft/node.go b/repository_after/pkg/raft/node.go
new file mode 100644
index 0000000..26ffc08
--- /dev/null
+++ b/repository_after/pkg/raft/node.go
@@ -0,0 +1,1447 @@
+package raft
+
+import (
+	"context"
+	"encoding/json"
+	"log"
+	"math/rand"
+	"sort"
+	"sync"
+	"sync/atomic"
+	"time"
+)
+
+type Node struct {
+	mu sync.RWMutex
+
+	id     string
+	config NodeConfig
+
+	// Persistent state
+	currentTerm uint64
+	votedFor    string
+	log         []LogEntry
+
+	// Volatile state
+	state       NodeState
+	commitIndex uint64
+	lastApplied uint64
+
+	// Leader state
+	nextIndex  map[string]uint64
+	matchIndex map[string]uint64
+
+	// Cluster configuration
+	cluster *ClusterConfig
+
+	// Channels
+	applyCh    chan ApplyMsg
+	commitCh   chan struct{}
+	stopCh     chan struct{}
+	resetTimer chan struct{}
+
+	// Pending operations
+	pendingCommands map[uint64]*PendingCommand
+
+	// Components
+	transport    Transport
+	wal          WALInterface
+	stateMachine StateMachineInterface
+
+	// Snapshot state
+	snapshot           *Snapshot
+	snapshotThreshold  uint64
+	snapshotInProgress int32
+
+	// Leader tracking
+	leaderID string
+
+	// Shutdown state
+	stopped int32
+
+	// Membership change state
+	membershipChangePending bool
+	membershipChangeIndex   uint64
+}
+
+// WALInterface defines the interface for write-ahead log
+type WALInterface interface {
+	Save(state *PersistentState) error
+	Load() (*PersistentState, error)
+	SaveSnapshot(snapshot *Snapshot) error
+	LoadSnapshot() (*Snapshot, error)
+	Close() error
+	Size() (int64, error)
+}
+
+// StateMachineInterface defines the interface for the state machine
+type StateMachineInterface interface {
+	Apply(cmd Command) string
+	Get(key string) (string, bool)
+	GetSnapshot() map[string]string
+	RestoreSnapshot(data map[string]string)
+}
+
+func NewNode(config NodeConfig, transport Transport, wal WALInterface, stateMachine StateMachineInterface) *Node {
+	n := &Node{
+		id:                config.ID,
+		config:            config,
+		currentTerm:       0,
+		votedFor:          "",
+		log:               make([]LogEntry, 0),
+		state:             Follower,
+		commitIndex:       0,
+		lastApplied:       0,
+		nextIndex:         make(map[string]uint64),
+		matchIndex:        make(map[string]uint64),
+		cluster:           NewClusterConfig(),
+		applyCh:           make(chan ApplyMsg, 100),
+		commitCh:          make(chan struct{}, 1),
+		stopCh:            make(chan struct{}),
+		resetTimer:        make(chan struct{}, 1),
+		pendingCommands:   make(map[uint64]*PendingCommand),
+		transport:         transport,
+		wal:               wal,
+		stateMachine:      stateMachine,
+		snapshotThreshold: config.SnapshotThreshold,
+	}
+
+	n.cluster.AddNode(config.ID)
+	for _, peer := range config.Peers {
+		n.cluster.AddNode(peer)
+	}
+
+	// Initialize log with dummy entry at index 0
+	n.log = append(n.log, LogEntry{Index: 0, Term: 0, Command: Command{Type: CommandNoop}})
+
+	return n
+}
+
+func (n *Node) Start() error {
+	if err := n.restore(); err != nil {
+		log.Printf("Node %s: Failed to restore state: %v", n.id, err)
+	}
+
+	go n.run()
+	go n.applyLoop()
+
+	return nil
+}
+
+func (n *Node) Stop() {
+	if !atomic.CompareAndSwapInt32(&n.stopped, 0, 1) {
+		return // Already stopped
+	}
+	close(n.stopCh)
+
+	// Wait a bit for loops to exit
+	time.Sleep(50 * time.Millisecond)
+
+	if n.wal != nil {
+		n.wal.Close()
+	}
+}
+
+func (n *Node) isStopped() bool {
+	return atomic.LoadInt32(&n.stopped) == 1
+}
+
+func (n *Node) run() {
+	for {
+		if n.isStopped() {
+			return
+		}
+
+		select {
+		case <-n.stopCh:
+			return
+		default:
+		}
+
+		n.mu.RLock()
+		state := n.state
+		n.mu.RUnlock()
+
+		switch state {
+		case Follower:
+			n.runFollower()
+		case Candidate:
+			n.runCandidate()
+		case Leader:
+			n.runLeader()
+		}
+	}
+}
+
+func (n *Node) runFollower() {
+	timer := time.NewTimer(n.randomElectionTimeout())
+	defer timer.Stop()
+
+	for {
+		select {
+		case <-n.stopCh:
+			return
+		case <-timer.C:
+			n.mu.Lock()
+			if n.state == Follower {
+				log.Printf("Node %s: Election timeout, becoming candidate", n.id)
+				n.state = Candidate
+			}
+			n.mu.Unlock()
+			return
+		case <-n.resetTimer:
+			if !timer.Stop() {
+				select {
+				case <-timer.C:
+				default:
+				}
+			}
+			timer.Reset(n.randomElectionTimeout())
+		}
+	}
+}
+
+func (n *Node) runCandidate() {
+	n.mu.Lock()
+	n.currentTerm++
+	n.votedFor = n.id
+	currentTerm := n.currentTerm
+	lastLogIndex := n.getLastLogIndex()
+	lastLogTerm := n.getLastLogTerm()
+	n.persist()
+	n.mu.Unlock()
+
+	log.Printf("Node %s: Starting election for term %d", n.id, currentTerm)
+
+	votesReceived := int32(1)
+	votesNeeded := int32(n.cluster.Size()/2 + 1)
+	electionWon := make(chan struct{}, 1)
+
+	peers := n.cluster.GetNodes()
+	for _, peer := range peers {
+		if peer == n.id {
+			continue
+		}
+
+		go func(peer string) {
+			args := &RequestVoteArgs{
+				Term:         currentTerm,
+				CandidateID:  n.id,
+				LastLogIndex: lastLogIndex,
+				LastLogTerm:  lastLogTerm,
+			}
+
+			reply, err := n.transport.RequestVote(peer, args)
+			if err != nil {
+				return
+			}
+
+			n.mu.Lock()
+			defer n.mu.Unlock()
+
+			if reply.Term > n.currentTerm {
+				n.becomeFollower(reply.Term)
+				return
+			}
+
+			if n.state != Candidate || n.currentTerm != currentTerm {
+				return
+			}
+
+			if reply.VoteGranted {
+				votes := atomic.AddInt32(&votesReceived, 1)
+				if votes >= votesNeeded {
+					select {
+					case electionWon <- struct{}{}:
+					default:
+					}
+				}
+			}
+		}(peer)
+	}
+
+	timer := time.NewTimer(n.randomElectionTimeout())
+	defer timer.Stop()
+
+	select {
+	case <-n.stopCh:
+		return
+	case <-electionWon:
+		n.mu.Lock()
+		if n.state == Candidate && n.currentTerm == currentTerm {
+			n.becomeLeader()
+		}
+		n.mu.Unlock()
+	case <-timer.C:
+		log.Printf("Node %s: Election timeout, restarting", n.id)
+	case <-n.resetTimer:
+		n.mu.Lock()
+		if n.state == Candidate {
+			n.state = Follower
+		}
+		n.mu.Unlock()
+	}
+}
+
+func (n *Node) runLeader() {
+	n.sendHeartbeats()
+
+	ticker := time.NewTicker(n.config.HeartbeatInterval)
+	defer ticker.Stop()
+
+	for {
+		select {
+		case <-n.stopCh:
+			return
+		case <-ticker.C:
+			n.mu.RLock()
+			isLeader := n.state == Leader
+			n.mu.RUnlock()
+
+			if !isLeader {
+				return
+			}
+
+			n.sendHeartbeats()
+			n.advanceCommitIndex()
+			n.maybeSnapshot()
+		}
+	}
+}
+
+func (n *Node) sendHeartbeats() {
+	n.mu.RLock()
+	if n.state != Leader {
+		n.mu.RUnlock()
+		return
+	}
+	currentTerm := n.currentTerm
+	commitIndex := n.commitIndex
+	n.mu.RUnlock()
+
+	peers := n.cluster.GetNodes()
+	for _, peer := range peers {
+		if peer == n.id {
+			continue
+		}
+		go n.sendAppendEntries(peer, currentTerm, commitIndex)
+	}
+}
+
+func (n *Node) sendAppendEntries(peer string, term uint64, leaderCommit uint64) {
+	n.mu.RLock()
+	if n.state != Leader || n.currentTerm != term {
+		n.mu.RUnlock()
+		return
+	}
+
+	nextIdx := n.nextIndex[peer]
+	if nextIdx == 0 {
+		nextIdx = n.getLastLogIndex() + 1
+	}
+
+	snapshotIdx := uint64(0)
+	if n.snapshot != nil {
+		snapshotIdx = n.snapshot.LastIncludedIndex
+	}
+
+	if snapshotIdx > 0 && nextIdx <= snapshotIdx {
+		n.mu.RUnlock()
+		n.sendSnapshot(peer)
+		return
+	}
+
+	prevLogIndex := nextIdx - 1
+	prevLogTerm := uint64(0)
+
+	if prevLogIndex > 0 {
+		if snapshotIdx > 0 && prevLogIndex == snapshotIdx {
+			prevLogTerm = n.snapshot.LastIncludedTerm
+		} else {
+			logIdx := n.logIndexToArrayIndex(prevLogIndex)
+			if logIdx >= 0 && logIdx < len(n.log) {
+				prevLogTerm = n.log[logIdx].Term
+			}
+		}
+	}
+
+	entries := make([]LogEntry, 0)
+	startIdx := n.logIndexToArrayIndex(nextIdx)
+	if startIdx >= 0 && startIdx < len(n.log) {
+		entries = append(entries, n.log[startIdx:]...)
+	}
+
+	args := &AppendEntriesArgs{
+		Term:         term,
+		LeaderID:     n.id,
+		PrevLogIndex: prevLogIndex,
+		PrevLogTerm:  prevLogTerm,
+		Entries:      entries,
+		LeaderCommit: leaderCommit,
+	}
+	n.mu.RUnlock()
+
+	reply, err := n.transport.AppendEntries(peer, args)
+	if err != nil {
+		return
+	}
+
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	if reply.Term > n.currentTerm {
+		n.becomeFollower(reply.Term)
+		return
+	}
+
+	if n.state != Leader || n.currentTerm != term {
+		return
+	}
+
+	if reply.Success {
+		newNextIndex := nextIdx + uint64(len(entries))
+		if newNextIndex > n.nextIndex[peer] {
+			n.nextIndex[peer] = newNextIndex
+		}
+		newMatchIndex := newNextIndex - 1
+		if newMatchIndex > n.matchIndex[peer] {
+			n.matchIndex[peer] = newMatchIndex
+		}
+		n.tryAdvanceCommitIndex()
+	} else {
+		if reply.ConflictTerm > 0 {
+			lastIndex := uint64(0)
+			for i := len(n.log) - 1; i >= 0; i-- {
+				if n.log[i].Term == reply.ConflictTerm {
+					lastIndex = n.log[i].Index
+					break
+				}
+			}
+			if lastIndex > 0 {
+				n.nextIndex[peer] = lastIndex + 1
+			} else {
+				n.nextIndex[peer] = reply.ConflictIndex
+			}
+		} else if reply.ConflictIndex > 0 {
+			n.nextIndex[peer] = reply.ConflictIndex
+		} else if n.nextIndex[peer] > 1 {
+			n.nextIndex[peer]--
+		}
+	}
+}
+
+func (n *Node) logIndexToArrayIndex(logIndex uint64) int {
+	if len(n.log) == 0 {
+		return -1
+	}
+	baseIndex := n.log[0].Index
+	if logIndex < baseIndex {
+		return -1
+	}
+	idx := int(logIndex - baseIndex)
+	if idx >= len(n.log) {
+		return -1
+	}
+	return idx
+}
+
+func (n *Node) sendSnapshot(peer string) {
+	n.mu.RLock()
+	if n.state != Leader || n.snapshot == nil {
+		n.mu.RUnlock()
+		return
+	}
+
+	snapshotData, err := json.Marshal(n.snapshot.Data)
+	if err != nil {
+		n.mu.RUnlock()
+		log.Printf("Node %s: Failed to marshal snapshot: %v", n.id, err)
+		return
+	}
+
+	args := &InstallSnapshotArgs{
+		Term:              n.currentTerm,
+		LeaderID:          n.id,
+		LastIncludedIndex: n.snapshot.LastIncludedIndex,
+		LastIncludedTerm:  n.snapshot.LastIncludedTerm,
+		Data:              snapshotData,
+	}
+	n.mu.RUnlock()
+
+	reply, err := n.transport.InstallSnapshot(peer, args)
+	if err != nil {
+		return
+	}
+
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	if reply.Term > n.currentTerm {
+		n.becomeFollower(reply.Term)
+		return
+	}
+
+	n.nextIndex[peer] = args.LastIncludedIndex + 1
+	n.matchIndex[peer] = args.LastIncludedIndex
+}
+
+// tryAdvanceCommitIndex - fixed majority calculation for even cluster sizes
+func (n *Node) tryAdvanceCommitIndex() {
+	if n.state != Leader {
+		return
+	}
+
+	clusterSize := n.cluster.Size()
+	if clusterSize == 0 {
+		return
+	}
+
+	// Collect all match indices including self
+	matchIndices := make([]uint64, 0, clusterSize)
+	matchIndices = append(matchIndices, n.getLastLogIndex()) // Leader's own log
+
+	for _, peer := range n.cluster.GetNodes() {
+		if peer == n.id {
+			continue
+		}
+		matchIndices = append(matchIndices, n.matchIndex[peer])
+	}
+
+	// Sort ascending
+	sort.Slice(matchIndices, func(i, j int) bool {
+		return matchIndices[i] < matchIndices[j]
+	})
+
+	// For majority: need (clusterSize / 2) + 1 nodes to have replicated
+	// The median index for majority is at position: clusterSize - (clusterSize/2 + 1)
+	// Which simplifies to: (clusterSize - 1) / 2
+	majorityIdx := (clusterSize - 1) / 2
+	if majorityIdx >= len(matchIndices) {
+		return
+	}
+
+	// With ascending sort, we want the index that at least majority have
+	// This is at position len - majority = len - (len/2 + 1)
+	newCommitIndex := matchIndices[len(matchIndices)-1-majorityIdx]
+
+	if newCommitIndex > n.commitIndex {
+		logIdx := n.logIndexToArrayIndex(newCommitIndex)
+		if logIdx >= 0 && logIdx < len(n.log) && n.log[logIdx].Term == n.currentTerm {
+			oldCommit := n.commitIndex
+			n.commitIndex = newCommitIndex
+			log.Printf("Node %s: Committed index %d (was %d)", n.id, newCommitIndex, oldCommit)
+
+			// Check if membership change was committed
+			if n.membershipChangePending && n.membershipChangeIndex <= newCommitIndex {
+				n.membershipChangePending = false
+				log.Printf("Node %s: Membership change at index %d committed", n.id, n.membershipChangeIndex)
+			}
+
+			select {
+			case n.commitCh <- struct{}{}:
+			default:
+			}
+		}
+	}
+}
+
+func (n *Node) advanceCommitIndex() {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+	n.tryAdvanceCommitIndex()
+}
+
+func (n *Node) HandleRequestVote(args *RequestVoteArgs) *RequestVoteReply {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	reply := &RequestVoteReply{
+		Term:        n.currentTerm,
+		VoteGranted: false,
+	}
+
+	if args.Term < n.currentTerm {
+		return reply
+	}
+
+	if args.Term > n.currentTerm {
+		n.becomeFollower(args.Term)
+	}
+
+	reply.Term = n.currentTerm
+
+	if (n.votedFor == "" || n.votedFor == args.CandidateID) && n.isLogUpToDate(args.LastLogIndex, args.LastLogTerm) {
+		n.votedFor = args.CandidateID
+		reply.VoteGranted = true
+		n.persist()
+		n.signalResetTimer()
+		log.Printf("Node %s: Granted vote to %s for term %d", n.id, args.CandidateID, args.Term)
+	}
+
+	return reply
+}
+
+func (n *Node) HandleAppendEntries(args *AppendEntriesArgs) *AppendEntriesReply {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	reply := &AppendEntriesReply{
+		Term:    n.currentTerm,
+		Success: false,
+	}
+
+	if args.Term < n.currentTerm {
+		return reply
+	}
+
+	if args.Term > n.currentTerm || n.state == Candidate {
+		n.becomeFollower(args.Term)
+	}
+
+	n.leaderID = args.LeaderID
+	n.signalResetTimer()
+
+	reply.Term = n.currentTerm
+
+	if args.PrevLogIndex > 0 {
+		logIdx := n.logIndexToArrayIndex(args.PrevLogIndex)
+		if logIdx < 0 {
+			// Log doesn't have this index
+			reply.ConflictIndex = n.getLastLogIndex() + 1
+			reply.ConflictTerm = 0
+			return reply
+		}
+
+		if n.log[logIdx].Term != args.PrevLogTerm {
+			conflictTerm := n.log[logIdx].Term
+			reply.ConflictTerm = conflictTerm
+
+			for i := logIdx; i >= 0; i-- {
+				if n.log[i].Term != conflictTerm {
+					reply.ConflictIndex = n.log[i+1].Index
+					break
+				}
+				if i == 0 {
+					reply.ConflictIndex = n.log[0].Index
+				}
+			}
+			return reply
+		}
+	}
+
+	for i, entry := range args.Entries {
+		logIdx := n.logIndexToArrayIndex(args.PrevLogIndex + 1 + uint64(i))
+		if logIdx >= 0 && logIdx < len(n.log) {
+			if n.log[logIdx].Term != entry.Term {
+				n.log = n.log[:logIdx]
+				n.log = append(n.log, entry)
+			}
+		} else {
+			n.log = append(n.log, entry)
+		}
+
+		// Apply membership changes from replicated entries
+		if entry.Command.Type == CommandAddNode {
+			n.cluster.AddNode(entry.Command.Key)
+		} else if entry.Command.Type == CommandRemoveNode {
+			n.cluster.RemoveNode(entry.Command.Key)
+		}
+	}
+
+	if len(args.Entries) > 0 {
+		n.persist()
+	}
+
+	if args.LeaderCommit > n.commitIndex {
+		lastNewIndex := args.PrevLogIndex + uint64(len(args.Entries))
+		if args.LeaderCommit < lastNewIndex {
+			n.commitIndex = args.LeaderCommit
+		} else {
+			n.commitIndex = lastNewIndex
+		}
+		select {
+		case n.commitCh <- struct{}{}:
+		default:
+		}
+	}
+
+	reply.Success = true
+	return reply
+}
+
+func (n *Node) HandleInstallSnapshot(args *InstallSnapshotArgs) *InstallSnapshotReply {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	reply := &InstallSnapshotReply{
+		Term: n.currentTerm,
+	}
+
+	if args.Term < n.currentTerm {
+		return reply
+	}
+
+	if args.Term > n.currentTerm {
+		n.becomeFollower(args.Term)
+	}
+
+	n.leaderID = args.LeaderID
+	n.signalResetTimer()
+
+	var snapshotData map[string]string
+	if err := json.Unmarshal(args.Data, &snapshotData); err != nil {
+		log.Printf("Node %s: Failed to unmarshal snapshot: %v", n.id, err)
+		return reply
+	}
+
+	n.log = []LogEntry{{
+		Index:   args.LastIncludedIndex,
+		Term:    args.LastIncludedTerm,
+		Command: Command{Type: CommandNoop},
+	}}
+
+	n.snapshot = &Snapshot{
+		LastIncludedIndex: args.LastIncludedIndex,
+		LastIncludedTerm:  args.LastIncludedTerm,
+		Data:              snapshotData,
+	}
+
+	if args.LastIncludedIndex > n.commitIndex {
+		n.commitIndex = args.LastIncludedIndex
+	}
+	if args.LastIncludedIndex > n.lastApplied {
+		n.lastApplied = args.LastIncludedIndex
+	}
+
+	n.stateMachine.RestoreSnapshot(snapshotData)
+	n.persist()
+
+	if n.wal != nil {
+		if err := n.wal.SaveSnapshot(n.snapshot); err != nil {
+			log.Printf("Node %s: Failed to save snapshot to WAL: %v", n.id, err)
+		}
+	}
+
+	log.Printf("Node %s: Installed snapshot at index %d", n.id, args.LastIncludedIndex)
+
+	return reply
+}
+
+func (n *Node) Submit(cmd Command) (uint64, uint64, bool) {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	if n.state != Leader {
+		return 0, 0, false
+	}
+
+	entry := LogEntry{
+		Index:   n.getLastLogIndex() + 1,
+		Term:    n.currentTerm,
+		Command: cmd,
+	}
+
+	n.log = append(n.log, entry)
+	n.persist()
+
+	log.Printf("Node %s: Appended entry %d", n.id, entry.Index)
+
+	return entry.Index, entry.Term, true
+}
+
+func (n *Node) SubmitWithResult(ctx context.Context, cmd Command) (CommitResult, error) {
+	index, term, isLeader := n.Submit(cmd)
+	if !isLeader {
+		return CommitResult{}, ErrNotLeader
+	}
+
+	resultCh := make(chan CommitResult, 1)
+	pending := &PendingCommand{
+		Index:    index,
+		Term:     term,
+		ResultCh: resultCh,
+	}
+
+	n.mu.Lock()
+	n.pendingCommands[index] = pending
+	n.mu.Unlock()
+
+	select {
+	case result := <-resultCh:
+		if result.Error != nil {
+			return result, result.Error
+		}
+		return result, nil
+	case <-ctx.Done():
+		n.mu.Lock()
+		delete(n.pendingCommands, index)
+		n.mu.Unlock()
+		return CommitResult{}, ctx.Err()
+	case <-n.stopCh:
+		n.mu.Lock()
+		delete(n.pendingCommands, index)
+		n.mu.Unlock()
+		return CommitResult{}, ErrNodeStopped
+	}
+}
+
+// Read performs a linearizable read with proper timeout handling
+func (n *Node) Read(ctx context.Context, key string) (string, error) {
+	// Handle nil context
+	if ctx == nil {
+		var cancel context.CancelFunc
+		ctx, cancel = context.WithTimeout(context.Background(), 5*time.Second)
+		defer cancel()
+	}
+
+	n.mu.Lock()
+	if n.state != Leader {
+		n.mu.Unlock()
+		return "", ErrNotLeader
+	}
+
+	readIdx := n.commitIndex
+	currentTerm := n.currentTerm
+	n.mu.Unlock()
+
+	// Use context deadline if available, otherwise use default timeout
+	timeout := 10 * time.Second
+	if deadline, ok := ctx.Deadline(); ok {
+		timeout = time.Until(deadline)
+		if timeout <= 0 {
+			return "", context.DeadlineExceeded
+		}
+	}
+
+	// Confirm leadership with heartbeat quorum
+	if !n.confirmLeadership(currentTerm, timeout) {
+		return "", ErrNotLeader
+	}
+
+	// Wait for apply index to catch up
+	deadline := time.Now().Add(timeout)
+	for {
+		n.mu.RLock()
+		lastApplied := n.lastApplied
+		n.mu.RUnlock()
+
+		if lastApplied >= readIdx {
+			break
+		}
+
+		if time.Now().After(deadline) {
+			return "", ErrTimeout
+		}
+
+		select {
+		case <-ctx.Done():
+			return "", ctx.Err()
+		case <-n.stopCh:
+			return "", ErrNodeStopped
+		case <-time.After(10 * time.Millisecond):
+		}
+	}
+
+	value, _ := n.stateMachine.Get(key)
+	return value, nil
+}
+
+// confirmLeadership - with configurable timeout
+func (n *Node) confirmLeadership(term uint64, timeout time.Duration) bool {
+	n.mu.RLock()
+	if n.state != Leader || n.currentTerm != term {
+		n.mu.RUnlock()
+		return false
+	}
+	peers := n.cluster.GetNodes()
+	needed := n.cluster.Size()/2 + 1
+	n.mu.RUnlock()
+
+	if needed <= 1 {
+		// Single node cluster
+		return true
+	}
+
+	ackCount := int32(1) // Count self
+	done := make(chan struct{}, 1)
+
+	for _, peer := range peers {
+		if peer == n.id {
+			continue
+		}
+
+		go func(peer string) {
+			n.mu.RLock()
+			if n.state != Leader {
+				n.mu.RUnlock()
+				return
+			}
+			args := &AppendEntriesArgs{
+				Term:         n.currentTerm,
+				LeaderID:     n.id,
+				PrevLogIndex: n.getLastLogIndex(),
+				PrevLogTerm:  n.getLastLogTerm(),
+				Entries:      nil,
+				LeaderCommit: n.commitIndex,
+			}
+			n.mu.RUnlock()
+
+			reply, err := n.transport.AppendEntries(peer, args)
+			if err != nil {
+				return
+			}
+
+			if reply.Success {
+				if atomic.AddInt32(&ackCount, 1) >= int32(needed) {
+					select {
+					case done <- struct{}{}:
+					default:
+					}
+				}
+			}
+		}(peer)
+	}
+
+	// Use reasonable timeout: min of provided timeout and 5x heartbeat
+	confirmTimeout := timeout
+	if maxTimeout := n.config.HeartbeatInterval * 5; confirmTimeout > maxTimeout {
+		confirmTimeout = maxTimeout
+	}
+
+	select {
+	case <-done:
+		return true
+	case <-time.After(confirmTimeout):
+		return atomic.LoadInt32(&ackCount) >= int32(needed)
+	case <-n.stopCh:
+		return false
+	}
+}
+
+// AddNode adds a new node to the cluster (single-server membership change)
+func (n *Node) AddNode(nodeID string) error {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	if n.state != Leader {
+		return ErrNotLeader
+	}
+
+	// Check if there's already a pending membership change
+	if n.membershipChangePending {
+		return ErrMembershipChangePending
+	}
+
+	// Check if node already exists
+	if n.cluster.HasNode(nodeID) {
+		return ErrNodeAlreadyExists
+	}
+
+	// Create membership change command
+	cmd := Command{
+		Type: CommandAddNode,
+		Key:  nodeID,
+	}
+
+	entry := LogEntry{
+		Index:   n.getLastLogIndex() + 1,
+		Term:    n.currentTerm,
+		Command: cmd,
+	}
+
+	n.log = append(n.log, entry)
+	n.membershipChangePending = true
+	n.membershipChangeIndex = entry.Index
+
+	// Add node to cluster immediately (leader uses new config right away)
+	n.cluster.AddNode(nodeID)
+	n.nextIndex[nodeID] = entry.Index + 1
+	n.matchIndex[nodeID] = 0
+
+	n.persist()
+
+	log.Printf("Node %s: Adding node %s at index %d", n.id, nodeID, entry.Index)
+
+	return nil
+}
+
+// RemoveNode removes a node from the cluster (single-server membership change)
+func (n *Node) RemoveNode(nodeID string) error {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	if n.state != Leader {
+		return ErrNotLeader
+	}
+
+	// Check if there's already a pending membership change
+	if n.membershipChangePending {
+		return ErrMembershipChangePending
+	}
+
+	// Check if node exists
+	if !n.cluster.HasNode(nodeID) {
+		return ErrNodeNotFound
+	}
+
+	// Cannot remove self if only node
+	if nodeID == n.id && n.cluster.Size() == 1 {
+		return ErrCannotRemoveLastNode
+	}
+
+	// Create membership change command
+	cmd := Command{
+		Type: CommandRemoveNode,
+		Key:  nodeID,
+	}
+
+	entry := LogEntry{
+		Index:   n.getLastLogIndex() + 1,
+		Term:    n.currentTerm,
+		Command: cmd,
+	}
+
+	n.log = append(n.log, entry)
+	n.membershipChangePending = true
+	n.membershipChangeIndex = entry.Index
+
+	// Remove node from cluster immediately (leader uses new config right away)
+	n.cluster.RemoveNode(nodeID)
+	delete(n.nextIndex, nodeID)
+	delete(n.matchIndex, nodeID)
+
+	n.persist()
+
+	log.Printf("Node %s: Removing node %s at index %d", n.id, nodeID, entry.Index)
+
+	// If removing self, step down after committing
+	if nodeID == n.id {
+		go func() {
+			time.Sleep(100 * time.Millisecond)
+			n.mu.Lock()
+			if n.commitIndex >= entry.Index {
+				n.becomeFollower(n.currentTerm)
+			}
+			n.mu.Unlock()
+		}()
+	}
+
+	return nil
+}
+
+// AddNodeWithContext adds a node and waits for commitment
+func (n *Node) AddNodeWithContext(ctx context.Context, nodeID string) error {
+	err := n.AddNode(nodeID)
+	if err != nil {
+		return err
+	}
+
+	// Wait for the membership change to be committed
+	n.mu.RLock()
+	changeIndex := n.membershipChangeIndex
+	n.mu.RUnlock()
+
+	return n.waitForCommit(ctx, changeIndex)
+}
+
+// RemoveNodeWithContext removes a node and waits for commitment
+func (n *Node) RemoveNodeWithContext(ctx context.Context, nodeID string) error {
+	err := n.RemoveNode(nodeID)
+	if err != nil {
+		return err
+	}
+
+	// Wait for the membership change to be committed
+	n.mu.RLock()
+	changeIndex := n.membershipChangeIndex
+	n.mu.RUnlock()
+
+	return n.waitForCommit(ctx, changeIndex)
+}
+
+func (n *Node) waitForCommit(ctx context.Context, index uint64) error {
+	ticker := time.NewTicker(50 * time.Millisecond)
+	defer ticker.Stop()
+
+	for {
+		select {
+		case <-ctx.Done():
+			return ctx.Err()
+		case <-n.stopCh:
+			return ErrNodeStopped
+		case <-ticker.C:
+			n.mu.RLock()
+			committed := n.commitIndex >= index
+			isLeader := n.state == Leader
+			n.mu.RUnlock()
+
+			if committed {
+				return nil
+			}
+			if !isLeader {
+				return ErrNotLeader
+			}
+		}
+	}
+}
+
+func (n *Node) applyLoop() {
+	for {
+		select {
+		case <-n.stopCh:
+			return
+		case <-n.commitCh:
+			n.applyCommitted()
+		case <-time.After(100 * time.Millisecond):
+			n.applyCommitted()
+		}
+	}
+}
+
+func (n *Node) applyCommitted() {
+	n.mu.Lock()
+	commitIndex := n.commitIndex
+	lastApplied := n.lastApplied
+	n.mu.Unlock()
+
+	for i := lastApplied + 1; i <= commitIndex; i++ {
+		n.mu.RLock()
+		arrIdx := n.logIndexToArrayIndex(i)
+		if arrIdx < 0 || arrIdx >= len(n.log) {
+			n.mu.RUnlock()
+			// Entry may be in snapshot, skip
+			n.mu.Lock()
+			if i > n.lastApplied {
+				n.lastApplied = i
+			}
+			n.mu.Unlock()
+			continue
+		}
+		entry := n.log[arrIdx]
+		n.mu.RUnlock()
+
+		var result string
+		// Apply membership changes
+		if entry.Command.Type == CommandAddNode || entry.Command.Type == CommandRemoveNode {
+			// Membership changes are applied during replication
+			result = ""
+		} else {
+			result = n.stateMachine.Apply(entry.Command)
+		}
+
+		select {
+		case n.applyCh <- ApplyMsg{
+			CommandValid: true,
+			Command:      entry.Command,
+			CommandIndex: entry.Index,
+			CommandTerm:  entry.Term,
+		}:
+		default:
+			// Channel full, drop message
+		}
+
+		n.mu.Lock()
+		n.lastApplied = i
+
+		if n.state == Leader {
+			if pending, ok := n.pendingCommands[i]; ok {
+				commitResult := CommitResult{
+					Index: i,
+					Term:  entry.Term,
+					Value: result,
+				}
+				select {
+				case pending.ResultCh <- commitResult:
+				default:
+				}
+				delete(n.pendingCommands, i)
+			}
+		}
+		n.mu.Unlock()
+	}
+}
+
+func (n *Node) maybeSnapshot() {
+	if atomic.LoadInt32(&n.snapshotInProgress) == 1 {
+		return
+	}
+
+	if n.wal == nil {
+		return
+	}
+
+	size, err := n.wal.Size()
+	if err != nil {
+		return
+	}
+
+	// Snapshot when WAL exceeds threshold bytes
+	thresholdBytes := int64(n.snapshotThreshold) * 10000 // ~1MB per 100 threshold
+	if size > thresholdBytes {
+		go func() {
+			if atomic.CompareAndSwapInt32(&n.snapshotInProgress, 0, 1) {
+				defer atomic.StoreInt32(&n.snapshotInProgress, 0)
+				n.mu.RLock()
+				lastApplied := n.lastApplied
+				n.mu.RUnlock()
+				if err := n.CreateSnapshot(lastApplied); err != nil {
+					log.Printf("Node %s: Snapshot creation failed: %v", n.id, err)
+				}
+			}
+		}()
+	}
+}
+
+// CreateSnapshot - fixed index check
+func (n *Node) CreateSnapshot(index uint64) error {
+	n.mu.Lock()
+	defer n.mu.Unlock()
+
+	arrIdx := n.logIndexToArrayIndex(index)
+	// Allow arrIdx == 0 for valid entries, but not < 0
+	if arrIdx < 0 || arrIdx >= len(n.log) {
+		return nil
+	}
+
+	// Don't snapshot if it's just the dummy entry
+	if index == 0 {
+		return nil
+	}
+
+	snapshot := &Snapshot{
+		LastIncludedIndex: index,
+		LastIncludedTerm:  n.log[arrIdx].Term,
+		Data:              n.stateMachine.GetSnapshot(),
+	}
+
+	// Keep only entries after snapshot
+	n.log = n.log[arrIdx:]
+	n.log[0] = LogEntry{
+		Index:   index,
+		Term:    snapshot.LastIncludedTerm,
+		Command: Command{Type: CommandNoop},
+	}
+
+	if n.wal != nil {
+		if err := n.wal.SaveSnapshot(snapshot); err != nil {
+			log.Printf("Node %s: Failed to save snapshot: %v", n.id, err)
+			return err
+		}
+		// Also persist the trimmed log
+		n.persist()
+	}
+
+	n.snapshot = snapshot
+	log.Printf("Node %s: Created snapshot at index %d", n.id, index)
+
+	return nil
+}
+
+func (n *Node) becomeFollower(term uint64) {
+	log.Printf("Node %s: Becoming follower for term %d", n.id, term)
+	n.state = Follower
+	n.currentTerm = term
+	n.votedFor = ""
+	n.leaderID = ""
+
+	for idx, pending := range n.pendingCommands {
+		result := CommitResult{
+			Index: idx,
+			Error: ErrNotLeader,
+		}
+		select {
+		case pending.ResultCh <- result:
+		default:
+		}
+	}
+	n.pendingCommands = make(map[uint64]*PendingCommand)
+
+	n.persist()
+	n.signalResetTimer()
+}
+
+func (n *Node) becomeLeader() {
+	log.Printf("Node %s: Becoming leader for term %d", n.id, n.currentTerm)
+	n.state = Leader
+	n.leaderID = n.id
+
+	lastLogIndex := n.getLastLogIndex()
+	for _, peer := range n.cluster.GetNodes() {
+		if peer != n.id {
+			n.nextIndex[peer] = lastLogIndex + 1
+			n.matchIndex[peer] = 0
+		}
+	}
+
+	noopEntry := LogEntry{
+		Index:   lastLogIndex + 1,
+		Term:    n.currentTerm,
+		Command: Command{Type: CommandNoop},
+	}
+	n.log = append(n.log, noopEntry)
+	n.persist()
+}
+
+func (n *Node) getLastLogIndex() uint64 {
+	if len(n.log) == 0 {
+		if n.snapshot != nil {
+			return n.snapshot.LastIncludedIndex
+		}
+		return 0
+	}
+	return n.log[len(n.log)-1].Index
+}
+
+func (n *Node) getLastLogTerm() uint64 {
+	if len(n.log) == 0 {
+		if n.snapshot != nil {
+			return n.snapshot.LastIncludedTerm
+		}
+		return 0
+	}
+	return n.log[len(n.log)-1].Term
+}
+
+func (n *Node) isLogUpToDate(lastLogIndex, lastLogTerm uint64) bool {
+	myLastTerm := n.getLastLogTerm()
+	myLastIndex := n.getLastLogIndex()
+
+	if lastLogTerm != myLastTerm {
+		return lastLogTerm > myLastTerm
+	}
+	return lastLogIndex >= myLastIndex
+}
+
+func (n *Node) randomElectionTimeout() time.Duration {
+	min := int64(n.config.ElectionTimeoutMin)
+	max := int64(n.config.ElectionTimeoutMax)
+	return time.Duration(min + rand.Int63n(max-min))
+}
+
+func (n *Node) signalResetTimer() {
+	select {
+	case n.resetTimer <- struct{}{}:
+	default:
+	}
+}
+
+func (n *Node) persist() {
+	if n.wal == nil {
+		return
+	}
+
+	state := &PersistentState{
+		CurrentTerm: n.currentTerm,
+		VotedFor:    n.votedFor,
+		Log:         n.log,
+	}
+
+	if err := n.wal.Save(state); err != nil {
+		log.Printf("Node %s: Failed to persist state: %v", n.id, err)
+	}
+}
+
+func (n *Node) restore() error {
+	if n.wal == nil {
+		return nil
+	}
+
+	snapshot, err := n.wal.LoadSnapshot()
+	if err == nil && snapshot != nil {
+		n.snapshot = snapshot
+		n.stateMachine.RestoreSnapshot(snapshot.Data)
+		n.lastApplied = snapshot.LastIncludedIndex
+		n.commitIndex = snapshot.LastIncludedIndex
+
+		n.log = []LogEntry{{
+			Index:   snapshot.LastIncludedIndex,
+			Term:    snapshot.LastIncludedTerm,
+			Command: Command{Type: CommandNoop},
+		}}
+	}
+
+	state, err := n.wal.Load()
+	if err != nil {
+		return err
+	}
+
+	if state != nil {
+		n.currentTerm = state.CurrentTerm
+		n.votedFor = state.VotedFor
+
+		if len(state.Log) > 0 {
+			if n.snapshot != nil {
+				newLog := make([]LogEntry, 0, len(state.Log))
+				for _, entry := range state.Log {
+					if entry.Index >= n.snapshot.LastIncludedIndex {
+						newLog = append(newLog, entry)
+					}
+				}
+				if len(newLog) > 0 {
+					n.log = newLog
+				}
+			} else {
+				n.log = state.Log
+			}
+		}
+	}
+
+	return nil
+}
+
+// Getters
+
+func (n *Node) GetState() (uint64, bool) {
+	n.mu.RLock()
+	defer n.mu.RUnlock()
+	return n.currentTerm, n.state == Leader
+}
+
+func (n *Node) GetLeaderID() string {
+	n.mu.RLock()
+	defer n.mu.RUnlock()
+	return n.leaderID
+}
+
+func (n *Node) GetID() string {
+	return n.id
+}
+
+func (n *Node) IsLeader() bool {
+	n.mu.RLock()
+	defer n.mu.RUnlock()
+	return n.state == Leader
+}
+
+func (n *Node) GetCommitIndex() uint64 {
+	n.mu.RLock()
+	defer n.mu.RUnlock()
+	return n.commitIndex
+}
+
+func (n *Node) GetLog() []LogEntry {
+	n.mu.RLock()
+	defer n.mu.RUnlock()
+	logCopy := make([]LogEntry, len(n.log))
+	copy(logCopy, n.log)
+	return logCopy
+}
+
+func (n *Node) GetApplyChan() <-chan ApplyMsg {
+	return n.applyCh
+}
+
+func (n *Node) GetClusterSize() int {
+	return n.cluster.Size()
+}
+
+func (n *Node) GetClusterNodes() []string {
+	return n.cluster.GetNodes()
+}
+
+// IsMembershipChangePending returns true if there's a pending membership change
+func (n *Node) IsMembershipChangePending() bool {
+	n.mu.RLock()
+	defer n.mu.RUnlock()
+	return n.membershipChangePending
+}
\ No newline at end of file
diff --git a/repository_after/pkg/raft/types.go b/repository_after/pkg/raft/types.go
new file mode 100644
index 0000000..a9360c9
--- /dev/null
+++ b/repository_after/pkg/raft/types.go
@@ -0,0 +1,221 @@
+package raft
+
+import (
+	"sync"
+	"time"
+)
+
+// NodeState represents the current state of a Raft node
+type NodeState int
+
+const (
+	Follower NodeState = iota
+	Candidate
+	Leader
+)
+
+func (s NodeState) String() string {
+	switch s {
+	case Follower:
+		return "Follower"
+	case Candidate:
+		return "Candidate"
+	case Leader:
+		return "Leader"
+	default:
+		return "Unknown"
+	}
+}
+
+// LogEntry represents a single entry in the Raft log
+type LogEntry struct {
+	Index   uint64
+	Term    uint64
+	Command Command
+}
+
+// Command represents a command to be applied to the state machine
+type Command struct {
+	Type  CommandType
+	Key   string
+	Value string
+}
+
+// CommandType represents the type of command
+type CommandType int
+
+const (
+	CommandSet CommandType = iota
+	CommandDelete
+	CommandNoop
+	CommandAddNode
+	CommandRemoveNode
+)
+
+// PersistentState represents state that must be persisted to disk
+type PersistentState struct {
+	CurrentTerm uint64
+	VotedFor    string
+	Log         []LogEntry
+}
+
+// NodeConfig holds the configuration for a Raft node
+type NodeConfig struct {
+	ID                 string
+	Peers              []string
+	ElectionTimeoutMin time.Duration
+	ElectionTimeoutMax time.Duration
+	HeartbeatInterval  time.Duration
+	WALPath            string
+	SnapshotThreshold  uint64
+}
+
+// DefaultConfig returns a default configuration
+func DefaultConfig(id string, peers []string) NodeConfig {
+	return NodeConfig{
+		ID:                 id,
+		Peers:              peers,
+		ElectionTimeoutMin: 150 * time.Millisecond,
+		ElectionTimeoutMax: 300 * time.Millisecond,
+		HeartbeatInterval:  50 * time.Millisecond,
+		WALPath:            "/tmp/raft-wal-" + id,
+		SnapshotThreshold:  1000,
+	}
+}
+
+// Snapshot represents a snapshot of the state machine
+type Snapshot struct {
+	LastIncludedIndex uint64
+	LastIncludedTerm  uint64
+	Data              map[string]string
+}
+
+// RequestVoteArgs represents arguments for RequestVote RPC
+type RequestVoteArgs struct {
+	Term         uint64
+	CandidateID  string
+	LastLogIndex uint64
+	LastLogTerm  uint64
+}
+
+// RequestVoteReply represents reply for RequestVote RPC
+type RequestVoteReply struct {
+	Term        uint64
+	VoteGranted bool
+}
+
+// AppendEntriesArgs represents arguments for AppendEntries RPC
+type AppendEntriesArgs struct {
+	Term         uint64
+	LeaderID     string
+	PrevLogIndex uint64
+	PrevLogTerm  uint64
+	Entries      []LogEntry
+	LeaderCommit uint64
+}
+
+// AppendEntriesReply represents reply for AppendEntries RPC
+type AppendEntriesReply struct {
+	Term          uint64
+	Success       bool
+	ConflictIndex uint64
+	ConflictTerm  uint64
+}
+
+// InstallSnapshotArgs represents arguments for InstallSnapshot RPC
+type InstallSnapshotArgs struct {
+	Term              uint64
+	LeaderID          string
+	LastIncludedIndex uint64
+	LastIncludedTerm  uint64
+	Data              []byte
+}
+
+// InstallSnapshotReply represents reply for InstallSnapshot RPC
+type InstallSnapshotReply struct {
+	Term uint64
+}
+
+// ApplyMsg represents a message sent to the state machine
+type ApplyMsg struct {
+	CommandValid bool
+	Command      Command
+	CommandIndex uint64
+	CommandTerm  uint64
+
+	SnapshotValid bool
+	Snapshot      []byte
+	SnapshotTerm  uint64
+	SnapshotIndex uint64
+}
+
+// Transport defines the interface for node-to-node communication
+type Transport interface {
+	RequestVote(target string, args *RequestVoteArgs) (*RequestVoteReply, error)
+	AppendEntries(target string, args *AppendEntriesArgs) (*AppendEntriesReply, error)
+	InstallSnapshot(target string, args *InstallSnapshotArgs) (*InstallSnapshotReply, error)
+}
+
+// CommitResult represents the result of committing a command
+type CommitResult struct {
+	Index uint64
+	Term  uint64
+	Value string
+	Error error
+}
+
+// PendingCommand represents a command waiting to be committed
+type PendingCommand struct {
+	Index    uint64
+	Term     uint64
+	ResultCh chan CommitResult
+}
+
+// ClusterConfig represents the current cluster configuration
+type ClusterConfig struct {
+	mu      sync.RWMutex
+	Nodes   map[string]bool
+	Version uint64
+}
+
+func NewClusterConfig() *ClusterConfig {
+	return &ClusterConfig{
+		Nodes: make(map[string]bool),
+	}
+}
+
+func (c *ClusterConfig) AddNode(nodeID string) {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	c.Nodes[nodeID] = true
+	c.Version++
+}
+
+func (c *ClusterConfig) RemoveNode(nodeID string) {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	delete(c.Nodes, nodeID)
+	c.Version++
+}
+
+func (c *ClusterConfig) GetNodes() []string {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	nodes := make([]string, 0, len(c.Nodes))
+	for node := range c.Nodes {
+		nodes = append(nodes, node)
+	}
+	return nodes
+}
+
+func (c *ClusterConfig) HasNode(nodeID string) bool {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	return c.Nodes[nodeID]
+}
+
+func (c *ClusterConfig) Size() int {
+	c.mu.RLock()
+	defer c.mu.RUnlock()
+	return len(c.Nodes)
+}
\ No newline at end of file
diff --git a/repository_after/pkg/rpc/transport.go b/repository_after/pkg/rpc/transport.go
new file mode 100644
index 0000000..796993b
--- /dev/null
+++ b/repository_after/pkg/rpc/transport.go
@@ -0,0 +1,164 @@
+package rpc
+
+import (
+	"sync"
+	"time"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+// LocalTransport implements in-memory transport for testing
+type LocalTransport struct {
+	mu       sync.RWMutex
+	nodes    map[string]*raft.Node
+	disabled map[string]map[string]bool // disabled[from][to] = true if connection is disabled
+	latency  time.Duration
+}
+
+// NewLocalTransport creates a new local transport for testing
+func NewLocalTransport() *LocalTransport {
+	return &LocalTransport{
+		nodes:    make(map[string]*raft.Node),
+		disabled: make(map[string]map[string]bool),
+	}
+}
+
+// Register registers a node with the transport
+func (t *LocalTransport) Register(id string, node *raft.Node) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	t.nodes[id] = node
+	t.disabled[id] = make(map[string]bool)
+}
+
+// SetLatency sets artificial latency for all RPCs
+func (t *LocalTransport) SetLatency(d time.Duration) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	t.latency = d
+}
+
+// Disconnect simulates network disconnect between two nodes
+func (t *LocalTransport) Disconnect(from, to string) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	if t.disabled[from] == nil {
+		t.disabled[from] = make(map[string]bool)
+	}
+	t.disabled[from][to] = true
+}
+
+// Connect restores network connection between two nodes
+func (t *LocalTransport) Connect(from, to string) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	if t.disabled[from] != nil {
+		delete(t.disabled[from], to)
+	}
+}
+
+// Partition isolates a node from the rest of the cluster
+func (t *LocalTransport) Partition(nodeID string) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+
+	for id := range t.nodes {
+		if id != nodeID {
+			if t.disabled[nodeID] == nil {
+				t.disabled[nodeID] = make(map[string]bool)
+			}
+			if t.disabled[id] == nil {
+				t.disabled[id] = make(map[string]bool)
+			}
+			t.disabled[nodeID][id] = true
+			t.disabled[id][nodeID] = true
+		}
+	}
+}
+
+// Heal restores all network connections for a node
+func (t *LocalTransport) Heal(nodeID string) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+
+	t.disabled[nodeID] = make(map[string]bool)
+	for id := range t.nodes {
+		if t.disabled[id] != nil {
+			delete(t.disabled[id], nodeID)
+		}
+	}
+}
+
+// HealAll restores all network connections
+func (t *LocalTransport) HealAll() {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	t.disabled = make(map[string]map[string]bool)
+}
+
+func (t *LocalTransport) isConnected(from, to string) bool {
+	if t.disabled[from] == nil {
+		return true
+	}
+	return !t.disabled[from][to]
+}
+
+// RequestVote sends a RequestVote RPC
+func (t *LocalTransport) RequestVote(target string, args *raft.RequestVoteArgs) (*raft.RequestVoteReply, error) {
+	t.mu.RLock()
+	node, ok := t.nodes[target]
+	connected := t.isConnected(args.CandidateID, target)
+	latency := t.latency
+	t.mu.RUnlock()
+
+	if !ok || !connected {
+		return nil, raft.ErrNodeNotFound
+	}
+
+	if latency > 0 {
+		time.Sleep(latency)
+	}
+
+	reply := node.HandleRequestVote(args)
+	return reply, nil
+}
+
+// AppendEntries sends an AppendEntries RPC
+func (t *LocalTransport) AppendEntries(target string, args *raft.AppendEntriesArgs) (*raft.AppendEntriesReply, error) {
+	t.mu.RLock()
+	node, ok := t.nodes[target]
+	connected := t.isConnected(args.LeaderID, target)
+	latency := t.latency
+	t.mu.RUnlock()
+
+	if !ok || !connected {
+		return nil, raft.ErrNodeNotFound
+	}
+
+	if latency > 0 {
+		time.Sleep(latency)
+	}
+
+	reply := node.HandleAppendEntries(args)
+	return reply, nil
+}
+
+// InstallSnapshot sends an InstallSnapshot RPC
+func (t *LocalTransport) InstallSnapshot(target string, args *raft.InstallSnapshotArgs) (*raft.InstallSnapshotReply, error) {
+	t.mu.RLock()
+	node, ok := t.nodes[target]
+	connected := t.isConnected(args.LeaderID, target)
+	latency := t.latency
+	t.mu.RUnlock()
+
+	if !ok || !connected {
+		return nil, raft.ErrNodeNotFound
+	}
+
+	if latency > 0 {
+		time.Sleep(latency)
+	}
+
+	reply := node.HandleInstallSnapshot(args)
+	return reply, nil
+}
\ No newline at end of file
diff --git a/repository_after/pkg/testing/cluster.go b/repository_after/pkg/testing/cluster.go
new file mode 100644
index 0000000..1990e90
--- /dev/null
+++ b/repository_after/pkg/testing/cluster.go
@@ -0,0 +1,251 @@
+package testing
+
+import (
+	"context"
+	"fmt"
+	"math/rand"
+	"os"
+	"time"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/kv"
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/rpc"
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/wal"
+)
+
+// TestCluster represents a test cluster
+type TestCluster struct {
+	Nodes     []*raft.Node
+	Stores    []*kv.Store
+	Transport *rpc.LocalTransport
+	WALs      []*wal.WAL
+	walDirs   []string
+	History   *History
+}
+
+// NewTestCluster creates a new test cluster
+func NewTestCluster(size int) (*TestCluster, error) {
+	transport := rpc.NewLocalTransport()
+
+	uniqueID := rand.Int63()
+
+	nodeIDs := make([]string, size)
+	for i := 0; i < size; i++ {
+		nodeIDs[i] = fmt.Sprintf("node-%d", i)
+	}
+
+	cluster := &TestCluster{
+		Nodes:     make([]*raft.Node, size),
+		Stores:    make([]*kv.Store, size),
+		Transport: transport,
+		WALs:      make([]*wal.WAL, size),
+		walDirs:   make([]string, size),
+		History:   NewHistory(),
+	}
+
+	for i := 0; i < size; i++ {
+		peers := make([]string, 0, size-1)
+		for j := 0; j < size; j++ {
+			if i != j {
+				peers = append(peers, nodeIDs[j])
+			}
+		}
+
+		walDir := fmt.Sprintf("/tmp/raft-test-wal-%d-%d-%d", os.Getpid(), uniqueID, i)
+		cluster.walDirs[i] = walDir
+		os.RemoveAll(walDir)
+
+		walInstance, err := wal.NewWAL(walDir)
+		if err != nil {
+			cluster.Cleanup()
+			return nil, err
+		}
+		cluster.WALs[i] = walInstance
+
+		store := kv.NewStore()
+		cluster.Stores[i] = store
+
+		config := raft.NodeConfig{
+			ID:                 nodeIDs[i],
+			Peers:              peers,
+			ElectionTimeoutMin: 500 * time.Millisecond,
+			ElectionTimeoutMax: 1000 * time.Millisecond,
+			HeartbeatInterval:  50 * time.Millisecond,
+			WALPath:            walDir,
+			SnapshotThreshold:  100,
+		}
+
+		node := raft.NewNode(config, transport, walInstance, store)
+		cluster.Nodes[i] = node
+		transport.Register(nodeIDs[i], node)
+	}
+
+	return cluster, nil
+}
+
+// Start starts all nodes in the cluster
+func (c *TestCluster) Start() error {
+	for _, node := range c.Nodes {
+		if err := node.Start(); err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+// Stop stops all nodes in the cluster
+func (c *TestCluster) Stop() {
+	for _, node := range c.Nodes {
+		if node != nil {
+			node.Stop()
+		}
+	}
+}
+
+// Cleanup removes all temporary files
+func (c *TestCluster) Cleanup() {
+	c.Stop()
+	time.Sleep(100 * time.Millisecond)
+	for _, dir := range c.walDirs {
+		os.RemoveAll(dir)
+	}
+}
+
+// WaitForLeader waits for a leader to be elected
+func (c *TestCluster) WaitForLeader(timeout time.Duration) (*raft.Node, error) {
+	deadline := time.Now().Add(timeout)
+	for time.Now().Before(deadline) {
+		for _, node := range c.Nodes {
+			if node.IsLeader() {
+				return node, nil
+			}
+		}
+		time.Sleep(50 * time.Millisecond)
+	}
+	return nil, fmt.Errorf("no leader elected within timeout")
+}
+
+// WaitForStableLeader waits for a leader that has committed at least one entry
+func (c *TestCluster) WaitForStableLeader(timeout time.Duration) (*raft.Node, error) {
+	deadline := time.Now().Add(timeout)
+
+	for time.Now().Before(deadline) {
+		for _, node := range c.Nodes {
+			if node.IsLeader() && node.GetCommitIndex() > 0 {
+				time.Sleep(200 * time.Millisecond)
+				if node.IsLeader() {
+					return node, nil
+				}
+			}
+		}
+		time.Sleep(100 * time.Millisecond)
+	}
+	return nil, fmt.Errorf("no stable leader elected within timeout")
+}
+
+// GetLeader returns the current leader
+func (c *TestCluster) GetLeader() *raft.Node {
+	for _, node := range c.Nodes {
+		if node.IsLeader() {
+			return node
+		}
+	}
+	return nil
+}
+
+// GetLeaderExcluding returns the current leader excluding a specific node
+func (c *TestCluster) GetLeaderExcluding(excludeID string) *raft.Node {
+	for _, node := range c.Nodes {
+		if node.GetID() != excludeID && node.IsLeader() {
+			return node
+		}
+	}
+	return nil
+}
+
+// PartitionLeader partitions the current leader from the cluster
+func (c *TestCluster) PartitionLeader() *raft.Node {
+	leader := c.GetLeader()
+	if leader != nil {
+		c.Transport.Partition(leader.GetID())
+	}
+	return leader
+}
+
+// HealPartition heals all network partitions
+func (c *TestCluster) HealPartition() {
+	c.Transport.HealAll()
+}
+
+// SubmitCommand submits a command with retry logic
+func (c *TestCluster) SubmitCommand(cmd raft.Command, timeout time.Duration) error {
+	return c.SubmitCommandExcluding(cmd, timeout, "")
+}
+
+// SubmitCommandExcluding submits a command, excluding a specific node
+func (c *TestCluster) SubmitCommandExcluding(cmd raft.Command, timeout time.Duration, excludeID string) error {
+	startTime := time.Now().UnixNano()
+	opID := c.History.RecordInvoke("write", cmd.Key, cmd.Value, startTime)
+
+	deadline := time.Now().Add(timeout)
+
+	for time.Now().Before(deadline) {
+		var leader *raft.Node
+		if excludeID != "" {
+			leader = c.GetLeaderExcluding(excludeID)
+		} else {
+			leader = c.GetLeader()
+		}
+
+		if leader == nil {
+			time.Sleep(100 * time.Millisecond)
+			continue
+		}
+
+		ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
+		_, err := leader.SubmitWithResult(ctx, cmd)
+		cancel()
+
+		if err == nil {
+			c.History.RecordOk(opID, cmd.Value, time.Now().UnixNano())
+			return nil
+		}
+
+		time.Sleep(100 * time.Millisecond)
+	}
+
+	c.History.RecordFail(opID, time.Now().UnixNano())
+	return fmt.Errorf("timeout submitting command")
+}
+
+// SubmitToNode submits a command directly to a specific node
+func (c *TestCluster) SubmitToNode(node *raft.Node, cmd raft.Command, timeout time.Duration) error {
+	ctx, cancel := context.WithTimeout(context.Background(), timeout)
+	defer cancel()
+
+	_, err := node.SubmitWithResult(ctx, cmd)
+	return err
+}
+
+// WaitForNewLeader waits for a new leader different from the specified node
+func (c *TestCluster) WaitForNewLeader(excludeID string, timeout time.Duration) (*raft.Node, error) {
+	deadline := time.Now().Add(timeout)
+	for time.Now().Before(deadline) {
+		for _, node := range c.Nodes {
+			if node.GetID() != excludeID && node.IsLeader() {
+				time.Sleep(300 * time.Millisecond)
+				if node.IsLeader() {
+					return node, nil
+				}
+			}
+		}
+		time.Sleep(50 * time.Millisecond)
+	}
+	return nil, fmt.Errorf("no new leader elected within timeout")
+}
+
+// CheckLinearizability verifies the operation history is linearizable
+func (c *TestCluster) CheckLinearizability() (bool, error) {
+	checker := NewLinearizabilityChecker(c.History)
+	return checker.Check()
+}
\ No newline at end of file
diff --git a/repository_after/pkg/testing/invariant_checker.go b/repository_after/pkg/testing/invariant_checker.go
new file mode 100644
index 0000000..f98448c
--- /dev/null
+++ b/repository_after/pkg/testing/invariant_checker.go
@@ -0,0 +1,410 @@
+package testing
+
+import (
+	"fmt"
+	"sync"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+// CommittedEntry represents a committed log entry
+type CommittedEntry struct {
+	Index   uint64
+	Term    uint64
+	Command raft.Command
+	NodeID  string
+}
+
+// InvariantChecker checks Raft safety invariants
+type InvariantChecker struct {
+	mu              sync.Mutex
+	committedByNode map[string][]CommittedEntry
+	violations      []InvariantViolation
+}
+
+// InvariantViolation represents a safety violation
+type InvariantViolation struct {
+	Type        string
+	Description string
+	Details     map[string]interface{}
+}
+
+// NewInvariantChecker creates a new invariant checker
+func NewInvariantChecker() *InvariantChecker {
+	return &InvariantChecker{
+		committedByNode: make(map[string][]CommittedEntry),
+		violations:      make([]InvariantViolation, 0),
+	}
+}
+
+// RecordCommit records a committed entry from a node
+func (ic *InvariantChecker) RecordCommit(nodeID string, index, term uint64, cmd raft.Command) {
+	ic.mu.Lock()
+	defer ic.mu.Unlock()
+
+	entry := CommittedEntry{
+		Index:   index,
+		Term:    term,
+		Command: cmd,
+		NodeID:  nodeID,
+	}
+
+	ic.committedByNode[nodeID] = append(ic.committedByNode[nodeID], entry)
+}
+
+// CheckSafetyInvariants checks all safety invariants
+func (ic *InvariantChecker) CheckSafetyInvariants() (bool, []InvariantViolation) {
+	ic.mu.Lock()
+	defer ic.mu.Unlock()
+
+	ic.violations = make([]InvariantViolation, 0)
+
+	// Check: No two nodes commit different values at the same index
+	ic.checkLogMatchingSafety()
+
+	// Check: Committed entries are never lost (monotonic commit)
+	ic.checkMonotonicCommit()
+
+	// Check: Term numbers are consistent
+	ic.checkTermConsistency()
+
+	return len(ic.violations) == 0, ic.violations
+}
+
+// checkLogMatchingSafety verifies that all nodes have the same value at each committed index
+func (ic *InvariantChecker) checkLogMatchingSafety() {
+	// Build index -> (nodeID -> entry) map
+	indexEntries := make(map[uint64]map[string]CommittedEntry)
+
+	for nodeID, entries := range ic.committedByNode {
+		for _, entry := range entries {
+			if indexEntries[entry.Index] == nil {
+				indexEntries[entry.Index] = make(map[string]CommittedEntry)
+			}
+			indexEntries[entry.Index][nodeID] = entry
+		}
+	}
+
+	// Check each index for consistency
+	for index, nodeEntries := range indexEntries {
+		var refEntry *CommittedEntry
+		var refNodeID string
+
+		for nodeID, entry := range nodeEntries {
+			if refEntry == nil {
+				refEntry = &entry
+				refNodeID = nodeID
+			} else {
+				// Check if entries match
+				if entry.Term != refEntry.Term {
+					ic.violations = append(ic.violations, InvariantViolation{
+						Type: "LOG_MATCHING_VIOLATION",
+						Description: fmt.Sprintf("Different terms at index %d: node %s has term %d, node %s has term %d",
+							index, refNodeID, refEntry.Term, nodeID, entry.Term),
+						Details: map[string]interface{}{
+							"index":     index,
+							"node1":     refNodeID,
+							"term1":     refEntry.Term,
+							"node2":     nodeID,
+							"term2":     entry.Term,
+							"command1":  refEntry.Command,
+							"command2":  entry.Command,
+						},
+					})
+				}
+
+				// For SET commands, check value consistency
+				if entry.Command.Type == raft.CommandSet && refEntry.Command.Type == raft.CommandSet {
+					if entry.Command.Key != refEntry.Command.Key || entry.Command.Value != refEntry.Command.Value {
+						ic.violations = append(ic.violations, InvariantViolation{
+							Type: "VALUE_MISMATCH",
+							Description: fmt.Sprintf("Different values at index %d: node %s has %s=%s, node %s has %s=%s",
+								index, refNodeID, refEntry.Command.Key, refEntry.Command.Value,
+								nodeID, entry.Command.Key, entry.Command.Value),
+							Details: map[string]interface{}{
+								"index":  index,
+								"node1":  refNodeID,
+								"key1":   refEntry.Command.Key,
+								"value1": refEntry.Command.Value,
+								"node2":  nodeID,
+								"key2":   entry.Command.Key,
+								"value2": entry.Command.Value,
+							},
+						})
+					}
+				}
+			}
+		}
+	}
+}
+
+// checkMonotonicCommit verifies commit index never decreases
+func (ic *InvariantChecker) checkMonotonicCommit() {
+	for nodeID, entries := range ic.committedByNode {
+		var lastIndex uint64 = 0
+		for _, entry := range entries {
+			if entry.Index < lastIndex {
+				ic.violations = append(ic.violations, InvariantViolation{
+					Type: "NON_MONOTONIC_COMMIT",
+					Description: fmt.Sprintf("Node %s committed index %d after index %d",
+						nodeID, entry.Index, lastIndex),
+					Details: map[string]interface{}{
+						"nodeID":    nodeID,
+						"prevIndex": lastIndex,
+						"currIndex": entry.Index,
+					},
+				})
+			}
+			lastIndex = entry.Index
+		}
+	}
+}
+
+// checkTermConsistency verifies term numbers are consistent
+func (ic *InvariantChecker) checkTermConsistency() {
+	// Terms at higher indices should be >= terms at lower indices
+	for nodeID, entries := range ic.committedByNode {
+		for i := 1; i < len(entries); i++ {
+			prev := entries[i-1]
+			curr := entries[i]
+
+			if curr.Index > prev.Index && curr.Term < prev.Term {
+				ic.violations = append(ic.violations, InvariantViolation{
+					Type: "TERM_CONSISTENCY_VIOLATION",
+					Description: fmt.Sprintf("Node %s has term %d at index %d, but term %d at higher index %d",
+						nodeID, prev.Term, prev.Index, curr.Term, curr.Index),
+					Details: map[string]interface{}{
+						"nodeID":    nodeID,
+						"prevIndex": prev.Index,
+						"prevTerm":  prev.Term,
+						"currIndex": curr.Index,
+						"currTerm":  curr.Term,
+					},
+				})
+			}
+		}
+	}
+}
+
+// Clear resets the checker
+func (ic *InvariantChecker) Clear() {
+	ic.mu.Lock()
+	defer ic.mu.Unlock()
+	ic.committedByNode = make(map[string][]CommittedEntry)
+	ic.violations = make([]InvariantViolation, 0)
+}
+
+// CollectFromNodes collects committed entries from cluster nodes
+func (ic *InvariantChecker) CollectFromNodes(nodes []*raft.Node) {
+	ic.mu.Lock()
+	defer ic.mu.Unlock()
+
+	for _, node := range nodes {
+		nodeID := node.GetID()
+		log := node.GetLog()
+		commitIndex := node.GetCommitIndex()
+
+		for _, entry := range log {
+			if entry.Index > 0 && entry.Index <= commitIndex {
+				ic.committedByNode[nodeID] = append(ic.committedByNode[nodeID], CommittedEntry{
+					Index:   entry.Index,
+					Term:    entry.Term,
+					Command: entry.Command,
+					NodeID:  nodeID,
+				})
+			}
+		}
+	}
+}
+
+// CompareStateMachines compares final state machine states across nodes
+func CompareStateMachines(stores []*SimulatedStore) (bool, []string) {
+	if len(stores) == 0 {
+		return true, nil
+	}
+
+	differences := make([]string, 0)
+	refState := stores[0].GetSnapshot()
+
+	for i := 1; i < len(stores); i++ {
+		state := stores[i].GetSnapshot()
+
+		// Check for missing keys
+		for key, refValue := range refState {
+			if value, ok := state[key]; !ok {
+				differences = append(differences, fmt.Sprintf("Store %d missing key %s (expected %s)", i, key, refValue))
+			} else if value != refValue {
+				differences = append(differences, fmt.Sprintf("Store %d has %s=%s, expected %s", i, key, value, refValue))
+			}
+		}
+
+		// Check for extra keys
+		for key, value := range state {
+			if _, ok := refState[key]; !ok {
+				differences = append(differences, fmt.Sprintf("Store %d has unexpected key %s=%s", i, key, value))
+			}
+		}
+	}
+
+	return len(differences) == 0, differences
+}
+
+// JepsenStyleChecker performs randomized safety testing
+type JepsenStyleChecker struct {
+	history    *History
+	checker    *InvariantChecker
+	operations []JepsenOperation
+	mu         sync.Mutex
+}
+
+// JepsenOperation records an operation for Jepsen-style analysis
+type JepsenOperation struct {
+	ID        int64
+	Type      string // "invoke" or "ok" or "fail" or "info"
+	OpType    string // "read" or "write" or "cas"
+	Key       string
+	Value     string
+	ReadValue string
+	StartTime int64
+	EndTime   int64
+	NodeID    string
+	Success   bool
+}
+
+// NewJepsenStyleChecker creates a new Jepsen-style checker
+func NewJepsenStyleChecker() *JepsenStyleChecker {
+	return &JepsenStyleChecker{
+		history:    NewHistory(),
+		checker:    NewInvariantChecker(),
+		operations: make([]JepsenOperation, 0),
+	}
+}
+
+// RecordInvoke records the start of an operation
+func (j *JepsenStyleChecker) RecordInvoke(nodeID, opType, key, value string, startTime int64) int64 {
+	j.mu.Lock()
+	defer j.mu.Unlock()
+
+	id := int64(len(j.operations))
+	j.operations = append(j.operations, JepsenOperation{
+		ID:        id,
+		Type:      "invoke",
+		OpType:    opType,
+		Key:       key,
+		Value:     value,
+		StartTime: startTime,
+		NodeID:    nodeID,
+	})
+
+	return id
+}
+
+// RecordOk records successful completion
+func (j *JepsenStyleChecker) RecordOk(id int64, readValue string, endTime int64) {
+	j.mu.Lock()
+	defer j.mu.Unlock()
+
+	if id >= 0 && id < int64(len(j.operations)) {
+		j.operations = append(j.operations, JepsenOperation{
+			ID:        id,
+			Type:      "ok",
+			OpType:    j.operations[id].OpType,
+			Key:       j.operations[id].Key,
+			Value:     j.operations[id].Value,
+			ReadValue: readValue,
+			EndTime:   endTime,
+			NodeID:    j.operations[id].NodeID,
+			Success:   true,
+		})
+	}
+}
+
+// RecordFail records operation failure
+func (j *JepsenStyleChecker) RecordFail(id int64, endTime int64) {
+	j.mu.Lock()
+	defer j.mu.Unlock()
+
+	if id >= 0 && id < int64(len(j.operations)) {
+		j.operations = append(j.operations, JepsenOperation{
+			ID:      id,
+			Type:    "fail",
+			OpType:  j.operations[id].OpType,
+			Key:     j.operations[id].Key,
+			EndTime: endTime,
+			NodeID:  j.operations[id].NodeID,
+			Success: false,
+		})
+	}
+}
+
+// CheckLinearizability performs linearizability check on recorded history
+func (j *JepsenStyleChecker) CheckLinearizability() (bool, []string) {
+	j.mu.Lock()
+	defer j.mu.Unlock()
+
+	issues := make([]string, 0)
+
+	// Build completed operations
+	invokes := make(map[int64]JepsenOperation)
+	completes := make(map[int64]JepsenOperation)
+
+	for _, op := range j.operations {
+		if op.Type == "invoke" {
+			invokes[op.ID] = op
+		} else if op.Type == "ok" || op.Type == "fail" {
+			completes[op.ID] = op
+		}
+	}
+
+	// For each key, verify read consistency
+	keyWrites := make(map[string][]JepsenOperation) // key -> list of writes
+
+	for id, complete := range completes {
+		invoke, ok := invokes[id]
+		if !ok {
+			continue
+		}
+
+		if invoke.OpType == "write" && complete.Success {
+			keyWrites[invoke.Key] = append(keyWrites[invoke.Key], complete)
+		}
+	}
+
+	// Check that reads return values that were written
+	for id, complete := range completes {
+		invoke, ok := invokes[id]
+		if !ok {
+			continue
+		}
+
+		if invoke.OpType == "read" && complete.Success && complete.ReadValue != "" {
+			// Check if this value was ever written
+			writes, hasWrites := keyWrites[invoke.Key]
+			if hasWrites {
+				found := false
+				for _, write := range writes {
+					if write.Value == complete.ReadValue {
+						found = true
+						break
+					}
+				}
+				if !found {
+					issues = append(issues, fmt.Sprintf(
+						"Read of key %s returned %s, but no write with that value found",
+						invoke.Key, complete.ReadValue))
+				}
+			}
+		}
+	}
+
+	return len(issues) == 0, issues
+}
+
+// GetOperations returns all recorded operations
+func (j *JepsenStyleChecker) GetOperations() []JepsenOperation {
+	j.mu.Lock()
+	defer j.mu.Unlock()
+	result := make([]JepsenOperation, len(j.operations))
+	copy(result, j.operations)
+	return result
+}
\ No newline at end of file
diff --git a/repository_after/pkg/testing/linearizability_checker.go b/repository_after/pkg/testing/linearizability_checker.go
new file mode 100644
index 0000000..db1781f
--- /dev/null
+++ b/repository_after/pkg/testing/linearizability_checker.go
@@ -0,0 +1,165 @@
+package testing
+
+import (
+	"fmt"
+	"sort"
+	"sync"
+)
+
+// Operation represents a single operation in the history
+type Operation struct {
+	ID        int64
+	Type      string // "invoke" or "ok" or "fail"
+	OpType    string // "read" or "write"
+	Key       string
+	Value     string
+	StartTime int64
+	EndTime   int64
+}
+
+// History records all operations for linearizability checking
+type History struct {
+	mu         sync.Mutex
+	operations []Operation
+	nextID     int64
+}
+
+// NewHistory creates a new history recorder
+func NewHistory() *History {
+	return &History{
+		operations: make([]Operation, 0),
+	}
+}
+
+// RecordInvoke records the start of an operation
+func (h *History) RecordInvoke(opType, key, value string, startTime int64) int64 {
+	h.mu.Lock()
+	defer h.mu.Unlock()
+
+	id := h.nextID
+	h.nextID++
+
+	h.operations = append(h.operations, Operation{
+		ID:        id,
+		Type:      "invoke",
+		OpType:    opType,
+		Key:       key,
+		Value:     value,
+		StartTime: startTime,
+	})
+
+	return id
+}
+
+// RecordOk records the successful completion of an operation
+func (h *History) RecordOk(id int64, value string, endTime int64) {
+	h.mu.Lock()
+	defer h.mu.Unlock()
+
+	h.operations = append(h.operations, Operation{
+		ID:      id,
+		Type:    "ok",
+		Value:   value,
+		EndTime: endTime,
+	})
+}
+
+// RecordFail records the failure of an operation
+func (h *History) RecordFail(id int64, endTime int64) {
+	h.mu.Lock()
+	defer h.mu.Unlock()
+
+	h.operations = append(h.operations, Operation{
+		ID:      id,
+		Type:    "fail",
+		EndTime: endTime,
+	})
+}
+
+// OperationPair pairs an invoke with its response
+type OperationPair struct {
+	Invoke   Operation
+	Response Operation
+	Complete bool
+}
+
+// LinearizabilityChecker verifies linearizability of a history
+type LinearizabilityChecker struct {
+	history *History
+}
+
+// NewLinearizabilityChecker creates a new linearizability checker
+func NewLinearizabilityChecker(h *History) *LinearizabilityChecker {
+	return &LinearizabilityChecker{history: h}
+}
+
+// Check performs linearizability verification
+func (lc *LinearizabilityChecker) Check() (bool, error) {
+	lc.history.mu.Lock()
+	ops := make([]Operation, len(lc.history.operations))
+	copy(ops, lc.history.operations)
+	lc.history.mu.Unlock()
+
+	// Build invoke-response pairs
+	pairs := make(map[int64]*OperationPair)
+	for _, op := range ops {
+		if op.Type == "invoke" {
+			pairs[op.ID] = &OperationPair{
+				Invoke: op,
+			}
+		} else if op.Type == "ok" {
+			if pair, ok := pairs[op.ID]; ok {
+				pair.Response = op
+				pair.Complete = true
+			}
+		}
+	}
+
+	// Extract complete operations
+	complete := make([]*OperationPair, 0)
+	for _, pair := range pairs {
+		if pair.Complete {
+			complete = append(complete, pair)
+		}
+	}
+
+	// Sort by start time
+	sort.Slice(complete, func(i, j int) bool {
+		return complete[i].Invoke.StartTime < complete[j].Invoke.StartTime
+	})
+
+	// Simple sequential consistency check for key-value store
+	state := make(map[string]string)
+
+	for _, pair := range complete {
+		if pair.Invoke.OpType == "write" {
+			state[pair.Invoke.Key] = pair.Invoke.Value
+		} else if pair.Invoke.OpType == "read" {
+			expected := state[pair.Invoke.Key]
+			if pair.Response.Value != expected {
+				// Check if there's a concurrent write that could explain this
+				if !lc.hasConcurrentWrite(complete, pair, pair.Response.Value) {
+					return false, fmt.Errorf("read of %s returned %s, expected %s",
+						pair.Invoke.Key, pair.Response.Value, expected)
+				}
+			}
+		}
+	}
+
+	return true, nil
+}
+
+func (lc *LinearizabilityChecker) hasConcurrentWrite(ops []*OperationPair, readOp *OperationPair, value string) bool {
+	for _, op := range ops {
+		if op.Invoke.OpType == "write" &&
+			op.Invoke.Key == readOp.Invoke.Key &&
+			op.Invoke.Value == value {
+			// Check if operations overlap in time
+			if op.Invoke.StartTime <= readOp.Response.EndTime &&
+				op.Response.EndTime >= readOp.Invoke.StartTime {
+				return true
+			}
+		}
+	}
+	return false
+}
\ No newline at end of file
diff --git a/repository_after/pkg/testing/simulator.go b/repository_after/pkg/testing/simulator.go
new file mode 100644
index 0000000..ea8127d
--- /dev/null
+++ b/repository_after/pkg/testing/simulator.go
@@ -0,0 +1,504 @@
+package testing
+
+import (
+	"container/heap"
+	"fmt"
+	"math/rand"
+	"sync"
+	"time"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+// Event represents a scheduled event in the simulation
+type Event struct {
+	Time     int64
+	Priority int
+	Action   func()
+	Index    int // for heap interface
+}
+
+// EventHeap implements heap.Interface for events
+type EventHeap []*Event
+
+func (h EventHeap) Len() int           { return len(h) }
+func (h EventHeap) Less(i, j int) bool { return h[i].Time < h[j].Time }
+func (h EventHeap) Swap(i, j int) {
+	h[i], h[j] = h[j], h[i]
+	h[i].Index = i
+	h[j].Index = j
+}
+
+func (h *EventHeap) Push(x interface{}) {
+	n := len(*h)
+	event := x.(*Event)
+	event.Index = n
+	*h = append(*h, event)
+}
+
+func (h *EventHeap) Pop() interface{} {
+	old := *h
+	n := len(old)
+	event := old[n-1]
+	old[n-1] = nil
+	event.Index = -1
+	*h = old[0 : n-1]
+	return event
+}
+
+// DeterministicClock provides a controllable clock for testing
+type DeterministicClock struct {
+	mu      sync.Mutex
+	current int64
+}
+
+func NewDeterministicClock() *DeterministicClock {
+	return &DeterministicClock{current: 0}
+}
+
+func (c *DeterministicClock) Now() time.Time {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	return time.Unix(0, c.current)
+}
+
+func (c *DeterministicClock) Advance(d time.Duration) {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	c.current += int64(d)
+}
+
+func (c *DeterministicClock) Set(t int64) {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	c.current = t
+}
+
+func (c *DeterministicClock) Get() int64 {
+	c.mu.Lock()
+	defer c.mu.Unlock()
+	return c.current
+}
+
+// NetworkCondition represents network behavior between two nodes
+type NetworkCondition struct {
+	Delay       time.Duration
+	DropRate    float64
+	Partitioned bool
+}
+
+// DeterministicTransport provides controlled message delivery
+type DeterministicTransport struct {
+	mu         sync.RWMutex
+	nodes      map[string]*raft.Node
+	conditions map[string]map[string]*NetworkCondition
+	clock      *DeterministicClock
+	events     *EventHeap
+	eventMu    sync.Mutex
+	rng        *rand.Rand
+	messages   []MessageRecord
+	msgMu      sync.Mutex
+}
+
+// MessageRecord records a message for later analysis
+type MessageRecord struct {
+	Time      int64
+	From      string
+	To        string
+	Type      string
+	Delivered bool
+	Dropped   bool
+}
+
+func NewDeterministicTransport(seed int64) *DeterministicTransport {
+	h := &EventHeap{}
+	heap.Init(h)
+	return &DeterministicTransport{
+		nodes:      make(map[string]*raft.Node),
+		conditions: make(map[string]map[string]*NetworkCondition),
+		clock:      NewDeterministicClock(),
+		events:     h,
+		rng:        rand.New(rand.NewSource(seed)),
+		messages:   make([]MessageRecord, 0),
+	}
+}
+
+func (t *DeterministicTransport) Register(id string, node *raft.Node) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	t.nodes[id] = node
+	t.conditions[id] = make(map[string]*NetworkCondition)
+}
+
+func (t *DeterministicTransport) GetClock() *DeterministicClock {
+	return t.clock
+}
+
+// SetNetworkCondition sets the network condition between two nodes
+func (t *DeterministicTransport) SetNetworkCondition(from, to string, cond *NetworkCondition) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	if t.conditions[from] == nil {
+		t.conditions[from] = make(map[string]*NetworkCondition)
+	}
+	t.conditions[from][to] = cond
+}
+
+// Partition isolates a node from all others
+func (t *DeterministicTransport) Partition(nodeID string) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	for id := range t.nodes {
+		if id != nodeID {
+			if t.conditions[nodeID] == nil {
+				t.conditions[nodeID] = make(map[string]*NetworkCondition)
+			}
+			if t.conditions[id] == nil {
+				t.conditions[id] = make(map[string]*NetworkCondition)
+			}
+			t.conditions[nodeID][id] = &NetworkCondition{Partitioned: true}
+			t.conditions[id][nodeID] = &NetworkCondition{Partitioned: true}
+		}
+	}
+}
+
+// Heal removes all partitions for a node
+func (t *DeterministicTransport) Heal(nodeID string) {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	t.conditions[nodeID] = make(map[string]*NetworkCondition)
+	for id := range t.nodes {
+		if t.conditions[id] != nil {
+			delete(t.conditions[id], nodeID)
+		}
+	}
+}
+
+// HealAll removes all network conditions
+func (t *DeterministicTransport) HealAll() {
+	t.mu.Lock()
+	defer t.mu.Unlock()
+	t.conditions = make(map[string]map[string]*NetworkCondition)
+}
+
+func (t *DeterministicTransport) getCondition(from, to string) *NetworkCondition {
+	if t.conditions[from] == nil {
+		return nil
+	}
+	return t.conditions[from][to]
+}
+
+func (t *DeterministicTransport) shouldDrop(from, to string) bool {
+	cond := t.getCondition(from, to)
+	if cond == nil {
+		return false
+	}
+	if cond.Partitioned {
+		return true
+	}
+	if cond.DropRate > 0 && t.rng.Float64() < cond.DropRate {
+		return true
+	}
+	return false
+}
+
+func (t *DeterministicTransport) recordMessage(from, to, msgType string, delivered, dropped bool) {
+	t.msgMu.Lock()
+	defer t.msgMu.Unlock()
+	t.messages = append(t.messages, MessageRecord{
+		Time:      t.clock.Get(),
+		From:      from,
+		To:        to,
+		Type:      msgType,
+		Delivered: delivered,
+		Dropped:   dropped,
+	})
+}
+
+// GetMessageHistory returns all recorded messages
+func (t *DeterministicTransport) GetMessageHistory() []MessageRecord {
+	t.msgMu.Lock()
+	defer t.msgMu.Unlock()
+	result := make([]MessageRecord, len(t.messages))
+	copy(result, t.messages)
+	return result
+}
+
+// RequestVote implements Transport interface
+func (t *DeterministicTransport) RequestVote(target string, args *raft.RequestVoteArgs) (*raft.RequestVoteReply, error) {
+	t.mu.RLock()
+	node, ok := t.nodes[target]
+	shouldDrop := t.shouldDrop(args.CandidateID, target)
+	t.mu.RUnlock()
+
+	if !ok {
+		t.recordMessage(args.CandidateID, target, "RequestVote", false, false)
+		return nil, raft.ErrNodeNotFound
+	}
+
+	if shouldDrop {
+		t.recordMessage(args.CandidateID, target, "RequestVote", false, true)
+		return nil, raft.ErrTimeout
+	}
+
+	t.recordMessage(args.CandidateID, target, "RequestVote", true, false)
+	reply := node.HandleRequestVote(args)
+	return reply, nil
+}
+
+// AppendEntries implements Transport interface
+func (t *DeterministicTransport) AppendEntries(target string, args *raft.AppendEntriesArgs) (*raft.AppendEntriesReply, error) {
+	t.mu.RLock()
+	node, ok := t.nodes[target]
+	shouldDrop := t.shouldDrop(args.LeaderID, target)
+	t.mu.RUnlock()
+
+	if !ok {
+		t.recordMessage(args.LeaderID, target, "AppendEntries", false, false)
+		return nil, raft.ErrNodeNotFound
+	}
+
+	if shouldDrop {
+		t.recordMessage(args.LeaderID, target, "AppendEntries", false, true)
+		return nil, raft.ErrTimeout
+	}
+
+	t.recordMessage(args.LeaderID, target, "AppendEntries", true, false)
+	reply := node.HandleAppendEntries(args)
+	return reply, nil
+}
+
+// InstallSnapshot implements Transport interface
+func (t *DeterministicTransport) InstallSnapshot(target string, args *raft.InstallSnapshotArgs) (*raft.InstallSnapshotReply, error) {
+	t.mu.RLock()
+	node, ok := t.nodes[target]
+	shouldDrop := t.shouldDrop(args.LeaderID, target)
+	t.mu.RUnlock()
+
+	if !ok {
+		t.recordMessage(args.LeaderID, target, "InstallSnapshot", false, false)
+		return nil, raft.ErrNodeNotFound
+	}
+
+	if shouldDrop {
+		t.recordMessage(args.LeaderID, target, "InstallSnapshot", false, true)
+		return nil, raft.ErrTimeout
+	}
+
+	t.recordMessage(args.LeaderID, target, "InstallSnapshot", true, false)
+	reply := node.HandleInstallSnapshot(args)
+	return reply, nil
+}
+
+// Simulator provides a deterministic simulation environment
+type Simulator struct {
+	Transport *DeterministicTransport
+	Nodes     []*raft.Node
+	Stores    []*SimulatedStore
+	clock     *DeterministicClock
+	rng       *rand.Rand
+	seed      int64
+}
+
+// SimulatedStore wraps a KV store with operation tracking
+type SimulatedStore struct {
+	mu   sync.RWMutex
+	data map[string]string
+	ops  []StoreOperation
+}
+
+type StoreOperation struct {
+	Time  int64
+	Op    string
+	Key   string
+	Value string
+}
+
+func NewSimulatedStore() *SimulatedStore {
+	return &SimulatedStore{
+		data: make(map[string]string),
+		ops:  make([]StoreOperation, 0),
+	}
+}
+
+func (s *SimulatedStore) Apply(cmd raft.Command) string {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	switch cmd.Type {
+	case raft.CommandSet:
+		s.data[cmd.Key] = cmd.Value
+		s.ops = append(s.ops, StoreOperation{
+			Time:  time.Now().UnixNano(),
+			Op:    "SET",
+			Key:   cmd.Key,
+			Value: cmd.Value,
+		})
+		return cmd.Value
+	case raft.CommandDelete:
+		delete(s.data, cmd.Key)
+		s.ops = append(s.ops, StoreOperation{
+			Time: time.Now().UnixNano(),
+			Op:   "DELETE",
+			Key:  cmd.Key,
+		})
+		return ""
+	default:
+		return ""
+	}
+}
+
+func (s *SimulatedStore) Get(key string) (string, bool) {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+	v, ok := s.data[key]
+	return v, ok
+}
+
+func (s *SimulatedStore) GetSnapshot() map[string]string {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+	result := make(map[string]string)
+	for k, v := range s.data {
+		result[k] = v
+	}
+	return result
+}
+
+func (s *SimulatedStore) RestoreSnapshot(data map[string]string) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+	s.data = make(map[string]string)
+	for k, v := range data {
+		s.data[k] = v
+	}
+}
+
+func (s *SimulatedStore) GetOperations() []StoreOperation {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+	result := make([]StoreOperation, len(s.ops))
+	copy(result, s.ops)
+	return result
+}
+
+// NewSimulator creates a new deterministic simulator
+func NewSimulator(size int, seed int64) (*Simulator, error) {
+	transport := NewDeterministicTransport(seed)
+	rng := rand.New(rand.NewSource(seed))
+
+	nodeIDs := make([]string, size)
+	for i := 0; i < size; i++ {
+		nodeIDs[i] = fmt.Sprintf("sim-node-%d", i)
+	}
+
+	sim := &Simulator{
+		Transport: transport,
+		Nodes:     make([]*raft.Node, size),
+		Stores:    make([]*SimulatedStore, size),
+		clock:     transport.GetClock(),
+		rng:       rng,
+		seed:      seed,
+	}
+
+	for i := 0; i < size; i++ {
+		peers := make([]string, 0, size-1)
+		for j := 0; j < size; j++ {
+			if i != j {
+				peers = append(peers, nodeIDs[j])
+			}
+		}
+
+		store := NewSimulatedStore()
+		sim.Stores[i] = store
+
+		config := raft.NodeConfig{
+			ID:                 nodeIDs[i],
+			Peers:              peers,
+			ElectionTimeoutMin: 150 * time.Millisecond,
+			ElectionTimeoutMax: 300 * time.Millisecond,
+			HeartbeatInterval:  50 * time.Millisecond,
+			SnapshotThreshold:  1000,
+		}
+
+		node := raft.NewNode(config, transport, nil, store)
+		sim.Nodes[i] = node
+		transport.Register(nodeIDs[i], node)
+	}
+
+	return sim, nil
+}
+
+// Start starts all nodes
+func (s *Simulator) Start() error {
+	for _, node := range s.Nodes {
+		if err := node.Start(); err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+// Stop stops all nodes
+func (s *Simulator) Stop() {
+	for _, node := range s.Nodes {
+		node.Stop()
+	}
+}
+
+// AdvanceTime advances the simulation clock
+func (s *Simulator) AdvanceTime(d time.Duration) {
+	s.clock.Advance(d)
+}
+
+// GetLeader returns the current leader
+func (s *Simulator) GetLeader() *raft.Node {
+	for _, node := range s.Nodes {
+		if node.IsLeader() {
+			return node
+		}
+	}
+	return nil
+}
+
+// WaitForLeader waits for a leader with simulated time
+func (s *Simulator) WaitForLeader(maxIterations int) *raft.Node {
+	for i := 0; i < maxIterations; i++ {
+		if leader := s.GetLeader(); leader != nil {
+			return leader
+		}
+		time.Sleep(50 * time.Millisecond)
+	}
+	return nil
+}
+
+// InjectPartition partitions a node
+func (s *Simulator) InjectPartition(nodeIdx int) {
+	if nodeIdx >= 0 && nodeIdx < len(s.Nodes) {
+		s.Transport.Partition(s.Nodes[nodeIdx].GetID())
+	}
+}
+
+// HealPartition heals partition for a node
+func (s *Simulator) HealPartition(nodeIdx int) {
+	if nodeIdx >= 0 && nodeIdx < len(s.Nodes) {
+		s.Transport.Heal(s.Nodes[nodeIdx].GetID())
+	}
+}
+
+// HealAll heals all partitions
+func (s *Simulator) HealAll() {
+	s.Transport.HealAll()
+}
+
+// RandomPartition creates a random partition scenario
+func (s *Simulator) RandomPartition() int {
+	idx := s.rng.Intn(len(s.Nodes))
+	s.InjectPartition(idx)
+	return idx
+}
+
+// GetSeed returns the simulation seed for reproducibility
+func (s *Simulator) GetSeed() int64 {
+	return s.seed
+}
\ No newline at end of file
diff --git a/repository_after/pkg/wal/wal.go b/repository_after/pkg/wal/wal.go
new file mode 100644
index 0000000..b18edfe
--- /dev/null
+++ b/repository_after/pkg/wal/wal.go
@@ -0,0 +1,172 @@
+package wal
+
+import (
+	"encoding/json"
+	"fmt"
+	"os"
+	"path/filepath"
+	"sync"
+
+	"github.com/vzdtic/distributed-consensus-raft-kv-store/repository_after/pkg/raft"
+)
+
+type WAL struct {
+	mu           sync.Mutex
+	dir          string
+	stateFile    string
+	snapshotFile string
+	closed       bool
+}
+
+func NewWAL(dir string) (*WAL, error) {
+	if err := os.MkdirAll(dir, 0755); err != nil {
+		return nil, fmt.Errorf("failed to create WAL directory: %w", err)
+	}
+
+	return &WAL{
+		dir:          dir,
+		stateFile:    filepath.Join(dir, "state.json"),
+		snapshotFile: filepath.Join(dir, "snapshot.json"),
+		closed:       false,
+	}, nil
+}
+
+func (w *WAL) ensureDir() error {
+	if _, err := os.Stat(w.dir); os.IsNotExist(err) {
+		return os.MkdirAll(w.dir, 0755)
+	}
+	return nil
+}
+
+func (w *WAL) Save(state *raft.PersistentState) error {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+
+	if w.closed {
+		return nil
+	}
+
+	if err := w.ensureDir(); err != nil {
+		return fmt.Errorf("failed to ensure WAL directory: %w", err)
+	}
+
+	data, err := json.Marshal(state)
+	if err != nil {
+		return fmt.Errorf("failed to marshal state: %w", err)
+	}
+
+	tmpFile := w.stateFile + ".tmp"
+	if err := os.WriteFile(tmpFile, data, 0644); err != nil {
+		return fmt.Errorf("failed to write state file: %w", err)
+	}
+
+	if err := os.Rename(tmpFile, w.stateFile); err != nil {
+		return fmt.Errorf("failed to rename state file: %w", err)
+	}
+
+	return nil
+}
+
+func (w *WAL) Load() (*raft.PersistentState, error) {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+
+	data, err := os.ReadFile(w.stateFile)
+	if err != nil {
+		if os.IsNotExist(err) {
+			return nil, nil
+		}
+		return nil, fmt.Errorf("failed to read state file: %w", err)
+	}
+
+	var state raft.PersistentState
+	if err := json.Unmarshal(data, &state); err != nil {
+		return nil, fmt.Errorf("failed to unmarshal state: %w", err)
+	}
+
+	return &state, nil
+}
+
+func (w *WAL) SaveSnapshot(snapshot *raft.Snapshot) error {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+
+	if w.closed {
+		return nil
+	}
+
+	if err := w.ensureDir(); err != nil {
+		return fmt.Errorf("failed to ensure WAL directory: %w", err)
+	}
+
+	data, err := json.Marshal(snapshot)
+	if err != nil {
+		return fmt.Errorf("failed to marshal snapshot: %w", err)
+	}
+
+	tmpFile := w.snapshotFile + ".tmp"
+	if err := os.WriteFile(tmpFile, data, 0644); err != nil {
+		return fmt.Errorf("failed to write snapshot file: %w", err)
+	}
+
+	if err := os.Rename(tmpFile, w.snapshotFile); err != nil {
+		return fmt.Errorf("failed to rename snapshot file: %w", err)
+	}
+
+	return nil
+}
+
+func (w *WAL) LoadSnapshot() (*raft.Snapshot, error) {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+
+	data, err := os.ReadFile(w.snapshotFile)
+	if err != nil {
+		if os.IsNotExist(err) {
+			return nil, nil
+		}
+		return nil, fmt.Errorf("failed to read snapshot file: %w", err)
+	}
+
+	var snapshot raft.Snapshot
+	if err := json.Unmarshal(data, &snapshot); err != nil {
+		return nil, fmt.Errorf("failed to unmarshal snapshot: %w", err)
+	}
+
+	return &snapshot, nil
+}
+
+func (w *WAL) Size() (int64, error) {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+
+	var totalSize int64
+
+	info, err := os.Stat(w.stateFile)
+	if err == nil {
+		totalSize += info.Size()
+	}
+
+	info, err = os.Stat(w.snapshotFile)
+	if err == nil {
+		totalSize += info.Size()
+	}
+
+	return totalSize, nil
+}
+
+func (w *WAL) Close() error {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+	w.closed = true
+	return nil
+}
+
+func (w *WAL) Clear() error {
+	w.mu.Lock()
+	defer w.mu.Unlock()
+
+	os.Remove(w.stateFile)
+	os.Remove(w.snapshotFile)
+	return nil
+}
\ No newline at end of file
