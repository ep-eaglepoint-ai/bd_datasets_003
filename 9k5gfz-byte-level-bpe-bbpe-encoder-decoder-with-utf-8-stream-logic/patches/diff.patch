diff -Naur repository_before/bytebpe.py repository_after/bytebpe.py
--- repository_before/bytebpe.py	1970-01-01 03:00:00.000000000 +0300
+++ repository_after/bytebpe.py	2026-02-05 20:40:10.381700100 +0300
@@ -0,0 +1,97 @@
+"""Byte-Level Byte Pair Encoding (BBPE) Tokenizer."""
+
+
+class ByteBPE:
+    """Byte-Level BPE tokenizer operating on UTF-8 byte streams."""
+    
+    def __init__(self, merges: list[tuple[int, int]]) -> None:
+        for i, merge in enumerate(merges):
+            if not isinstance(merge, tuple):
+                raise TypeError(f"Merge rules must be tuples, got {type(merge)} at index {i}")
+            if len(merge) != 2:
+                raise ValueError(f"Merge rules must be tuples of exactly 2 elements, got {len(merge)} at index {i}")
+            if not isinstance(merge[0], int) or not isinstance(merge[1], int):
+                raise TypeError(f"Merge rule elements must be integers at index {i}")
+        
+        self.merges: list[tuple[int, int]] = list(merges)
+        self.vocab_size: int = 256 + len(merges)
+        self.merge_map: dict[int, tuple[int, int]] = {256 + i: pair for i, pair in enumerate(merges)}
+        self.pair_to_token: dict[tuple[int, int], int] = {pair: 256 + i for i, pair in enumerate(merges)}
+    
+    def encode(self, text: str) -> list[int]:
+        if not text:
+            return []
+        
+        tokens = list(text.encode('utf-8'))
+        
+        for pair in self.merges:
+            token_id = self.pair_to_token[pair]
+            tokens = self._apply_merge_exhaustive(tokens, pair, token_id)
+        
+        return tokens
+    
+    def _apply_merge_exhaustive(self, tokens: list[int], pair: tuple[int, int], new_token: int) -> list[int]:
+        while True:
+            new_tokens = self._apply_merge(tokens, pair, new_token)
+            if new_tokens == tokens:
+                break
+            tokens = new_tokens
+        return tokens
+    
+    def _apply_merge(self, tokens: list[int], pair: tuple[int, int], new_token: int) -> list[int]:
+        if len(tokens) < 2:
+            return tokens
+        
+        result = []
+        i = 0
+        
+        while i < len(tokens):
+            if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:
+                result.append(new_token)
+                i += 2
+            else:
+                result.append(tokens[i])
+                i += 1
+        
+        return result
+    
+    def decode(self, tokens: list[int]) -> str:
+        if not tokens:
+            return ""
+        
+        for token in tokens:
+            if token < 0:
+                raise ValueError(f"Token IDs must be non-negative, got {token}")
+            if token >= self.vocab_size:
+                raise ValueError(f"Token {token} exceeds vocabulary size {self.vocab_size}")
+        
+        bytes_list = []
+        for token in tokens:
+            bytes_list.extend(self._decompose_token(token))
+        
+        byte_array = bytes(bytes_list)
+        return byte_array.decode('utf-8', errors='replace')
+    
+    def _decompose_token(self, token: int) -> list[int]:
+        if token < 256:
+            return [token]
+        
+        left, right = self.merge_map[token]
+        return self._decompose_token(left) + self._decompose_token(right)
+
+
+def main():
+    tokenizer = ByteBPE([(240, 159)])
+    
+    text = "ðŸ‘‹"
+    tokens = tokenizer.encode(text)
+    decoded = tokenizer.decode(tokens)
+    
+    print(f"Original: {text}")
+    print(f"Encoded: {tokens}")
+    print(f"Decoded: {decoded}")
+    print(f"Round-trip: {decoded == text}")
+
+
+if __name__ == "__main__":
+    main()
