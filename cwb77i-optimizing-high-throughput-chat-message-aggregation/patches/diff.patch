diff --git a/repository_before/main.go b/repository_after/main.go
index f67373a..cbcd0d0 100644
--- a/repository_before/main.go
+++ b/repository_after/main.go
@@ -6,6 +6,7 @@ import (
 	"time"
 )
 
+// Message represents a chat message.
 type Message struct {
 	RoomID    string    `json:"room_id"`
 	UserID    string    `json:"user_id"`
@@ -13,55 +14,117 @@ type Message struct {
 	Timestamp time.Time `json:"timestamp"`
 }
 
+// Optimization Constants
+const (
+	numShards          = 64   // Number of shards to balance lock contention
+	maxMessagesPerRoom = 1000 // Upper bound for messages per room to prevent OOM
+	initialRoomCap     = 100  // Pre-allocation hint for room buffers
+)
+
+// shard represents a subset of the aggregator's data with its own lock.
+type shard struct {
+	mu      sync.Mutex
+	buffers map[string][]*Message
+}
+
+// Aggregator handles high-throughput message batching using lock sharding.
 type Aggregator struct {
-	mu       sync.Mutex
-	buffers  map[string][]*Message // roomID -> messages
+	shards [numShards]*shard
 }
 
+// NewAggregator initializes a sharded aggregator.
 func NewAggregator() *Aggregator {
-	return &Aggregator{
-		buffers: make(map[string][]*Message),
+	a := &Aggregator{}
+	for i := 0; i < numShards; i++ {
+		a.shards[i] = &shard{
+			buffers: make(map[string][]*Message),
+		}
 	}
+	return a
 }
 
-// AddMessage is called from multiple goroutines (one per WebSocket)
+// hash provides a fast FNV-1a like hashing for string room IDs.
+func (a *Aggregator) hash(s string) uint32 {
+	h := uint32(2166136261)
+	for i := 0; i < len(s); i++ {
+		h *= 16777619
+		h ^= uint32(s[i])
+	}
+	return h
+}
+
+// AddMessage is called from multiple goroutines concurrently.
+// It uses sharding to minimize lock contention and bounds memory usage per room.
 func (a *Aggregator) AddMessage(msg *Message) {
-	a.mu.Lock()
-	defer a.mu.Unlock()
+	if msg == nil {
+		return
+	}
+
+	// 1. Determine which shard this room belongs to
+	shardID := a.hash(msg.RoomID) % numShards
+	s := a.shards[shardID]
 
-	if _, ok := a.buffers[msg.RoomID]; !ok {
-		a.buffers[msg.RoomID] = make([]*Message, 0, 100)
+	// 2. Lock only the relevant shard
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	msgs := s.buffers[msg.RoomID]
+
+	// 3. Memory Bounding: Use a sliding window if the buffer is full
+	if len(msgs) >= maxMessagesPerRoom {
+		// Optimization: Shift existing elements to drop the oldest.
+		// For slices of 1000, this is extremely fast compared to an OOM crash.
+		copy(msgs, msgs[1:])
+		msgs[len(msgs)-1] = msg
+		s.buffers[msg.RoomID] = msgs
+	} else {
+		// Pre-allocate first time to reduce allocation frequency
+		if msgs == nil {
+			msgs = make([]*Message, 0, initialRoomCap)
+		}
+		s.buffers[msg.RoomID] = append(msgs, msg)
 	}
-	a.buffers[msg.RoomID] = append(a.buffers[msg.RoomID], msg)
 }
 
-// Flush is called periodically by a single background goroutine
+// Flush collects all current message batches room-by-room.
+// It iterates through shards one-by-one to avoid holding a global lock.
 func (a *Aggregator) Flush() map[string][]*Message {
-	a.mu.Lock()
-	defer a.mu.Unlock()
-
 	batches := make(map[string][]*Message)
-	for roomID, msgs := range a.buffers {
-		if len(msgs) > 0 {
-			batches[roomID] = msgs
-			a.buffers[roomID] = nil // reset buffer
+
+	for i := 0; i < numShards; i++ {
+		s := a.shards[i]
+
+		// Lock the shard briefly to extract data
+		s.mu.Lock()
+		for roomID, msgs := range s.buffers {
+			if len(msgs) > 0 {
+				// Create a copy of the slice to avoid data races
+				msgsCopy := make([]*Message, len(msgs))
+				copy(msgsCopy, msgs)
+				batches[roomID] = msgsCopy
+				// Reset the buffer for this room, keeping the underlying array for reuse
+				s.buffers[roomID] = msgs[:0]
+			}
 		}
+		s.mu.Unlock()
 	}
+
 	return batches
 }
 
-// Background flusher (simplified)
+// StartFlusher executes periodic flushing in a background goroutine.
 func (a *Aggregator) StartFlusher(interval time.Duration) {
 	ticker := time.NewTicker(interval)
 	go func() {
 		for range ticker.C {
 			batches := a.Flush()
-			// Simulate DB write / broadcast
+
+			// Process batches (e.g., push to DB or broadcast)
 			for _, msgs := range batches {
+				// The prompt expects JSON marshaling as part of the simulation
 				jsonData, _ := json.Marshal(msgs)
-				// ... write to DB, broadcast ...
-				_ = jsonData
+				_ = jsonData // Simulating data sink
 			}
 		}
 	}()
-}
\ No newline at end of file
+}
