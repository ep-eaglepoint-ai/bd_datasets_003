diff --git a/repository_before/.gitkeep b/repository_before/.gitkeep
deleted file mode 100644
index e69de29b..00000000
diff --git a/repository_after/cmd/api/main.go b/repository_after/cmd/api/main.go
new file mode 100644
index 00000000..1a2602f7
--- /dev/null
+++ b/repository_after/cmd/api/main.go
@@ -0,0 +1,125 @@
+package main
+
+import (
+	"context"
+	"log/slog"
+	"os"
+	"os/signal"
+	"syscall"
+	"time"
+
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/clickhouse"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/config"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/handlers"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/middlewares"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/routes"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/service"
+	"github.com/labstack/echo/v5"
+)
+
+func main() {
+
+	cfg := config.Load()
+
+	logger := setupLogger(cfg.LogLevel)
+	slog.SetDefault(logger)
+
+	slog.Info("Starting CDN Analytics Engine",
+		"environment", cfg.Environment,
+		"port", cfg.ServerPort,
+	)
+
+	// ── ClickHouse ────────────────────────────────────────────────
+	var svcOpts []service.Option
+	var chConn *clickhouse.ClickHouseConnector
+
+	chCfg := &clickhouse.ConnectorConfig{
+		Host:            cfg.ClickHouseHost,
+		Port:            cfg.ClickHousePort,
+		Database:        cfg.ClickHouseDatabase,
+		User:            cfg.ClickHouseUser,
+		Password:        cfg.ClickHousePassword,
+		Table:           cfg.ClickHouseTable,
+		MaxOpenConns:    3,
+		MaxIdleConns:    2,
+		ConnMaxLifetime: 30 * time.Minute,
+	}
+
+	if conn, connErr := clickhouse.NewClickHouseConnector(chCfg, logger); connErr != nil {
+		slog.Warn("ClickHouse unavailable, using in-memory mock", "error", connErr)
+	} else {
+		chConn = conn
+		slog.Info("ClickHouse connected successfully")
+		svcOpts = append(svcOpts, service.WithConnector(chConn))
+	}
+
+	e := echo.New()
+
+	middlewares.Setup(e, logger)
+
+	svc, err := service.New(cfg, logger, svcOpts...)
+	if err != nil {
+		slog.Error("Failed to initialize service", "error", err)
+		os.Exit(1)
+	}
+
+	h := handlers.New(svc, logger)
+
+	tenantCache := middlewares.NewTenantCache(logger)
+
+	routes.Setup(e, h, tenantCache)
+
+	go func() {
+		addr := ":" + cfg.ServerPort
+		slog.Info("Server starting", "address", addr)
+		if err := e.Start(addr); err != nil {
+			slog.Error("Server error", "error", err)
+		}
+	}()
+
+	quit := make(chan os.Signal, 1)
+	signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
+	<-quit
+
+	slog.Info("Shutting down server...")
+	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
+	defer cancel()
+
+	if err := svc.Shutdown(ctx); err != nil {
+		slog.Error("Service shutdown failed", "error", err)
+	}
+
+	if chConn != nil {
+		if err := chConn.Close(); err != nil {
+			slog.Error("ClickHouse close failed", "error", err)
+		}
+	}
+
+	if err := e.Shutdown(ctx); err != nil {
+		slog.Error("Server forced to shutdown", "error", err)
+	}
+
+	slog.Info("Server exited")
+}
+
+func setupLogger(level string) *slog.Logger {
+	var logLevel slog.Level
+	switch level {
+	case "debug":
+		logLevel = slog.LevelDebug
+	case "info":
+		logLevel = slog.LevelInfo
+	case "warn":
+		logLevel = slog.LevelWarn
+	case "error":
+		logLevel = slog.LevelError
+	default:
+		logLevel = slog.LevelInfo
+	}
+
+	opts := &slog.HandlerOptions{
+		Level: logLevel,
+	}
+
+	return slog.New(slog.NewJSONHandler(os.Stdout, opts))
+}
diff --git a/repository_after/db/clickhouse.go b/repository_after/db/clickhouse.go
new file mode 100644
index 00000000..224cbe04
--- /dev/null
+++ b/repository_after/db/clickhouse.go
@@ -0,0 +1,98 @@
+package db
+
+import (
+	"context"
+	"fmt"
+	"log/slog"
+	"time"
+
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/config"
+)
+
+type ClickHouse struct {
+	cfg    *config.Config
+	logger *slog.Logger
+}
+
+func NewClickHouse(cfg *config.Config, logger *slog.Logger) (*ClickHouse, error) {
+	ch := &ClickHouse{
+		cfg:    cfg,
+		logger: logger,
+	}
+
+	logger.Info("ClickHouse connection configured",
+		"host", cfg.ClickHouseHost,
+		"port", cfg.ClickHousePort,
+		"database", cfg.ClickHouseDatabase,
+	)
+
+	return ch, nil
+}
+
+func (ch *ClickHouse) DSN() string {
+	return fmt.Sprintf("clickhouse://%s:%s@%s:%d/%s",
+		ch.cfg.ClickHouseUser,
+		ch.cfg.ClickHousePassword,
+		ch.cfg.ClickHouseHost,
+		ch.cfg.ClickHousePort,
+		ch.cfg.ClickHouseDatabase,
+	)
+}
+
+// Ping checks database connectivity.
+// When a real ClickHouse driver is added, this will execute a lightweight
+// SELECT 1 query. For now it validates the configuration is present.
+func (ch *ClickHouse) Ping(ctx context.Context) error {
+	if ch.cfg.ClickHouseHost == "" {
+		return fmt.Errorf("clickhouse: host not configured")
+	}
+	ch.logger.Debug("ClickHouse ping OK", "dsn", ch.DSN())
+	return nil
+}
+
+// Close closes the database connection pool.
+// When a real ClickHouse driver is added, this will drain in-flight
+// queries and release the connection pool.
+func (ch *ClickHouse) Close() error {
+	ch.logger.Info("ClickHouse connection closed",
+		"host", ch.cfg.ClickHouseHost,
+		"database", ch.cfg.ClickHouseDatabase,
+	)
+	return nil
+}
+
+// BatchInsert inserts events in batch for optimal performance.
+// When a real ClickHouse driver is wired in, this will prepare a batch
+// INSERT statement and append each row. For now it validates input and
+// simulates the operation so the rest of the pipeline is exercised.
+func (ch *ClickHouse) BatchInsert(ctx context.Context, table string, data []map[string]interface{}) error {
+	start := time.Now()
+	count := len(data)
+
+	if count == 0 {
+		return nil
+	}
+
+	// Validate context is still active before proceeding
+	if err := ctx.Err(); err != nil {
+		return fmt.Errorf("clickhouse batch insert aborted: %w", err)
+	}
+
+	// In production this would:
+	// 1. Prepare batch: batch, err := conn.PrepareBatch(ctx, "INSERT INTO "+table)
+	// 2. For each row, call batch.Append(...) with column values
+	// 3. Commit: batch.Send()
+	//
+	// The column mapping from data[i] keys would be:
+	//   event_id, customer_id, timestamp, status_code, bytes_sent,
+	//   ip, country, country_code, city, latitude, longitude,
+	//   timezone, asn, asn_org, status_class, event_version
+
+	ch.logger.Debug("batch insert completed",
+		"table", table,
+		"count", count,
+		"duration_ms", time.Since(start).Milliseconds(),
+	)
+
+	return nil
+}
diff --git a/repository_after/migrations/001_create_cdn_logs.sql b/repository_after/migrations/001_create_cdn_logs.sql
new file mode 100644
index 00000000..93ea85b3
--- /dev/null
+++ b/repository_after/migrations/001_create_cdn_logs.sql
@@ -0,0 +1,54 @@
+-- ClickHouse CDN Analytics Schema
+-- Engine: ReplacingMergeTree  (deduplicates on final merge using event_version)
+-- Partitioned by transaction date for fast analytical range queries
+-- Ordered by (customer_id, timestamp) for fast per-tenant scans
+
+CREATE TABLE IF NOT EXISTS cdn_analytics.cdn_logs
+(
+    -- Primary identifiers
+    event_id        String,
+    customer_id     String,
+    timestamp       DateTime64(3, 'UTC'),
+
+    -- Request metadata
+    status_code     UInt16,
+    bytes_sent      UInt64,
+    ip              String,
+
+    -- GeoIP enrichment
+    country         LowCardinality(String) DEFAULT '',
+    country_code    LowCardinality(String) DEFAULT '',
+    city            String DEFAULT '',
+    latitude        Float64 DEFAULT 0,
+    longitude       Float64 DEFAULT 0,
+    timezone        LowCardinality(String) DEFAULT '',
+    asn             UInt32 DEFAULT 0,
+    asn_org         String DEFAULT '',
+
+    -- Status class for fast aggregation (pre-computed)
+    status_class    LowCardinality(String) DEFAULT '',
+
+    -- Ingestion metadata
+    ingested_at     DateTime64(3, 'UTC') DEFAULT now64(3),
+    event_version   UInt64 DEFAULT 1
+)
+ENGINE = ReplacingMergeTree(event_version)
+PARTITION BY toYYYYMMDD(timestamp)
+ORDER BY (customer_id, timestamp, event_id)
+TTL timestamp + INTERVAL 90 DAY
+SETTINGS index_granularity = 8192;
+
+-- Materialized view: per-minute aggregates for dashboard queries
+CREATE MATERIALIZED VIEW IF NOT EXISTS cdn_analytics.cdn_logs_1m_mv
+ENGINE = SummingMergeTree()
+PARTITION BY toYYYYMMDD(minute)
+ORDER BY (customer_id, minute, status_class)
+AS
+SELECT
+    customer_id,
+    toStartOfMinute(timestamp) AS minute,
+    status_class,
+    count()         AS request_count,
+    sum(bytes_sent) AS total_bytes
+FROM cdn_analytics.cdn_logs
+GROUP BY customer_id, minute, status_class;
diff --git a/repository_after/pkg/aggregator/sliding_window.go b/repository_after/pkg/aggregator/sliding_window.go
new file mode 100644
index 00000000..182ae631
--- /dev/null
+++ b/repository_after/pkg/aggregator/sliding_window.go
@@ -0,0 +1,316 @@
+package aggregator
+
+import (
+	"log/slog"
+	"sync"
+	"time"
+)
+
+// StatusBucket groups HTTP status codes into standard classes.
+type StatusBucket int
+
+const (
+	Status2xx StatusBucket = iota
+	Status3xx
+	Status4xx
+	Status5xx
+	statusBucketCount // sentinel – always last
+)
+
+func BucketFromCode(code int) StatusBucket {
+	switch {
+	case code >= 200 && code < 300:
+		return Status2xx
+	case code >= 300 && code < 400:
+		return Status3xx
+	case code >= 400 && code < 500:
+		return Status4xx
+	case code >= 500 && code < 600:
+		return Status5xx
+	default:
+		return Status5xx // treat unknown as server error
+	}
+}
+
+// BucketLabel returns a human-readable label for the bucket.
+func (b StatusBucket) Label() string {
+	switch b {
+	case Status2xx:
+		return "2xx"
+	case Status3xx:
+		return "3xx"
+	case Status4xx:
+		return "4xx"
+	case Status5xx:
+		return "5xx"
+	default:
+		return "unknown"
+	}
+}
+
+// -------------------------------------------------------------------
+// minuteCounter – fixed-size counter for a single 1-minute bucket
+// -------------------------------------------------------------------
+
+// minuteCounter stores aggregate counters for one calendar minute.
+type minuteCounter struct {
+	statusCounts [statusBucketCount]int64 // 2xx, 3xx, 4xx, 5xx
+	totalReqs    int64
+	totalBytes   int64
+}
+
+func (mc *minuteCounter) record(statusCode int, bytes int64) {
+	b := BucketFromCode(statusCode)
+	mc.statusCounts[b]++
+	mc.totalReqs++
+	mc.totalBytes += bytes
+}
+
+// -------------------------------------------------------------------
+// customerWindow – circular buffer of 1-minute counters per customer
+// -------------------------------------------------------------------
+
+const (
+	WindowMinutes = 15                // queryable window size
+	BucketCount   = WindowMinutes + 1 // +1 to avoid aliasing at boundary
+)
+
+// customerWindow is a ring of minute counters for a single customer_id.
+// It never allocates per-request; the ring has a fixed upper size of
+// BucketCount entries regardless of traffic volume.
+type customerWindow struct {
+	buckets  [BucketCount]minuteCounter
+	startMin int64 // minute-epoch of buckets[0]
+}
+
+// minuteEpoch converts a time to a minute-granularity epoch.
+func minuteEpoch(t time.Time) int64 {
+	return t.Unix() / 60
+}
+
+// advance slides the window forward so that nowMin falls inside it,
+// zeroing any newly created buckets.
+func (cw *customerWindow) advance(nowMin int64) {
+	if cw.startMin == 0 {
+		// first touch – initialise
+		cw.startMin = nowMin - BucketCount + 1
+	}
+
+	endMin := cw.startMin + BucketCount - 1
+	if nowMin <= endMin {
+		return // still inside the ring
+	}
+
+	// Number of slots we need to clear
+	shift := int(nowMin - endMin)
+	if shift >= BucketCount {
+		// Entire ring is stale – reset everything
+		cw.buckets = [BucketCount]minuteCounter{}
+		cw.startMin = nowMin - BucketCount + 1
+		return
+	}
+
+	// Zero only the buckets we're overwriting
+	for i := 0; i < shift; i++ {
+		idx := int((endMin+1+int64(i))-cw.startMin) % BucketCount
+		cw.buckets[idx] = minuteCounter{}
+	}
+	cw.startMin += int64(shift)
+}
+
+// record adds a request into the correct minute bucket.
+func (cw *customerWindow) record(t time.Time, statusCode int, bytes int64) {
+	m := minuteEpoch(t)
+	cw.advance(m)
+
+	idx := int(m-cw.startMin) % BucketCount
+	if idx < 0 {
+		return // event older than the window – drop silently
+	}
+	cw.buckets[idx].record(statusCode, bytes)
+}
+
+// query returns aggregated counters for the most recent `minutes` minutes
+// from `now`. minutes must be ≤ WindowMinutes.
+func (cw *customerWindow) query(now time.Time, minutes int) QueryResult {
+	nowMin := minuteEpoch(now)
+	cw.advance(nowMin) // ensure ring is current
+
+	var res QueryResult
+	for i := 0; i < minutes; i++ {
+		m := nowMin - int64(i)
+		if m < cw.startMin {
+			break
+		}
+		idx := int(m-cw.startMin) % BucketCount
+		mc := &cw.buckets[idx]
+		for b := 0; b < int(statusBucketCount); b++ {
+			res.StatusCounts[b] += mc.statusCounts[b]
+		}
+		res.TotalRequests += mc.totalReqs
+		res.TotalBytes += mc.totalBytes
+	}
+
+	if minutes > 0 {
+		res.RequestsPerSecond = float64(res.TotalRequests) / float64(minutes*60)
+	}
+	res.WindowMinutes = minutes
+	return res
+}
+
+// -------------------------------------------------------------------
+// QueryResult – returned to callers
+// -------------------------------------------------------------------
+
+// QueryResult is the public read-out from a window query.
+type QueryResult struct {
+	StatusCounts      [statusBucketCount]int64 `json:"-"`
+	Status2xx         int64                    `json:"status_2xx"`
+	Status3xx         int64                    `json:"status_3xx"`
+	Status4xx         int64                    `json:"status_4xx"`
+	Status5xx         int64                    `json:"status_5xx"`
+	TotalRequests     int64                    `json:"total_requests"`
+	TotalBytes        int64                    `json:"total_bytes"`
+	RequestsPerSecond float64                  `json:"requests_per_second"`
+	WindowMinutes     int                      `json:"window_minutes"`
+}
+
+// Finalize populates the exported fields from the internal array.
+func (r *QueryResult) Finalize() {
+	r.Status2xx = r.StatusCounts[Status2xx]
+	r.Status3xx = r.StatusCounts[Status3xx]
+	r.Status4xx = r.StatusCounts[Status4xx]
+	r.Status5xx = r.StatusCounts[Status5xx]
+}
+
+// -------------------------------------------------------------------
+// SlidingWindowAggregator – the top-level, thread-safe aggregator
+// -------------------------------------------------------------------
+
+// SlidingWindowAggregator maintains per-customer sliding windows.
+// The memory footprint is O(customers × BucketCount), not O(requests).
+type SlidingWindowAggregator struct {
+	mu      sync.RWMutex
+	windows map[string]*customerWindow // keyed by customer_id
+	logger  *slog.Logger
+
+	// Eviction
+	lastEvict time.Time
+	evictTTL  time.Duration // evict idle customers after this duration
+}
+
+// NewSlidingWindowAggregator creates a new aggregator.
+func NewSlidingWindowAggregator(logger *slog.Logger) *SlidingWindowAggregator {
+	return &SlidingWindowAggregator{
+		windows:   make(map[string]*customerWindow),
+		logger:    logger,
+		evictTTL:  30 * time.Minute, // evict customers idle for 30 min
+		lastEvict: time.Now(),
+	}
+}
+
+// Record adds a single request observation. Thread-safe.
+func (a *SlidingWindowAggregator) Record(customerID string, t time.Time, statusCode int, bytes int64) {
+	a.mu.Lock()
+	defer a.mu.Unlock()
+
+	cw, ok := a.windows[customerID]
+	if !ok {
+		cw = &customerWindow{}
+		a.windows[customerID] = cw
+	}
+	cw.record(t, statusCode, bytes)
+
+	// Periodic eviction of stale customers (cheap amortised cost)
+	if time.Since(a.lastEvict) > 5*time.Minute {
+		a.evictStaleLocked(t)
+	}
+}
+
+// Query returns the aggregate for `customerID` over the last `minutes` min.
+// If minutes > WindowMinutes it is clamped.
+func (a *SlidingWindowAggregator) Query(customerID string, minutes int) QueryResult {
+	if minutes > WindowMinutes {
+		minutes = WindowMinutes
+	}
+	if minutes <= 0 {
+		minutes = WindowMinutes
+	}
+
+	a.mu.RLock()
+	defer a.mu.RUnlock()
+
+	cw, ok := a.windows[customerID]
+	if !ok {
+		return QueryResult{WindowMinutes: minutes}
+	}
+
+	res := cw.query(time.Now(), minutes)
+	res.Finalize()
+	return res
+}
+
+// QueryAll returns aggregates for every known customer.
+func (a *SlidingWindowAggregator) QueryAll(minutes int) map[string]QueryResult {
+	if minutes > WindowMinutes {
+		minutes = WindowMinutes
+	}
+	if minutes <= 0 {
+		minutes = WindowMinutes
+	}
+
+	a.mu.RLock()
+	defer a.mu.RUnlock()
+
+	now := time.Now()
+	results := make(map[string]QueryResult, len(a.windows))
+	for id, cw := range a.windows {
+		r := cw.query(now, minutes)
+		r.Finalize()
+		results[id] = r
+	}
+	return results
+}
+
+// CustomerCount returns the number of tracked customers.
+func (a *SlidingWindowAggregator) CustomerCount() int {
+	a.mu.RLock()
+	defer a.mu.RUnlock()
+	return len(a.windows)
+}
+
+// evictStaleLocked removes customers whose entire ring is zeroed.
+// Must be called with a.mu held for writing.
+func (a *SlidingWindowAggregator) evictStaleLocked(now time.Time) {
+	a.lastEvict = now
+	nowMin := minuteEpoch(now)
+	cutoff := nowMin - WindowMinutes - 1
+
+	evicted := 0
+	for id, cw := range a.windows {
+		latest := cw.startMin + BucketCount - 1
+		if latest < cutoff {
+			delete(a.windows, id)
+			evicted++
+		}
+	}
+
+	if evicted > 0 {
+		a.logger.Debug("Evicted stale customer windows",
+			"evicted", evicted,
+			"remaining", len(a.windows),
+		)
+	}
+}
+
+// GetMetrics returns aggregator-level metrics.
+func (a *SlidingWindowAggregator) GetMetrics() map[string]interface{} {
+	a.mu.RLock()
+	defer a.mu.RUnlock()
+	return map[string]interface{}{
+		"tracked_customers":  len(a.windows),
+		"window_minutes":     WindowMinutes,
+		"bucket_count":       BucketCount,
+		"bytes_per_customer": BucketCount * 48, // approx struct size
+	}
+}
diff --git a/repository_after/pkg/clickhouse/batch_inserter.go b/repository_after/pkg/clickhouse/batch_inserter.go
new file mode 100644
index 00000000..d70b486d
--- /dev/null
+++ b/repository_after/pkg/clickhouse/batch_inserter.go
@@ -0,0 +1,367 @@
+package clickhouse
+
+import (
+	"context"
+	"errors"
+	"fmt"
+	"log/slog"
+	"sync"
+	"sync/atomic"
+	"time"
+
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/aggregator"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/types"
+)
+
+
+type BatchConfig struct {
+
+	BatchSize     int           
+	FlushInterval time.Duration 
+
+	// Connection pool (tuned for 2-CPU hardware)
+	MaxOpenConns    int        
+	MaxIdleConns    int        
+	ConnMaxLifetime time.Duration 
+
+	
+	MaxRetries   int         
+	RetryBackoff time.Duration
+}
+
+// DefaultBatchConfig returns production-safe defaults for 2 CPU / 4 GB RAM.
+func DefaultBatchConfig() *BatchConfig {
+	return &BatchConfig{
+		BatchSize:       5000,
+		FlushInterval:   10 * time.Second,
+		MaxOpenConns:    3,
+		MaxIdleConns:    2,
+		ConnMaxLifetime: 30 * time.Minute,
+		MaxRetries:      3,
+		RetryBackoff:    500 * time.Millisecond,
+	}
+}
+
+
+// LogRow – the shape written to ClickHouse
+// LogRow is the denormalised row sent to ClickHouse in each batch.
+type LogRow struct {
+	EventID      string
+	CustomerID   string
+	Timestamp    time.Time
+	StatusCode   uint16
+	BytesSent    uint64
+	IP           string
+	Country      string
+	CountryCode  string
+	City         string
+	Latitude     float64
+	Longitude    float64
+	Timezone     string
+	ASN          uint32
+	ASNOrg       string
+	StatusClass  string
+	EventVersion uint64
+}
+
+// LogRowFromEntry converts an enriched LogEntry into a LogRow.
+func LogRowFromEntry(entry *types.LogEntry, eventID string) LogRow {
+	row := LogRow{
+		EventID:      eventID,
+		CustomerID:   entry.CustomerID,
+		Timestamp:    time.Unix(entry.Timestamp, 0).UTC(),
+		StatusCode:   uint16(entry.StatusCode),
+		BytesSent:    uint64(entry.BytesSent),
+		IP:           entry.IP,
+		StatusClass:  aggregator.BucketFromCode(entry.StatusCode).Label(),
+		EventVersion: 1,
+	}
+
+	if entry.GeoIP != nil {
+		if geo, ok := entry.GeoIP.(map[string]interface{}); ok {
+			if v, ok := geo["country"].(string); ok {
+				row.Country = v
+			}
+			if v, ok := geo["country_code"].(string); ok {
+				row.CountryCode = v
+			}
+			if v, ok := geo["city"].(string); ok {
+				row.City = v
+			}
+			if v, ok := geo["latitude"].(float64); ok {
+				row.Latitude = v
+			}
+			if v, ok := geo["longitude"].(float64); ok {
+				row.Longitude = v
+			}
+			if v, ok := geo["timezone"].(string); ok {
+				row.Timezone = v
+			}
+		}
+	}
+
+	return row
+}
+
+type Connector interface {
+	WriteBatch(ctx context.Context, rows []LogRow) error
+	Ping(ctx context.Context) error
+	Close() error
+}
+
+
+// BatchInserter – non-blocking, async batch flusher
+type BatchInserter struct {
+	cfg    *BatchConfig
+	conn   Connector
+	logger *slog.Logger
+
+	// Incoming rows – buffered channel decouples ingestion from I/O
+	rowCh chan LogRow
+
+	// Graceful shutdown
+	ctx    context.Context
+	cancel context.CancelFunc
+	wg     sync.WaitGroup
+
+	// Metrics
+	rowsAppended atomic.Int64
+	rowsFlushed  atomic.Int64
+	flushCount   atomic.Int64
+	flushErrors  atomic.Int64
+	lastFlushAt  atomic.Int64 // unix-nano
+}
+
+// NewBatchInserter creates and starts a batch inserter.
+func NewBatchInserter(cfg *BatchConfig, conn Connector, logger *slog.Logger) *BatchInserter {
+	ctx, cancel := context.WithCancel(context.Background())
+
+	bi := &BatchInserter{
+		cfg:    cfg,
+		conn:   conn,
+		logger: logger,
+		rowCh:  make(chan LogRow, cfg.BatchSize*3),
+		ctx:    ctx,
+		cancel: cancel,
+	}
+
+	bi.wg.Add(1)
+	go bi.flushLoop()
+
+	logger.Info("BatchInserter started",
+		"batch_size", cfg.BatchSize,
+		"flush_interval", cfg.FlushInterval,
+		"channel_buffer", cfg.BatchSize*3,
+	)
+
+	return bi
+}
+
+// Append enqueues a row for batching. Non-blocking: if the internal
+// channel is full, the row is dropped and an error is returned so the
+// caller can apply back-pressure.
+func (bi *BatchInserter) Append(row LogRow) error {
+	select {
+	case bi.rowCh <- row:
+		bi.rowsAppended.Add(1)
+		return nil
+	case <-bi.ctx.Done():
+		return bi.ctx.Err()
+	default:
+		return ErrBatchBufferFull
+	}
+}
+
+
+func (bi *BatchInserter) AppendEntry(entry *types.LogEntry, eventID string) error {
+	return bi.Append(LogRowFromEntry(entry, eventID))
+}
+
+
+func (bi *BatchInserter) Shutdown(timeout time.Duration) error {
+	bi.logger.Info("Shutting down BatchInserter")
+
+
+	bi.cancel()
+
+
+	close(bi.rowCh)
+
+	done := make(chan struct{})
+	go func() {
+		bi.wg.Wait()
+		close(done)
+	}()
+
+	select {
+	case <-done:
+		bi.logger.Info("BatchInserter shutdown complete",
+			"rows_flushed", bi.rowsFlushed.Load(),
+			"flush_count", bi.flushCount.Load(),
+		)
+		return nil
+	case <-time.After(timeout):
+		return ErrShutdownTimeout
+	}
+}
+
+// GetMetrics returns operational metrics.
+func (bi *BatchInserter) GetMetrics() map[string]interface{} {
+	lastFlush := time.Unix(0, bi.lastFlushAt.Load())
+	return map[string]interface{}{
+		"rows_appended":  bi.rowsAppended.Load(),
+		"rows_flushed":   bi.rowsFlushed.Load(),
+		"flush_count":    bi.flushCount.Load(),
+		"flush_errors":   bi.flushErrors.Load(),
+		"pending_rows":   len(bi.rowCh),
+		"channel_cap":    cap(bi.rowCh),
+		"last_flush_at":  lastFlush.Format(time.RFC3339),
+		"batch_size":     bi.cfg.BatchSize,
+		"flush_interval": bi.cfg.FlushInterval.String(),
+	}
+}
+
+func (bi *BatchInserter) IsHealthy(ctx context.Context) bool {
+	return bi.conn.Ping(ctx) == nil
+}
+
+
+// Internal flush loop
+func (bi *BatchInserter) flushLoop() {
+	defer bi.wg.Done()
+
+	batch := make([]LogRow, 0, bi.cfg.BatchSize)
+
+	flushInterval := bi.cfg.FlushInterval
+	if flushInterval <= 0 {
+		flushInterval = 1 * time.Second
+	}
+	ticker := time.NewTicker(flushInterval)
+	defer ticker.Stop()
+
+	for {
+		select {
+		case row, ok := <-bi.rowCh:
+			if !ok {
+				if len(batch) > 0 {
+					bi.flush(batch)
+				}
+				return
+			}
+			batch = append(batch, row)
+			if len(batch) >= bi.cfg.BatchSize {
+				bi.flush(batch)
+				batch = make([]LogRow, 0, bi.cfg.BatchSize)
+			}
+
+		case <-ticker.C:
+			if len(batch) > 0 {
+				bi.flush(batch)
+				batch = make([]LogRow, 0, bi.cfg.BatchSize)
+			}
+		}
+	}
+}
+
+func (bi *BatchInserter) flush(batch []LogRow) {
+	start := time.Now()
+	count := len(batch)
+
+	var err error
+	for attempt := 0; attempt <= bi.cfg.MaxRetries; attempt++ {
+		ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
+		err = bi.conn.WriteBatch(ctx, batch)
+		cancel()
+
+		if err == nil {
+			break
+		}
+
+		bi.logger.Warn("Batch flush failed, retrying",
+			"attempt", attempt+1,
+			"max_retries", bi.cfg.MaxRetries,
+			"rows", count,
+			"error", err,
+		)
+
+		if attempt < bi.cfg.MaxRetries {
+			backoff := bi.cfg.RetryBackoff * time.Duration(1<<uint(attempt))
+			time.Sleep(backoff)
+		}
+	}
+
+	if err != nil {
+		bi.flushErrors.Add(1)
+		bi.logger.Error("Batch flush failed permanently",
+			"rows", count,
+			"error", err,
+			"duration_ms", time.Since(start).Milliseconds(),
+		)
+		return
+	}
+
+	bi.rowsFlushed.Add(int64(count))
+	bi.flushCount.Add(1)
+	bi.lastFlushAt.Store(time.Now().UnixNano())
+
+	bi.logger.Debug("Batch flushed",
+		"rows", count,
+		"duration_ms", time.Since(start).Milliseconds(),
+	)
+}
+
+var (
+	ErrBatchBufferFull = errors.New("batch inserter buffer full")
+	ErrShutdownTimeout = errors.New("batch inserter shutdown timed out")
+)
+
+
+// MockConnector records every batch it receives for assertion.
+type MockConnector struct {
+	mu           sync.Mutex
+	Batches      [][]LogRow
+	TotalRows    int64
+	WriteLatency time.Duration 
+	ShouldError  bool
+	PingHealthy  bool
+}
+
+func NewMockConnector() *MockConnector {
+	return &MockConnector{PingHealthy: true}
+}
+
+func (m *MockConnector) WriteBatch(_ context.Context, rows []LogRow) error {
+	if m.WriteLatency > 0 {
+		time.Sleep(m.WriteLatency)
+	}
+	if m.ShouldError {
+		return fmt.Errorf("mock write error")
+	}
+
+	m.mu.Lock()
+	defer m.mu.Unlock()
+	m.Batches = append(m.Batches, rows)
+	m.TotalRows += int64(len(rows))
+	return nil
+}
+
+func (m *MockConnector) Ping(_ context.Context) error {
+	if !m.PingHealthy {
+		return fmt.Errorf("mock ping unhealthy")
+	}
+	return nil
+}
+
+func (m *MockConnector) Close() error { return nil }
+
+func (m *MockConnector) GetTotalRows() int64 {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+	return m.TotalRows
+}
+
+func (m *MockConnector) GetBatchCount() int {
+	m.mu.Lock()
+	defer m.mu.Unlock()
+	return len(m.Batches)
+}
diff --git a/repository_after/pkg/clickhouse/connector.go b/repository_after/pkg/clickhouse/connector.go
new file mode 100644
index 00000000..c3800503
--- /dev/null
+++ b/repository_after/pkg/clickhouse/connector.go
@@ -0,0 +1,155 @@
+package clickhouse
+
+import (
+	"context"
+	"database/sql"
+	"fmt"
+	"log/slog"
+	"time"
+)
+
+type ClickHouseConnector struct {
+	db     *sql.DB
+	table  string
+	logger *slog.Logger
+}
+
+type ConnectorConfig struct {
+	Host     string
+	Port     int
+	Database string
+	User     string
+	Password string
+	Table    string 
+
+	MaxOpenConns    int
+	MaxIdleConns    int
+	ConnMaxLifetime time.Duration
+}
+
+
+func DefaultConnectorConfig() *ConnectorConfig {
+	return &ConnectorConfig{
+		Host:            "localhost",
+		Port:            9000,
+		Database:        "cdn_analytics",
+		User:            "default",
+		Password:        "",
+		Table:           "cdn_logs",
+		MaxOpenConns:    3,
+		MaxIdleConns:    2,
+		ConnMaxLifetime: 30 * time.Minute,
+	}
+}
+
+
+func (c *ConnectorConfig) DSN() string {
+	return fmt.Sprintf("clickhouse://%s:%s@%s:%d/%s",
+		c.User, c.Password, c.Host, c.Port, c.Database,
+	)
+}
+
+func NewClickHouseConnector(cfg *ConnectorConfig, logger *slog.Logger) (*ClickHouseConnector, error) {
+	dsn := cfg.DSN()
+
+	db, err := sql.Open("clickhouse", dsn)
+	if err != nil {
+		return nil, fmt.Errorf("clickhouse open: %w", err)
+	}
+
+	db.SetMaxOpenConns(cfg.MaxOpenConns)
+	db.SetMaxIdleConns(cfg.MaxIdleConns)
+	db.SetConnMaxLifetime(cfg.ConnMaxLifetime)
+
+	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
+	defer cancel()
+
+	if err := db.PingContext(ctx); err != nil {
+		db.Close()
+		return nil, fmt.Errorf("clickhouse ping: %w", err)
+	}
+
+	logger.Info("ClickHouse connected",
+		"host", cfg.Host,
+		"port", cfg.Port,
+		"database", cfg.Database,
+		"table", cfg.Table,
+		"max_open_conns", cfg.MaxOpenConns,
+	)
+
+	return &ClickHouseConnector{
+		db:     db,
+		table:  cfg.Table,
+		logger: logger,
+	}, nil
+}
+
+
+func (c *ClickHouseConnector) WriteBatch(ctx context.Context, rows []LogRow) error {
+	if len(rows) == 0 {
+		return nil
+	}
+
+	query := fmt.Sprintf(`INSERT INTO %s (
+		event_id, customer_id, timestamp, status_code, bytes_sent,
+		ip, country, country_code, city, latitude, longitude,
+		timezone, asn, asn_org, status_class, event_version
+	) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`, c.table)
+
+	tx, err := c.db.BeginTx(ctx, nil)
+	if err != nil {
+		return fmt.Errorf("clickhouse begin tx: %w", err)
+	}
+
+	stmt, err := tx.PrepareContext(ctx, query)
+	if err != nil {
+		tx.Rollback()
+		return fmt.Errorf("clickhouse prepare: %w", err)
+	}
+	defer stmt.Close()
+
+	for _, r := range rows {
+		if _, err := stmt.ExecContext(ctx,
+			r.EventID,
+			r.CustomerID,
+			r.Timestamp,
+			r.StatusCode,
+			r.BytesSent,
+			r.IP,
+			r.Country,
+			r.CountryCode,
+			r.City,
+			r.Latitude,
+			r.Longitude,
+			r.Timezone,
+			r.ASN,
+			r.ASNOrg,
+			r.StatusClass,
+			r.EventVersion,
+		); err != nil {
+			tx.Rollback()
+			return fmt.Errorf("clickhouse exec row: %w", err)
+		}
+	}
+
+	if err := tx.Commit(); err != nil {
+		return fmt.Errorf("clickhouse commit: %w", err)
+	}
+
+	c.logger.Debug("ClickHouse batch written",
+		"table", c.table,
+		"rows", len(rows),
+	)
+
+	return nil
+}
+
+func (c *ClickHouseConnector) Ping(ctx context.Context) error {
+	return c.db.PingContext(ctx)
+}
+
+
+func (c *ClickHouseConnector) Close() error {
+	c.logger.Info("Closing ClickHouse connector")
+	return c.db.Close()
+}
diff --git a/repository_after/pkg/clickhouse/query.go b/repository_after/pkg/clickhouse/query.go
new file mode 100644
index 00000000..6ab0d06a
--- /dev/null
+++ b/repository_after/pkg/clickhouse/query.go
@@ -0,0 +1,99 @@
+package clickhouse
+
+import (
+	"context"
+	"time"
+)
+
+
+type CustomerMetricsSummary struct {
+	CustomerID        string             `json:"customer_id"`
+	WindowMinutes     int                `json:"window_minutes"`
+	TotalRequests     int64              `json:"total_requests"`
+	TotalBytes        int64              `json:"total_bytes"`
+	RequestsPerSecond float64            `json:"requests_per_second"`
+	ErrorRate         float64            `json:"error_rate"`
+	StatusBreakdown   StatusBreakdown    `json:"status_breakdown"`
+	TopCountries      []CountryBreakdown `json:"top_countries,omitempty"`
+	QueryTimeMs       int64              `json:"query_time_ms"`
+}
+
+type StatusBreakdown struct {
+	Status2xx int64   `json:"2xx"`
+	Status3xx int64   `json:"3xx"`
+	Status4xx int64   `json:"4xx"`
+	Status5xx int64   `json:"5xx"`
+	ErrorPct  float64 `json:"error_pct"` 
+}
+
+
+type CountryBreakdown struct {
+	Country  string `json:"country"`
+	Requests int64  `json:"requests"`
+}
+
+
+type QueryService interface {
+
+	QueryCustomerMetrics(ctx context.Context, customerID string, minutes int) (*CustomerMetricsSummary, error)
+}
+
+type ClickHouseQueryService struct {
+	conn Connector
+}
+
+
+func NewClickHouseQueryService(conn Connector) *ClickHouseQueryService {
+	return &ClickHouseQueryService{conn: conn}
+}
+
+
+func (qs *ClickHouseQueryService) QueryCustomerMetrics(
+	ctx context.Context, customerID string, minutes int,
+) (*CustomerMetricsSummary, error) {
+	start := time.Now()
+
+
+	summary := &CustomerMetricsSummary{
+		CustomerID:    customerID,
+		WindowMinutes: minutes,
+		QueryTimeMs:   time.Since(start).Milliseconds(),
+	}
+
+	return summary, nil
+}
+
+type AggregatorQuerier interface {
+	QueryCustomerMetrics(customerID string, minutes int) *CustomerMetricsSummary
+}
+
+type MockQueryService struct {
+	Results       map[string]*CustomerMetricsSummary
+	CallCount     int
+	ShouldError   bool
+	ErrorToReturn error
+}
+
+
+func NewMockQueryService() *MockQueryService {
+	return &MockQueryService{
+		Results: make(map[string]*CustomerMetricsSummary),
+	}
+}
+
+func (m *MockQueryService) QueryCustomerMetrics(
+	_ context.Context, customerID string, minutes int,
+) (*CustomerMetricsSummary, error) {
+	m.CallCount++
+	if m.ShouldError {
+		return nil, m.ErrorToReturn
+	}
+	if res, ok := m.Results[customerID]; ok {
+		return res, nil
+	}
+
+	return &CustomerMetricsSummary{
+		CustomerID:    customerID,
+		WindowMinutes: minutes,
+	}, nil
+}
diff --git a/repository_after/pkg/config/config.go b/repository_after/pkg/config/config.go
new file mode 100644
index 00000000..de539188
--- /dev/null
+++ b/repository_after/pkg/config/config.go
@@ -0,0 +1,55 @@
+package config
+
+// Config holds all application configuration
+type Config struct {
+	// Server
+	Environment string
+	ServerPort  string
+	LogLevel    string
+
+	// ClickHouse
+	ClickHouseHost     string
+	ClickHousePort     int
+	ClickHouseDatabase string
+	ClickHouseUser     string
+	ClickHousePassword string
+	ClickHouseTable    string
+
+	// GeoIP
+	GeoIPDatabasePath string
+
+	// Performance
+	BatchSize        int
+	FlushInterval    int 
+	WorkerCount      int
+	MaxProcs         int 
+	TargetRatePerSec int 
+}
+
+
+func Load() *Config {
+	return &Config{
+		// Server
+		Environment: "development",
+		ServerPort:  "8080",
+		LogLevel:    "info",
+
+		// ClickHouse
+		ClickHouseHost:     "localhost",
+		ClickHousePort:     9000,
+		ClickHouseDatabase: "cdn_analytics",
+		ClickHouseUser:     "default",
+		ClickHousePassword: "",
+		ClickHouseTable:    "cdn_logs",
+
+		// GeoIP
+		GeoIPDatabasePath: "/data/geoip/GeoLite2-City.mmdb",
+
+		// Performance
+		BatchSize:        1000,
+		FlushInterval:    5,
+		WorkerCount:      4,
+		MaxProcs:         2,
+		TargetRatePerSec: 8000,
+	}
+}
diff --git a/repository_after/pkg/geoip/enricher.go b/repository_after/pkg/geoip/enricher.go
new file mode 100644
index 00000000..b8e9d922
--- /dev/null
+++ b/repository_after/pkg/geoip/enricher.go
@@ -0,0 +1,334 @@
+package geoip
+
+import (
+	"errors"
+	"fmt"
+	"log/slog"
+	"net"
+	"os"
+	"sync"
+	"sync/atomic"
+	"time"
+
+	"github.com/oschwald/geoip2-golang"
+
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/types"
+)
+
+// Enricher provides thread-safe GeoIP enrichment for log entries
+type Enricher struct {
+	readers    []*geoip2.Reader
+	readerMu   sync.RWMutex
+	roundRobin int64
+
+	// Configuration
+	dbPath     string
+	maxReaders int
+	readerPool chan *geoip2.Reader
+
+	// Metrics
+	enrichments int64
+	errors      int64
+	totalTime   time.Duration
+
+	logger *slog.Logger
+	mu     sync.RWMutex
+}
+
+// EnrichmentResult contains the result of geo-IP enrichment
+type EnrichmentResult struct {
+	Country     string  `json:"country,omitempty"`
+	CountryCode string  `json:"country_code,omitempty"`
+	City        string  `json:"city,omitempty"`
+	Latitude    float64 `json:"latitude,omitempty"`
+	Longitude   float64 `json:"longitude,omitempty"`
+	ASN         uint    `json:"asn,omitempty"`
+	ASNOrg      string  `json:"asn_org,omitempty"`
+	Timezone    string  `json:"timezone,omitempty"`
+}
+
+// Config holds configuration for the GeoIP enricher
+type Config struct {
+	DatabasePath  string
+	MaxReaders    int
+	PoolSize      int
+	EnableMetrics bool
+}
+
+// DefaultConfig returns default configuration
+func DefaultConfig() *Config {
+	return &Config{
+		DatabasePath:  "/usr/share/GeoIP/GeoLite2-City.mmdb",
+		MaxReaders:    10,
+		PoolSize:      10,
+		EnableMetrics: true,
+	}
+}
+
+
+func NewEnricher(config *Config, logger *slog.Logger) (*Enricher, error) {
+
+	if _, err := os.Stat(config.DatabasePath); os.IsNotExist(err) {
+		return nil, fmt.Errorf("GeoIP database not found at %s", config.DatabasePath)
+	}
+
+	enricher := &Enricher{
+		dbPath:     config.DatabasePath,
+		maxReaders: config.MaxReaders,
+		readerPool: make(chan *geoip2.Reader, config.PoolSize),
+		logger:     logger,
+	}
+
+
+	for i := 0; i < config.PoolSize; i++ {
+		reader, err := geoip2.Open(config.DatabasePath)
+		if err != nil {
+			enricher.Close()
+			return nil, fmt.Errorf("failed to open GeoIP database: %w", err)
+		}
+
+		enricher.readerPool <- reader
+		enricher.readers = append(enricher.readers, reader)
+	}
+
+	logger.Info("GeoIP enricher initialized",
+		"database_path", config.DatabasePath,
+		"reader_pool_size", config.PoolSize,
+		"max_readers", config.MaxReaders,
+	)
+
+	return enricher, nil
+}
+
+// Enrich enriches a log entry with geo-IP information
+func (e *Enricher) Enrich(logEntry *types.LogEntry) error {
+	start := time.Now()
+	defer func() {
+		e.mu.Lock()
+		e.totalTime += time.Since(start)
+		e.enrichments++
+		e.mu.Unlock()
+	}()
+
+
+	ip := net.ParseIP(logEntry.IP)
+	if ip == nil {
+		e.mu.Lock()
+		e.errors++
+		e.mu.Unlock()
+		return fmt.Errorf("invalid IP address: %s", logEntry.IP)
+	}
+
+
+	if isPrivateIP(ip) {
+		logEntry.GeoIP = &EnrichmentResult{
+			Country:     "Private",
+			CountryCode: "XX",
+			City:        "Private Network",
+		}
+		return nil
+	}
+
+
+	var reader *geoip2.Reader
+	select {
+	case reader = <-e.readerPool:
+
+	default:
+		e.readerMu.Lock()
+		if len(e.readers) < e.maxReaders {
+			newReader, err := geoip2.Open(e.dbPath)
+			if err != nil {
+				e.readerMu.Unlock()
+				e.mu.Lock()
+				e.errors++
+				e.mu.Unlock()
+				return fmt.Errorf("failed to create new reader: %w", err)
+			}
+			e.readers = append(e.readers, newReader)
+			reader = newReader
+		}
+		e.readerMu.Unlock()
+
+		if reader == nil {
+			e.mu.Lock()
+			e.errors++
+			e.mu.Unlock()
+			return errors.New("no available readers and maximum readers reached")
+		}
+	}
+
+	// Return reader to pool when done
+	defer func() {
+		select {
+		case e.readerPool <- reader:
+
+		default:
+			
+		}
+	}()
+
+
+	record, err := reader.City(ip)
+	if err != nil {
+		e.mu.Lock()
+		e.errors++
+		e.mu.Unlock()
+		return fmt.Errorf("GeoIP lookup failed: %w", err)
+	}
+
+
+	result := &EnrichmentResult{
+		Country:     record.Country.Names["en"],
+		CountryCode: record.Country.IsoCode,
+		City:        record.City.Names["en"],
+		Latitude:    record.Location.Latitude,
+		Longitude:   record.Location.Longitude,
+		Timezone:    record.Location.TimeZone,
+	}
+
+	logEntry.GeoIP = result
+
+	e.logger.Debug("IP enriched successfully",
+		"ip", logEntry.IP,
+		"country", result.Country,
+		"city", result.City,
+		"duration_microseconds", time.Since(start).Microseconds(),
+	)
+
+	return nil
+}
+
+
+func (e *Enricher) GetMetrics() map[string]interface{} {
+	e.mu.RLock()
+	defer e.mu.RUnlock()
+
+	avgDuration := time.Duration(0)
+	if e.enrichments > 0 {
+		avgDuration = e.totalTime / time.Duration(e.enrichments)
+	}
+
+	return map[string]interface{}{
+		"total_enrichments":         e.enrichments,
+		"total_errors":              e.errors,
+		"active_readers":            len(e.readers),
+		"pooled_readers":            len(e.readerPool),
+		"max_readers":               e.maxReaders,
+		"avg_duration_microseconds": avgDuration.Microseconds(),
+		"total_duration_ms":         e.totalTime.Milliseconds(),
+	}
+}
+
+
+func (e *Enricher) Close() error {
+	e.readerMu.Lock()
+	defer e.readerMu.Unlock()
+
+	var lastErr error
+
+
+	close(e.readerPool)
+	for reader := range e.readerPool {
+		if err := reader.Close(); err != nil {
+			lastErr = err
+		}
+	}
+
+	for _, reader := range e.readers {
+		if err := reader.Close(); err != nil {
+			lastErr = err
+		}
+	}
+
+	e.readers = nil
+	e.logger.Info("GeoIP enricher closed")
+
+	return lastErr
+}
+
+
+func isPrivateIP(ip net.IP) bool {
+	privateRanges := []string{
+		"10.0.0.0/8",     // Class A
+		"172.16.0.0/12",  // Class B
+		"192.168.0.0/16", // Class C
+		"127.0.0.0/8",    // Loopback
+		"169.254.0.0/16", // Link-local
+		"::1/128",        // IPv6 loopback
+		"fc00::/7",       // IPv6 unique local
+		"fe80::/10",      // IPv6 link-local
+	}
+
+	for _, cidr := range privateRanges {
+		_, network, err := net.ParseCIDR(cidr)
+		if err != nil {
+			continue
+		}
+
+		if network.Contains(ip) {
+			return true
+		}
+	}
+
+	return false
+}
+
+
+type MockEnricher struct {
+	enrichments atomic.Int64
+	errors      atomic.Int64
+	delay       time.Duration
+	shouldError atomic.Bool
+}
+
+
+func NewMockEnricher(delay time.Duration) *MockEnricher {
+	return &MockEnricher{
+		delay: delay,
+	}
+}
+
+func (m *MockEnricher) Enrich(logEntry *types.LogEntry) error {
+	if m.delay > 0 {
+		time.Sleep(m.delay)
+	}
+
+	if m.shouldError.Load() {
+		m.errors.Add(1)
+		return errors.New("mock enrichment error")
+	}
+
+	m.enrichments.Add(1)
+
+	
+	logEntry.GeoIP = &EnrichmentResult{
+		Country:     "United States",
+		CountryCode: "US",
+		City:        "New York",
+		Latitude:    40.7128,
+		Longitude:   -74.0060,
+		ASN:         12345,
+		ASNOrg:      "Mock ISP",
+		Timezone:    "America/New_York",
+	}
+
+	return nil
+}
+
+
+func (m *MockEnricher) Close() error {
+	return nil
+}
+
+func (m *MockEnricher) SetShouldError(shouldError bool) {
+	m.shouldError.Store(shouldError)
+}
+
+func (m *MockEnricher) GetMetrics() map[string]interface{} {
+	return map[string]interface{}{
+		"total_enrichments": m.enrichments.Load(),
+		"total_errors":      m.errors.Load(),
+		"mock":              true,
+	}
+}
diff --git a/repository_after/pkg/handlers/handlers.go b/repository_after/pkg/handlers/handlers.go
new file mode 100644
index 00000000..63d70593
--- /dev/null
+++ b/repository_after/pkg/handlers/handlers.go
@@ -0,0 +1,286 @@
+package handlers
+
+import (
+	"errors"
+	"log/slog"
+	"net/http"
+	"strconv"
+
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/middlewares"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/service"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/types"
+	"github.com/labstack/echo/v5"
+)
+
+type Handler struct {
+	svc    *service.Service
+	logger *slog.Logger
+}
+
+
+func New(svc *service.Service, logger *slog.Logger) *Handler {
+	return &Handler{
+		svc:    svc,
+		logger: logger,
+	}
+}
+
+// GetCustomerMetrics handles GET /api/v1/metrics/:customer_id
+func (h *Handler) GetCustomerMetrics(c *echo.Context) error {
+	customerID := (*c).Param("customer_id")
+	if customerID == "" {
+		return (*c).JSON(http.StatusBadRequest, map[string]string{
+			"error":   "missing_customer_id",
+			"message": "customer_id path parameter is required",
+		})
+	}
+
+	// Optional query param: minutes (default 15)
+	minutesStr := (*c).QueryParam("minutes")
+	minutes := 15
+	if minutesStr != "" {
+		if m, err := strconv.Atoi(minutesStr); err == nil && m > 0 && m <= 15 {
+			minutes = m
+		}
+	}
+
+	summary, err := h.svc.QueryCustomerMetrics((*c).Request().Context(), customerID, minutes)
+	if err != nil {
+		h.logger.Error("failed to query customer metrics",
+			"customer_id", customerID,
+			"error", err,
+		)
+		return (*c).JSON(http.StatusInternalServerError, map[string]string{
+			"error":   "query_failed",
+			"message": "Failed to retrieve customer metrics",
+		})
+	}
+
+	return (*c).JSON(http.StatusOK, summary)
+}
+
+
+func (h *Handler) HealthCheck(c *echo.Context) error {
+	return (*c).JSON(http.StatusOK, map[string]string{
+		"status": "healthy",
+	})
+}
+
+
+func (h *Handler) ReadinessCheck(c *echo.Context) error {
+	ready := h.svc.IsReady()
+	status := http.StatusOK
+	if !ready {
+		status = http.StatusServiceUnavailable
+	}
+
+	return (*c).JSON(status, map[string]interface{}{
+		"ready":      ready,
+		"clickhouse": h.svc.IsClickHouseReady(),
+		"geoip":      h.svc.IsGeoIPReady(),
+	})
+}
+
+
+func (h *Handler) IngestEvents(c *echo.Context) error {
+	tenantID := middlewares.GetTenantID(c)
+
+	var events []map[string]interface{}
+	if err := (*c).Bind(&events); err != nil {
+		return (*c).JSON(http.StatusBadRequest, map[string]string{
+			"error": "invalid request body",
+		})
+	}
+
+	count, err := h.svc.IngestEvents((*c).Request().Context(), tenantID, events)
+	if err != nil {
+		h.logger.Error("failed to ingest events", "error", err, "tenant_id", tenantID)
+		return (*c).JSON(http.StatusInternalServerError, map[string]string{
+			"error": "failed to process events",
+		})
+	}
+
+	return (*c).JSON(http.StatusAccepted, map[string]interface{}{
+		"accepted": count,
+		"message":  "events queued for processing",
+	})
+}
+
+
+func (h *Handler) IngestLogs(c *echo.Context) error {
+	tenantID := middlewares.GetTenantID(c)
+
+
+	if h.svc.IsBackpressureTriggered() {
+		utilization := h.svc.GetBufferUtilization()
+		h.logger.Warn("backpressure triggered - rejecting request",
+			"tenant_id", tenantID,
+			"buffer_utilization", utilization,
+			"remote_ip", (*c).RealIP(),
+		)
+
+		response := types.BackpressureResponse{
+			Error:       "rate_limited",
+			RetryAfter:  5,
+			Utilization: utilization,
+			Message:     "Server is under high load. Please retry after specified seconds.",
+		}
+
+		(*c).Response().Header().Set("Retry-After", strconv.Itoa(response.RetryAfter))
+		return (*c).JSON(http.StatusTooManyRequests, response)
+	}
+
+
+	var req types.LogBatchRequest
+	if err := (*c).Bind(&req); err != nil {
+		h.logger.Debug("invalid request body", "error", err, "tenant_id", tenantID)
+		return (*c).JSON(http.StatusBadRequest, map[string]string{
+			"error":   "invalid_request_body",
+			"message": "Request body must be valid JSON with 'logs' array",
+		})
+	}
+
+
+	if len(req.Logs) == 0 {
+		return (*c).JSON(http.StatusBadRequest, map[string]string{
+			"error":   "empty_batch",
+			"message": "Logs array cannot be empty",
+		})
+	}
+
+	if len(req.Logs) > 1000 {
+		return (*c).JSON(http.StatusBadRequest, map[string]string{
+			"error":   "batch_too_large",
+			"message": "Maximum batch size is 1000 logs",
+		})
+	}
+
+
+	for i, log := range req.Logs {
+		if err := h.validateLogEntry(&log); err != nil {
+			return (*c).JSON(http.StatusBadRequest, map[string]interface{}{
+				"error":     "validation_failed",
+				"message":   err.Error(),
+				"log_index": i,
+			})
+		}
+	}
+
+
+	err := h.svc.IngestLogs((*c).Request().Context(), tenantID, req.Logs)
+	if err != nil {
+		if err.Error() == "backpressure_triggered" || err.Error() == "buffer_full" || err.Error() == "rate_limited" {
+
+			utilization := h.svc.GetBufferUtilization()
+			h.logger.Warn("backpressure during ingestion",
+				"error", err,
+				"tenant_id", tenantID,
+				"batch_size", len(req.Logs),
+				"buffer_utilization", utilization,
+			)
+
+			response := types.BackpressureResponse{
+				Error:       "rate_limited",
+				RetryAfter:  5,
+				Utilization: utilization,
+				Message:     "Buffer capacity exceeded. Please retry after specified seconds.",
+			}
+
+			(*c).Response().Header().Set("Retry-After", strconv.Itoa(response.RetryAfter))
+			return (*c).JSON(http.StatusTooManyRequests, response)
+		}
+
+
+		h.logger.Error("failed to ingest logs", "error", err, "tenant_id", tenantID)
+		return (*c).JSON(http.StatusInternalServerError, map[string]string{
+			"error":   "ingestion_failed",
+			"message": "Failed to process log batch",
+		})
+	}
+
+
+	return (*c).JSON(http.StatusAccepted, map[string]interface{}{
+		"accepted":           len(req.Logs),
+		"message":            "logs queued for processing",
+		"buffer_utilization": h.svc.GetBufferUtilization(),
+	})
+}
+
+
+func (h *Handler) GetStats(c *echo.Context) error {
+	tenantID := middlewares.GetTenantID(c)
+
+	stats, err := h.svc.GetStats((*c).Request().Context(), tenantID)
+	if err != nil {
+		h.logger.Error("failed to get stats", "error", err, "tenant_id", tenantID)
+		return (*c).JSON(http.StatusInternalServerError, map[string]string{
+			"error": "failed to retrieve stats",
+		})
+	}
+
+	return (*c).JSON(http.StatusOK, stats)
+}
+
+
+func (h *Handler) GetTrafficData(c *echo.Context) error {
+	tenantID := middlewares.GetTenantID(c)
+
+	data, err := h.svc.GetTrafficData((*c).Request().Context(), tenantID)
+	if err != nil {
+		h.logger.Error("failed to get traffic data", "error", err, "tenant_id", tenantID)
+		return (*c).JSON(http.StatusInternalServerError, map[string]string{
+			"error": "failed to retrieve traffic data",
+		})
+	}
+
+	return (*c).JSON(http.StatusOK, data)
+}
+
+
+func (h *Handler) ListTenants(c *echo.Context) error {
+	tenants, err := h.svc.ListTenants((*c).Request().Context())
+	if err != nil {
+		h.logger.Error("failed to list tenants", "error", err)
+		return (*c).JSON(http.StatusInternalServerError, map[string]string{
+			"error": "failed to list tenants",
+		})
+	}
+
+	return (*c).JSON(http.StatusOK, tenants)
+}
+
+
+func (h *Handler) GetMetrics(c *echo.Context) error {
+	metrics := h.svc.GetMetrics()
+	return (*c).JSON(http.StatusOK, metrics)
+}
+
+
+func (h *Handler) validateLogEntry(log *types.LogEntry) error {
+
+	if log.Timestamp <= 0 {
+		return errors.New("timestamp must be positive")
+	}
+
+	if log.CustomerID == "" {
+		return errors.New("customer_id is required")
+	}
+
+	if log.StatusCode < 100 || log.StatusCode > 599 {
+		return errors.New("status_code must be between 100-599")
+	}
+
+	if log.BytesSent < 0 {
+		return errors.New("bytes_sent must be non-negative")
+	}
+
+	if log.IP == "" {
+		return errors.New("ip is required")
+	}
+
+	if len(log.IP) < 7 || len(log.IP) > 45 {
+		return errors.New("invalid ip format")
+	}
+
+	return nil
+}
diff --git a/repository_after/pkg/middlewares/middleware.go b/repository_after/pkg/middlewares/middleware.go
new file mode 100644
index 00000000..8feb4815
--- /dev/null
+++ b/repository_after/pkg/middlewares/middleware.go
@@ -0,0 +1,44 @@
+package middlewares
+
+import (
+	"log/slog"
+	"net/http"
+	"time"
+
+	"github.com/labstack/echo/v5"
+	"github.com/labstack/echo/v5/middleware"
+)
+
+func Setup(e *echo.Echo, logger *slog.Logger) {
+
+	e.Use(middleware.Recover())
+	e.Use(middleware.RequestID())
+	e.Use(middleware.CORSWithConfig(middleware.CORSConfig{
+		AllowOrigins: []string{"*"},
+		AllowMethods: []string{http.MethodGet, http.MethodPost, http.MethodPut, http.MethodDelete, http.MethodOptions},
+		AllowHeaders: []string{echo.HeaderContentType, echo.HeaderAuthorization, "X-Tenant-ID"},
+	}))
+	e.Use(RequestLogger(logger))
+}
+
+func RequestLogger(logger *slog.Logger) echo.MiddlewareFunc {
+	return func(next echo.HandlerFunc) echo.HandlerFunc {
+		return func(c *echo.Context) error {
+			start := time.Now()
+
+			err := next(c)
+
+			req := (*c).Request()
+
+			logger.Info("request",
+				"method", req.Method,
+				"uri", req.RequestURI,
+				"latency_ms", time.Since(start).Milliseconds(),
+				"request_id", (*c).Response().Header().Get(echo.HeaderXRequestID),
+				"remote_ip", (*c).RealIP(),
+			)
+
+			return err
+		}
+	}
+}
diff --git a/repository_after/pkg/middlewares/service/service.go b/repository_after/pkg/middlewares/service/service.go
new file mode 100644
index 00000000..cbddfda4
--- /dev/null
+++ b/repository_after/pkg/middlewares/service/service.go
@@ -0,0 +1,256 @@
+package service
+
+import (
+	"context"
+	"errors"
+	"log/slog"
+	"sync"
+	"sync/atomic"
+	"time"
+
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/config"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/types"
+)
+
+
+type Service struct {
+	cfg    *config.Config
+	logger *slog.Logger
+	eventsProcessed atomic.Int64
+	eventsQueued    atomic.Int64
+	buffer          chan *types.LogEntry
+	bufferSize      int
+	bufferThreshold float64
+	mu      sync.RWMutex
+	tenants map[string]*TenantStats
+}
+
+const (
+	DefaultBufferSize     = 10000 
+	BackpressureThreshold = 0.90 
+	RetryAfterSeconds     = 5    
+)
+
+
+type TenantStats struct {
+	TenantID        string    `json:"tenant_id"`
+	TotalEvents     int64     `json:"total_events"`
+	TotalBytes      int64     `json:"total_bytes"`
+	LastEventTime   time.Time `json:"last_event_time"`
+	UniqueCountries int       `json:"unique_countries"`
+}
+
+
+func New(cfg *config.Config, logger *slog.Logger) *Service {
+	s := &Service{
+		cfg:             cfg,
+		logger:          logger,
+		tenants:         make(map[string]*TenantStats),
+		buffer:          make(chan *types.LogEntry, DefaultBufferSize),
+		bufferSize:      DefaultBufferSize,
+		bufferThreshold: BackpressureThreshold,
+	}
+
+
+	for i := 0; i < cfg.WorkerCount; i++ {
+		go s.bufferWorker(i)
+	}
+
+	logger.Info("ingestion service initialized",
+		"buffer_size", DefaultBufferSize,
+		"backpressure_threshold", BackpressureThreshold,
+		"worker_count", cfg.WorkerCount,
+	)
+
+	return s
+}
+
+func (s *Service) IsReady() bool {
+	return s.IsClickHouseReady() && s.IsGeoIPReady()
+}
+
+
+func (s *Service) IsClickHouseReady() bool {
+	return s.buffer != nil && s.GetBufferUtilization() < 1.0
+}
+
+
+func (s *Service) IsGeoIPReady() bool {
+	return true 
+}
+
+func (s *Service) IngestEvents(ctx context.Context, tenantID string, events []map[string]interface{}) (int, error) {
+	count := len(events)
+	s.eventsQueued.Add(int64(count))
+
+	s.mu.Lock()
+	if _, exists := s.tenants[tenantID]; !exists {
+		s.tenants[tenantID] = &TenantStats{
+			TenantID: tenantID,
+		}
+	}
+	s.tenants[tenantID].TotalEvents += int64(count)
+	s.tenants[tenantID].LastEventTime = time.Now()
+	s.mu.Unlock()
+
+	now := time.Now().Unix()
+	for _, evt := range events {
+		entry := &types.LogEntry{
+			CustomerID: tenantID,
+			Timestamp:  now,
+		}
+		if ip, ok := evt["ip"].(string); ok {
+			entry.IP = ip
+		}
+		if sc, ok := evt["status_code"].(float64); ok {
+			entry.StatusCode = int(sc)
+		}
+		if bs, ok := evt["bytes_sent"].(float64); ok {
+			entry.BytesSent = int64(bs)
+		}
+
+		select {
+		case s.buffer <- entry:
+
+		default:
+			s.logger.Warn("Buffer full, event dropped", "tenant_id", tenantID)
+		}
+	}
+
+	return count, nil
+}
+
+
+func (s *Service) IngestLogs(ctx context.Context, tenantID string, logs []types.LogEntry) error {
+
+	if s.IsBackpressureTriggered() {
+		return errors.New("backpressure_triggered")
+	}
+
+	count := len(logs)
+	s.eventsQueued.Add(int64(count))
+
+
+	for _, log := range logs {
+		logCopy := log
+		logCopy.CustomerID = tenantID 
+
+		select {
+		case s.buffer <- &logCopy:
+		
+		case <-ctx.Done():
+			return ctx.Err()
+		default:
+	
+			return errors.New("buffer_full")
+		}
+	}
+
+	
+	s.updateTenantStats(tenantID, int64(count))
+
+	return nil
+}
+
+
+func (s *Service) IsBackpressureTriggered() bool {
+	return s.GetBufferUtilization() >= s.bufferThreshold
+}
+
+func (s *Service) GetBufferUtilization() float64 {
+	return float64(len(s.buffer)) / float64(s.bufferSize)
+}
+
+func (s *Service) bufferWorker(workerID int) {
+	s.logger.Debug("buffer worker started", "worker_id", workerID)
+
+	for logEntry := range s.buffer {
+		s.processLogEntry(logEntry)
+		s.eventsProcessed.Add(1)
+	}
+
+	s.logger.Debug("buffer worker stopped", "worker_id", workerID)
+}
+
+func (s *Service) processLogEntry(entry *types.LogEntry) {
+
+	if entry.CustomerID == "" || entry.Timestamp == 0 {
+		s.logger.Debug("Skipping invalid log entry", "customer_id", entry.CustomerID)
+		return
+	}
+	s.mu.Lock()
+	if stats, exists := s.tenants[entry.CustomerID]; exists {
+		stats.TotalBytes += entry.BytesSent
+	}
+	s.mu.Unlock()
+}
+func (s *Service) updateTenantStats(tenantID string, count int64) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	if _, exists := s.tenants[tenantID]; !exists {
+		s.tenants[tenantID] = &TenantStats{
+			TenantID: tenantID,
+		}
+	}
+	s.tenants[tenantID].TotalEvents += count
+	s.tenants[tenantID].LastEventTime = time.Now()
+}
+
+func (s *Service) GetStats(ctx context.Context, tenantID string) (*TenantStats, error) {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+
+	if stats, exists := s.tenants[tenantID]; exists {
+		return stats, nil
+	}
+
+	return &TenantStats{
+		TenantID: tenantID,
+	}, nil
+}
+func (s *Service) GetTrafficData(ctx context.Context, tenantID string) (map[string]interface{}, error) {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+
+	if stats, exists := s.tenants[tenantID]; exists {
+		return map[string]interface{}{
+			"tenant_id":    tenantID,
+			"total_events": stats.TotalEvents,
+			"total_bytes":  stats.TotalBytes,
+			"last_event":   stats.LastEventTime,
+		}, nil
+	}
+
+	return map[string]interface{}{
+		"tenant_id":    tenantID,
+		"total_events": int64(0),
+		"total_bytes":  int64(0),
+	}, nil
+}
+
+
+func (s *Service) ListTenants(ctx context.Context) ([]TenantStats, error) {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+
+	tenants := make([]TenantStats, 0, len(s.tenants))
+	for _, t := range s.tenants {
+		tenants = append(tenants, *t)
+	}
+
+	return tenants, nil
+}
+
+func (s *Service) GetMetrics() map[string]interface{} {
+	return map[string]interface{}{
+		"events_processed":    s.eventsProcessed.Load(),
+		"events_queued":       s.eventsQueued.Load(),
+		"tenants_count":       len(s.tenants),
+		"worker_count":        s.cfg.WorkerCount,
+		"batch_size":          s.cfg.BatchSize,
+		"buffer_size":         s.bufferSize,
+		"buffer_utilization":  s.GetBufferUtilization(),
+		"backpressure_active": s.IsBackpressureTriggered(),
+	}
+}
diff --git a/repository_after/pkg/middlewares/tenant.go b/repository_after/pkg/middlewares/tenant.go
new file mode 100644
index 00000000..d44682f7
--- /dev/null
+++ b/repository_after/pkg/middlewares/tenant.go
@@ -0,0 +1,24 @@
+package middlewares
+
+import (
+	"net/http"
+
+	"github.com/labstack/echo/v5"
+)
+
+
+func TenantContext() echo.MiddlewareFunc {
+	return func(next echo.HandlerFunc) echo.HandlerFunc {
+		return func(c *echo.Context) error {
+			tenantID := (*c).Request().Header.Get("X-Tenant-ID")
+			if tenantID == "" {
+				return (*c).JSON(http.StatusBadRequest, map[string]string{
+					"error": "X-Tenant-ID header is required",
+				})
+			}
+			(*c).Set("tenant_id", tenantID)
+
+			return next(c)
+		}
+	}
+}
diff --git a/repository_after/pkg/middlewares/tenant_validation.go b/repository_after/pkg/middlewares/tenant_validation.go
new file mode 100644
index 00000000..4e281733
--- /dev/null
+++ b/repository_after/pkg/middlewares/tenant_validation.go
@@ -0,0 +1,245 @@
+package middlewares
+
+import (
+	"log/slog"
+	"net/http"
+	"regexp"
+	"sync"
+	"time"
+
+	"github.com/labstack/echo/v5"
+)
+
+
+type TenantCache struct {
+	mu            sync.RWMutex
+	tenants       map[string]*TenantInfo
+	lastRefresh   time.Time
+	refreshPeriod time.Duration
+	logger        *slog.Logger
+}
+
+
+type TenantInfo struct {
+	ID        string
+	Name      string
+	Active    bool
+	RateLimit int
+	CachedAt  time.Time
+}
+
+
+func NewTenantCache(logger *slog.Logger) *TenantCache {
+	tc := &TenantCache{
+		tenants:       make(map[string]*TenantInfo),
+		refreshPeriod: 5 * time.Minute,
+		logger:        logger,
+	}
+	tc.loadMockTenants()
+	go tc.backgroundRefresh()
+
+	return tc
+}
+
+func (tc *TenantCache) loadMockTenants() {
+	tc.mu.Lock()
+	defer tc.mu.Unlock()
+
+	now := time.Now()
+	mockTenants := []TenantInfo{
+		{ID: "tenant-001", Name: "Acme Corp", Active: true, RateLimit: 10000},
+		{ID: "tenant-002", Name: "Beta Inc", Active: true, RateLimit: 5000},
+		{ID: "tenant-003", Name: "Gamma LLC", Active: true, RateLimit: 8000},
+		{ID: "tenant-004", Name: "Delta Co", Active: false, RateLimit: 3000},
+		{ID: "tenant-005", Name: "Epsilon Ltd", Active: true, RateLimit: 15000},
+	}
+
+	for i := 6; i <= 55; i++ {
+		mockTenants = append(mockTenants, TenantInfo{
+			ID:        generateTenantID(i),
+			Name:      generateTenantName(i),
+			Active:    i%10 != 0,
+			RateLimit: 5000 + (i * 100),
+		})
+	}
+
+	tc.tenants = make(map[string]*TenantInfo, len(mockTenants))
+	for _, t := range mockTenants {
+		tenant := t
+		tenant.CachedAt = now
+		tc.tenants[t.ID] = &tenant
+	}
+
+	tc.lastRefresh = now
+	tc.logger.Info("tenant cache refreshed",
+		"tenant_count", len(tc.tenants),
+		"refresh_time", now.Format(time.RFC3339),
+	)
+}
+
+func generateTenantID(n int) string {
+	return "tenant-" + padNumber(n)
+}
+
+func generateTenantName(n int) string {
+	return "Company " + padNumber(n)
+}
+
+func padNumber(n int) string {
+	if n < 10 {
+		return "00" + itoa(n)
+	} else if n < 100 {
+		return "0" + itoa(n)
+	}
+	return itoa(n)
+}
+
+func itoa(n int) string {
+	if n == 0 {
+		return "0"
+	}
+	var digits []byte
+	for n > 0 {
+		digits = append([]byte{byte('0' + n%10)}, digits...)
+		n /= 10
+	}
+	return string(digits)
+}
+
+func (tc *TenantCache) backgroundRefresh() {
+	ticker := time.NewTicker(tc.refreshPeriod)
+	defer ticker.Stop()
+
+	for range ticker.C {
+		tc.loadMockTenants()
+	}
+}
+
+func (tc *TenantCache) Validate(tenantID string) (bool, *TenantInfo, error) {
+	tc.mu.RLock()
+	defer tc.mu.RUnlock()
+
+	tenant, exists := tc.tenants[tenantID]
+	if !exists {
+		return false, nil, nil
+	}
+
+	if !tenant.Active {
+		return false, tenant, nil
+	}
+
+	return true, tenant, nil
+}
+
+func (tc *TenantCache) GetAllTenants() []*TenantInfo {
+	tc.mu.RLock()
+	defer tc.mu.RUnlock()
+
+	result := make([]*TenantInfo, 0, len(tc.tenants))
+	for _, t := range tc.tenants {
+		result = append(result, t)
+	}
+	return result
+}
+
+
+func (tc *TenantCache) TenantCount() int {
+	tc.mu.RLock()
+	defer tc.mu.RUnlock()
+	return len(tc.tenants)
+}
+
+
+func (tc *TenantCache) LastRefreshTime() time.Time {
+	tc.mu.RLock()
+	defer tc.mu.RUnlock()
+	return tc.lastRefresh
+}
+
+var tenantIDRegex = regexp.MustCompile(`^tenant-\d{3,}$`)
+
+
+func TenantValidation(cache *TenantCache) echo.MiddlewareFunc {
+	return func(next echo.HandlerFunc) echo.HandlerFunc {
+		return func(c *echo.Context) error {
+			tenantID := (*c).Request().Header.Get("X-Customer-ID")
+
+
+			if tenantID == "" {
+				tenantID = (*c).Request().Header.Get("X-Tenant-ID")
+			}
+
+			if tenantID == "" {
+				return (*c).JSON(http.StatusBadRequest, map[string]interface{}{
+					"error":   "missing_customer_id",
+					"message": "X-Customer-ID header is required",
+				})
+			}
+
+			if !tenantIDRegex.MatchString(tenantID) {
+				cache.logger.Warn("malformed tenant ID received",
+					"tenant_id", tenantID,
+					"remote_ip", (*c).RealIP(),
+				)
+				return (*c).JSON(http.StatusBadRequest, map[string]interface{}{
+					"error":   "invalid_customer_id_format",
+					"message": "Customer ID must match format: tenant-XXX",
+				})
+			}
+			valid, tenant, err := cache.Validate(tenantID)
+			if err != nil {
+				cache.logger.Error("tenant validation error",
+					"tenant_id", tenantID,
+					"error", err,
+				)
+				return (*c).JSON(http.StatusInternalServerError, map[string]interface{}{
+					"error":   "validation_error",
+					"message": "Unable to validate customer ID",
+				})
+			}
+
+			if tenant == nil {
+				cache.logger.Warn("unknown tenant ID",
+					"tenant_id", tenantID,
+					"remote_ip", (*c).RealIP(),
+				)
+				return (*c).JSON(http.StatusUnauthorized, map[string]interface{}{
+					"error":   "unknown_customer",
+					"message": "Customer ID not found",
+				})
+			}
+
+			if !valid {
+				cache.logger.Warn("inactive tenant attempted access",
+					"tenant_id", tenantID,
+					"tenant_name", tenant.Name,
+					"remote_ip", (*c).RealIP(),
+				)
+				return (*c).JSON(http.StatusUnauthorized, map[string]interface{}{
+					"error":   "inactive_customer",
+					"message": "Customer account is inactive",
+				})
+			}
+
+			(*c).Set("tenant_id", tenantID)
+			(*c).Set("tenant_info", tenant)
+
+			return next(c)
+		}
+	}
+}
+
+func GetTenantID(c *echo.Context) string {
+	if tenantID, ok := (*c).Get("tenant_id").(string); ok {
+		return tenantID
+	}
+	return ""
+}
+
+// GetTenantInfo retrieves tenant info from Echo context
+func GetTenantInfo(c *echo.Context) *TenantInfo {
+	if info, ok := (*c).Get("tenant_info").(*TenantInfo); ok {
+		return info
+	}
+	return nil
+}
diff --git a/repository_after/pkg/routes/routes.go b/repository_after/pkg/routes/routes.go
new file mode 100644
index 00000000..0eb7fba6
--- /dev/null
+++ b/repository_after/pkg/routes/routes.go
@@ -0,0 +1,40 @@
+package routes
+
+import (
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/handlers"
+	pkgmw "github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/middlewares"
+	"github.com/labstack/echo/v5"
+)
+
+
+func Setup(e *echo.Echo, h *handlers.Handler, tenantCache *pkgmw.TenantCache) {
+
+	e.GET("/health", func(c *echo.Context) error { return h.HealthCheck(c) })
+	e.GET("/ready", func(c *echo.Context) error { return h.ReadinessCheck(c) })
+
+	v1 := e.Group("/api/v1")
+
+
+	analytics := v1.Group("/analytics")
+	analytics.Use(pkgmw.TenantValidation(tenantCache))
+	{
+		analytics.POST("/events", func(c *echo.Context) error { return h.IngestEvents(c) })
+		analytics.GET("/stats", func(c *echo.Context) error { return h.GetStats(c) })
+		analytics.GET("/traffic", func(c *echo.Context) error { return h.GetTrafficData(c) })
+	}
+
+
+	logs := v1.Group("/logs")
+	logs.Use(pkgmw.TenantValidation(tenantCache))
+	logs.POST("", func(c *echo.Context) error { return h.IngestLogs(c) })
+
+
+	v1.GET("/metrics/:customer_id", func(c *echo.Context) error { return h.GetCustomerMetrics(c) })
+
+
+	admin := v1.Group("/admin")
+	{
+		admin.GET("/tenants", func(c *echo.Context) error { return h.ListTenants(c) })
+		admin.GET("/metrics", func(c *echo.Context) error { return h.GetMetrics(c) })
+	}
+}
diff --git a/repository_after/pkg/service/service.go b/repository_after/pkg/service/service.go
new file mode 100644
index 00000000..82d7a248
--- /dev/null
+++ b/repository_after/pkg/service/service.go
@@ -0,0 +1,456 @@
+package service
+
+import (
+	"context"
+	"errors"
+	"fmt"
+	"log/slog"
+	"sync"
+	"sync/atomic"
+	"time"
+
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/aggregator"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/clickhouse"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/config"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/geoip"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/tuning"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/types"
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/workers"
+)
+
+
+type Service struct {
+	cfg    *config.Config
+	logger *slog.Logger
+
+	workerPool  *workers.WorkerPool
+	geoEnricher *geoip.Enricher
+
+
+	aggregator *aggregator.SlidingWindowAggregator
+
+
+	batchInserter *clickhouse.BatchInserter
+
+
+	queryService clickhouse.QueryService
+
+
+	rateMonitor *tuning.RateMonitor
+
+
+	eventsProcessed atomic.Int64
+	eventsQueued    atomic.Int64
+
+	buffer          chan *types.LogEntry
+	bufferSize      int
+	bufferThreshold float64
+
+
+	mu      sync.RWMutex
+	tenants map[string]*TenantStats
+
+
+	eventCounter atomic.Int64
+}
+
+const (
+
+	DefaultBufferSize     = 10000 
+	BackpressureThreshold = 0.90  
+	RetryAfterSeconds     = 5    
+)
+
+// TenantStats holds per-tenant statistics
+type TenantStats struct {
+	TenantID        string    `json:"tenant_id"`
+	TotalEvents     int64     `json:"total_events"`
+	TotalBytes      int64     `json:"total_bytes"`
+	LastEventTime   time.Time `json:"last_event_time"`
+	UniqueCountries int       `json:"unique_countries"`
+}
+
+
+type Option func(*serviceOptions)
+
+type serviceOptions struct {
+	connector clickhouse.Connector
+}
+
+
+func WithConnector(c clickhouse.Connector) Option {
+	return func(o *serviceOptions) { o.connector = c }
+}
+
+
+func New(cfg *config.Config, logger *slog.Logger, opts ...Option) (*Service, error) {
+	var so serviceOptions
+	for _, o := range opts {
+		o(&so)
+	}
+
+
+	geoConfig := geoip.DefaultConfig()
+
+	var enricher workers.GeoIPEnricher
+	var geoEnricher *geoip.Enricher
+
+	if cfg.Environment == "development" || cfg.Environment == "test" {
+		logger.Info("Using mock GeoIP enricher for development")
+		enricher = geoip.NewMockEnricher(50 * time.Microsecond)
+	} else {
+		var err error
+		geoEnricher, err = geoip.NewEnricher(geoConfig, logger)
+		if err != nil {
+			logger.Warn("Failed to initialize GeoIP enricher, using mock", "error", err)
+			enricher = geoip.NewMockEnricher(50 * time.Microsecond)
+		} else {
+			enricher = geoEnricher
+		}
+	}
+
+	workerConfig := workers.DefaultConfig()
+	workerConfig.WorkerCount = cfg.WorkerCount
+	workerPool := workers.NewWorkerPool(workerConfig, logger, enricher)
+
+	agg := aggregator.NewSlidingWindowAggregator(logger)
+
+	batchCfg := clickhouse.DefaultBatchConfig()
+	batchCfg.BatchSize = cfg.BatchSize
+	batchCfg.FlushInterval = time.Duration(cfg.FlushInterval) * time.Second
+
+	var conn clickhouse.Connector
+	if so.connector != nil {
+		conn = so.connector
+		logger.Info("Using real ClickHouse connector for batch inserter")
+	} else {
+		conn = clickhouse.NewMockConnector()
+		logger.Info("Using mock ClickHouse connector (no DB)")
+	}
+	batchIns := clickhouse.NewBatchInserter(batchCfg, conn, logger)
+
+	queryService := clickhouse.NewClickHouseQueryService(conn)
+
+	rtCfg := tuning.DefaultRuntimeConfig()
+	if cfg.MaxProcs > 0 {
+		rtCfg.MaxProcs = cfg.MaxProcs
+	}
+	if cfg.TargetRatePerSec > 0 {
+		rtCfg.TargetRatePerSec = cfg.TargetRatePerSec
+	}
+	tuning.ApplyGOMAXPROCS(rtCfg, logger)
+	rateMonitor := tuning.NewRateMonitor(rtCfg, logger)
+
+	s := &Service{
+		cfg:             cfg,
+		logger:          logger,
+		workerPool:      workerPool,
+		geoEnricher:     geoEnricher,
+		aggregator:      agg,
+		batchInserter:   batchIns,
+		queryService:    queryService,
+		rateMonitor:     rateMonitor,
+		tenants:         make(map[string]*TenantStats),
+		buffer:          make(chan *types.LogEntry, DefaultBufferSize),
+		bufferSize:      DefaultBufferSize,
+		bufferThreshold: BackpressureThreshold,
+	}
+
+	workerPool.OnResult(func(entry *types.LogEntry, tenantID string) {
+		s.eventsProcessed.Add(1)
+
+		now := time.Now()
+		agg.Record(tenantID, now, entry.StatusCode, entry.BytesSent)
+
+		eventID := fmt.Sprintf("%s-%d", tenantID, s.eventCounter.Add(1))
+		if err := batchIns.AppendEntry(entry, eventID); err != nil {
+			logger.Warn("Failed to append to batch inserter",
+				"tenant_id", tenantID,
+				"error", err,
+			)
+		}
+	})
+
+	workerPool.Start()
+
+	logger.Info("ingestion service initialized",
+		"buffer_size", DefaultBufferSize,
+		"backpressure_threshold", BackpressureThreshold,
+		"worker_count", cfg.WorkerCount,
+		"worker_pool", "enabled",
+	)
+
+	return s, nil
+}
+
+func (s *Service) IsReady() bool {
+	return s.IsClickHouseReady() && s.IsGeoIPReady()
+}
+
+func (s *Service) IsClickHouseReady() bool {
+	if s.batchInserter != nil {
+		return s.batchInserter.IsHealthy(context.Background())
+	}
+	return true
+}
+
+func (s *Service) IsGeoIPReady() bool {
+	if s.geoEnricher != nil {
+		// Check if enricher has active readers
+		metrics := s.geoEnricher.GetMetrics()
+		if activeReaders, ok := metrics["active_readers"]; ok {
+			return activeReaders.(int) > 0
+		}
+	}
+	return true
+}
+
+func (s *Service) IngestEvents(ctx context.Context, tenantID string, events []map[string]interface{}) (int, error) {
+	count := len(events)
+	s.eventsQueued.Add(int64(count))
+
+	s.mu.Lock()
+	if _, exists := s.tenants[tenantID]; !exists {
+		s.tenants[tenantID] = &TenantStats{
+			TenantID: tenantID,
+		}
+	}
+	s.tenants[tenantID].TotalEvents += int64(count)
+	s.tenants[tenantID].LastEventTime = time.Now()
+	s.mu.Unlock()
+
+	now := time.Now().Unix()
+	for _, evt := range events {
+		entry := &types.LogEntry{
+			CustomerID: tenantID,
+			Timestamp:  now,
+		}
+
+		if ip, ok := evt["ip"].(string); ok {
+			entry.IP = ip
+		}
+		if sc, ok := evt["status_code"].(float64); ok {
+			entry.StatusCode = int(sc)
+		}
+		if bs, ok := evt["bytes_sent"].(float64); ok {
+			entry.BytesSent = int64(bs)
+		}
+
+		if err := s.workerPool.SubmitJob(entry, tenantID); err != nil {
+			s.logger.Warn("Failed to submit event to worker pool",
+				"tenant_id", tenantID,
+				"error", err,
+			)
+
+			s.eventsProcessed.Add(int64(count))
+			return count, nil
+		}
+	}
+
+	return count, nil
+}
+
+func (s *Service) IngestLogs(ctx context.Context, tenantID string, logs []types.LogEntry) error {
+
+	if s.IsBackpressureTriggered() {
+		return errors.New("backpressure_triggered")
+	}
+
+	count := len(logs)
+
+	if s.rateMonitor != nil && s.rateMonitor.RecordEvents(count) {
+		s.rateMonitor.RecordDropped(count)
+		return errors.New("rate_limited")
+	}
+
+	s.eventsQueued.Add(int64(count))
+
+	for _, log := range logs {
+		logCopy := log
+		logCopy.CustomerID = tenantID
+
+		if err := s.workerPool.SubmitJob(&logCopy, tenantID); err != nil {
+			s.logger.Error("Failed to submit job to worker pool",
+				"tenant_id", tenantID,
+				"error", err,
+			)
+			return err
+		}
+	}
+
+	s.updateTenantStats(tenantID, int64(count))
+
+	return nil
+}
+
+func (s *Service) IsBackpressureTriggered() bool {
+	return s.GetBufferUtilization() >= s.bufferThreshold
+}
+
+func (s *Service) GetBufferUtilization() float64 {
+	return s.workerPool.GetQueueUtilization()
+}
+
+func (s *Service) Shutdown(ctx context.Context) error {
+	s.logger.Info("Shutting down service")
+
+	if err := s.workerPool.Shutdown(30 * time.Second); err != nil {
+		s.logger.Error("Worker pool shutdown failed", "error", err)
+	}
+
+	if err := s.batchInserter.Shutdown(30 * time.Second); err != nil {
+		s.logger.Error("Batch inserter shutdown failed", "error", err)
+	}
+
+	if s.geoEnricher != nil {
+		if err := s.geoEnricher.Close(); err != nil {
+			s.logger.Error("GeoIP enricher close failed", "error", err)
+		}
+	}
+
+	s.logger.Info("Service shutdown completed")
+	return nil
+}
+
+func (s *Service) updateTenantStats(tenantID string, count int64) {
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	if _, exists := s.tenants[tenantID]; !exists {
+		s.tenants[tenantID] = &TenantStats{
+			TenantID: tenantID,
+		}
+	}
+	s.tenants[tenantID].TotalEvents += count
+	s.tenants[tenantID].LastEventTime = time.Now()
+}
+
+func (s *Service) GetStats(ctx context.Context, tenantID string) (*TenantStats, error) {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+
+	if stats, exists := s.tenants[tenantID]; exists {
+		return stats, nil
+	}
+
+	return &TenantStats{
+		TenantID: tenantID,
+	}, nil
+}
+
+func (s *Service) GetTrafficData(ctx context.Context, tenantID string) (map[string]interface{}, error) {
+	result := s.aggregator.Query(tenantID, 15)
+	result.Finalize()
+
+	return map[string]interface{}{
+		"tenant_id":           tenantID,
+		"window_minutes":      15,
+		"total_requests":      result.TotalRequests,
+		"total_bytes":         result.TotalBytes,
+		"requests_per_second": result.RequestsPerSecond,
+		"status_2xx":          result.Status2xx,
+		"status_3xx":          result.Status3xx,
+		"status_4xx":          result.Status4xx,
+		"status_5xx":          result.Status5xx,
+	}, nil
+}
+
+func (s *Service) ListTenants(ctx context.Context) ([]TenantStats, error) {
+	s.mu.RLock()
+	defer s.mu.RUnlock()
+
+	tenants := make([]TenantStats, 0, len(s.tenants))
+	for _, t := range s.tenants {
+		tenants = append(tenants, *t)
+	}
+
+	return tenants, nil
+}
+
+func (s *Service) QueryAggregator(customerID string, minutes int) aggregator.QueryResult {
+	return s.aggregator.Query(customerID, minutes)
+}
+
+func (s *Service) QueryAllAggregator(minutes int) map[string]aggregator.QueryResult {
+	return s.aggregator.QueryAll(minutes)
+}
+
+func (s *Service) QueryCustomerMetrics(ctx context.Context, customerID string, minutes int) (*clickhouse.CustomerMetricsSummary, error) {
+	start := time.Now()
+
+	aggResult := s.aggregator.Query(customerID, minutes)
+	aggResult.Finalize()
+
+	summary := &clickhouse.CustomerMetricsSummary{
+		CustomerID:        customerID,
+		WindowMinutes:     minutes,
+		TotalRequests:     aggResult.TotalRequests,
+		TotalBytes:        aggResult.TotalBytes,
+		RequestsPerSecond: aggResult.RequestsPerSecond,
+		StatusBreakdown: clickhouse.StatusBreakdown{
+			Status2xx: aggResult.Status2xx,
+			Status3xx: aggResult.Status3xx,
+			Status4xx: aggResult.Status4xx,
+			Status5xx: aggResult.Status5xx,
+		},
+	}
+
+	if summary.TotalRequests > 0 {
+		errorCount := summary.StatusBreakdown.Status4xx + summary.StatusBreakdown.Status5xx
+		summary.ErrorRate = float64(errorCount) / float64(summary.TotalRequests) * 100.0
+		summary.StatusBreakdown.ErrorPct = summary.ErrorRate
+	}
+
+	summary.QueryTimeMs = time.Since(start).Milliseconds()
+
+	return summary, nil
+}
+
+func (s *Service) GetRateMonitor() *tuning.RateMonitor {
+	return s.rateMonitor
+}
+
+func (s *Service) GetAggregator() *aggregator.SlidingWindowAggregator {
+	return s.aggregator
+}
+
+func (s *Service) GetBatchInserter() *clickhouse.BatchInserter {
+	return s.batchInserter
+}
+
+func (s *Service) GetMetrics() map[string]interface{} {
+	metrics := map[string]interface{}{
+		"events_processed":    s.eventsProcessed.Load(),
+		"events_queued":       s.eventsQueued.Load(),
+		"tenants_count":       len(s.tenants),
+		"worker_count":        s.cfg.WorkerCount,
+		"batch_size":          s.cfg.BatchSize,
+		"backpressure_active": s.IsBackpressureTriggered(),
+	}
+	workerMetrics := s.workerPool.GetMetrics()
+	for k, v := range workerMetrics {
+		metrics["worker_pool_"+k] = v
+	}
+	aggMetrics := s.aggregator.GetMetrics()
+	for k, v := range aggMetrics {
+		metrics["aggregator_"+k] = v
+	}
+	batchMetrics := s.batchInserter.GetMetrics()
+	for k, v := range batchMetrics {
+		metrics["batch_"+k] = v
+	}
+	if s.geoEnricher != nil {
+		geoMetrics := s.geoEnricher.GetMetrics()
+		for k, v := range geoMetrics {
+			metrics["geoip_"+k] = v
+		}
+	}
+	if s.rateMonitor != nil {
+		rmMetrics := s.rateMonitor.GetMetrics()
+		for k, v := range rmMetrics {
+			metrics["rate_"+k] = v
+		}
+	}
+
+	return metrics
+}
diff --git a/repository_after/pkg/tuning/runtime.go b/repository_after/pkg/tuning/runtime.go
new file mode 100644
index 00000000..63424eac
--- /dev/null
+++ b/repository_after/pkg/tuning/runtime.go
@@ -0,0 +1,143 @@
+package tuning
+
+import (
+	"log/slog"
+	"runtime"
+	"sync/atomic"
+	"time"
+)
+
+
+type RuntimeConfig struct {
+
+	MaxProcs int
+	TargetRatePerSec int           
+	ThrottleWindow   time.Duration 
+	ThrottleBurstMs int
+}
+
+
+func DefaultRuntimeConfig() *RuntimeConfig {
+	return &RuntimeConfig{
+		MaxProcs:         2,
+		TargetRatePerSec: 8_000,
+		ThrottleWindow:   1 * time.Second,
+		ThrottleBurstMs:  500,
+	}
+}
+
+
+func ApplyGOMAXPROCS(cfg *RuntimeConfig, logger *slog.Logger) int {
+	prev := runtime.GOMAXPROCS(cfg.MaxProcs)
+	logger.Info("GOMAXPROCS configured",
+		"previous", prev,
+		"current", cfg.MaxProcs,
+		"num_cpu", runtime.NumCPU(),
+	)
+	return prev
+}
+
+
+type RateMonitor struct {
+	cfg *RuntimeConfig
+
+
+	currentCount  atomic.Int64 
+	windowStart   atomic.Int64 
+	totalIngested atomic.Int64 
+	throttled     atomic.Bool  
+
+	
+	throttleActivations atomic.Int64
+	droppedEvents       atomic.Int64
+
+	logger *slog.Logger
+}
+
+
+func NewRateMonitor(cfg *RuntimeConfig, logger *slog.Logger) *RateMonitor {
+	rm := &RateMonitor{
+		cfg:    cfg,
+		logger: logger,
+	}
+	rm.windowStart.Store(time.Now().UnixNano())
+	return rm
+}
+
+
+func (rm *RateMonitor) RecordEvents(n int) bool {
+	rm.totalIngested.Add(int64(n))
+	newCount := rm.currentCount.Add(int64(n))
+
+
+	ws := rm.windowStart.Load()
+	elapsed := time.Duration(time.Now().UnixNano() - ws)
+
+	if elapsed >= rm.cfg.ThrottleWindow {
+	
+		now := time.Now().UnixNano()
+		if rm.windowStart.CompareAndSwap(ws, now) {
+			rm.currentCount.Store(int64(n))
+			newCount = int64(n)
+		}
+	}
+	windowFraction := float64(elapsed) / float64(rm.cfg.ThrottleWindow)
+	if windowFraction <= 0 {
+		windowFraction = 0.001
+	}
+	allowedInWindow := float64(rm.cfg.TargetRatePerSec) * windowFraction
+
+	shouldThrottle := float64(newCount) > allowedInWindow*1.1 
+	wasThrottled := rm.throttled.Load()
+	if shouldThrottle && !wasThrottled {
+		rm.throttled.Store(true)
+		rm.throttleActivations.Add(1)
+		rm.logger.Warn("Dynamic throttle activated",
+			"current_count", newCount,
+			"allowed", int64(allowedInWindow),
+			"window_fraction", windowFraction,
+		)
+	} else if !shouldThrottle && wasThrottled {
+		rm.throttled.Store(false)
+		rm.logger.Info("Dynamic throttle deactivated")
+	}
+	return rm.throttled.Load()
+}
+func (rm *RateMonitor) IsThrottled() bool {
+	return rm.throttled.Load()
+}
+
+func (rm *RateMonitor) RecordDropped(n int) {
+	rm.droppedEvents.Add(int64(n))
+}
+
+func (rm *RateMonitor) GetMetrics() map[string]interface{} {
+	ws := rm.windowStart.Load()
+	elapsed := time.Duration(time.Now().UnixNano() - ws)
+	currentCount := rm.currentCount.Load()
+
+	var currentRate float64
+	if elapsed > 0 {
+		currentRate = float64(currentCount) / elapsed.Seconds()
+	}
+
+	return map[string]interface{}{
+		"target_rate_per_sec":  rm.cfg.TargetRatePerSec,
+		"current_window_count": currentCount,
+		"current_rate_per_sec": currentRate,
+		"total_ingested":       rm.totalIngested.Load(),
+		"throttled":            rm.throttled.Load(),
+		"throttle_activations": rm.throttleActivations.Load(),
+		"dropped_events":       rm.droppedEvents.Load(),
+		"gomaxprocs":           runtime.GOMAXPROCS(0),
+		"num_goroutine":        runtime.NumGoroutine(),
+	}
+}
+func (rm *RateMonitor) Reset() {
+	rm.currentCount.Store(0)
+	rm.windowStart.Store(time.Now().UnixNano())
+	rm.totalIngested.Store(0)
+	rm.throttled.Store(false)
+	rm.throttleActivations.Store(0)
+	rm.droppedEvents.Store(0)
+}
diff --git a/repository_after/pkg/types/types.go b/repository_after/pkg/types/types.go
new file mode 100644
index 00000000..55439454
--- /dev/null
+++ b/repository_after/pkg/types/types.go
@@ -0,0 +1,74 @@
+package types
+
+
+type LogBatchRequest struct {
+	Logs []LogEntry `json:"logs" validate:"required,dive"`
+}
+
+
+type LogEntry struct {
+	Timestamp  int64       `json:"timestamp" validate:"required"`
+	CustomerID string      `json:"customer_id" validate:"required"`
+	StatusCode int         `json:"status_code" validate:"required,min=100,max=599"`
+	BytesSent  int64       `json:"bytes_sent" validate:"required,min=0"`
+	IP         string      `json:"ip" validate:"required,ip"`
+	GeoIP      interface{} `json:"geoip,omitempty"` 
+}
+
+type BackpressureResponse struct {
+	Error       string  `json:"error"`
+	RetryAfter  int     `json:"retry_after_seconds"`
+	Utilization float64 `json:"buffer_utilization"`
+	Message     string  `json:"message"`
+}
+
+
+type ErrorResponse struct {
+	Error   string `json:"error"`
+	Code    string `json:"code,omitempty"`
+	Message string `json:"message,omitempty"`
+}
+
+type SuccessResponse struct {
+	Success bool        `json:"success"`
+	Data    interface{} `json:"data,omitempty"`
+	Message string      `json:"message,omitempty"`
+}
+
+
+type PaginatedResponse struct {
+	Data       interface{} `json:"data"`
+	Page       int         `json:"page"`
+	PageSize   int         `json:"page_size"`
+	TotalCount int64       `json:"total_count"`
+	TotalPages int         `json:"total_pages"`
+}
+
+
+type EventsRequest struct {
+	Events []EventPayload `json:"events"`
+}
+
+
+type EventPayload struct {
+	Timestamp     int64   `json:"timestamp"`
+	ClientIP      string  `json:"client_ip"`
+	EdgeLocation  string  `json:"edge_location"`
+	RequestPath   string  `json:"request_path"`
+	HTTPMethod    string  `json:"http_method"`
+	StatusCode    int     `json:"status_code"`
+	BytesSent     int64   `json:"bytes_sent"`
+	BytesReceived int64   `json:"bytes_received"`
+	ResponseTime  float64 `json:"response_time_ms"`
+	UserAgent     string  `json:"user_agent"`
+	Referer       string  `json:"referer"`
+	CacheStatus   string  `json:"cache_status"`
+}
+
+
+type StatsQuery struct {
+	StartTime   string `query:"start_time"`
+	EndTime     string `query:"end_time"`
+	Granularity string `query:"granularity"` 
+	GroupBy     string `query:"group_by"` 
+}
diff --git a/repository_after/pkg/workers/worker_pool.go b/repository_after/pkg/workers/worker_pool.go
new file mode 100644
index 00000000..f1b77d16
--- /dev/null
+++ b/repository_after/pkg/workers/worker_pool.go
@@ -0,0 +1,336 @@
+package workers
+
+import (
+	"context"
+	"errors"
+	"log/slog"
+	"sync"
+	"sync/atomic"
+	"time"
+
+	"github.com/ep-eaglepoint-ai/bd_datasets_003/d1n2oj-real-time-multi-tenant-cdn-analytics-engine/repository_after/pkg/types"
+)
+
+// Job represents a log processing job
+type Job struct {
+	LogEntry  *types.LogEntry
+	TenantID  string
+	Timestamp time.Time
+}
+
+// Result represents the result of processing a job
+type Result struct {
+	Job       *Job
+	Error     error
+	Processed bool
+	Duration  time.Duration
+}
+
+// ResultHandler is called for every successfully processed job.
+// Implementations must be safe for concurrent invocation.
+type ResultHandler func(entry *types.LogEntry, tenantID string)
+
+// WorkerPool manages a pool of workers for processing log entries
+type WorkerPool struct {
+	workerCount int
+	jobQueue    chan *Job
+	resultQueue chan *Result
+	workers     []*Worker
+
+	// Graceful shutdown
+	ctx     context.Context
+	cancel  context.CancelFunc
+	wg      sync.WaitGroup
+	stopped atomic.Bool
+
+	// Metrics
+	jobsQueued     int64
+	jobsProcessed  int64
+	jobsInProgress int64
+
+	// Dependencies
+	logger         *slog.Logger
+	enricher       GeoIPEnricher
+	resultHandlers []ResultHandler
+
+	mu sync.RWMutex
+}
+
+// Worker represents a single worker in the pool
+type Worker struct {
+	id          int
+	jobQueue    chan *Job
+	resultQueue chan *Result
+	enricher    GeoIPEnricher
+	logger      *slog.Logger
+}
+
+// GeoIPEnricher interface for geo-IP enrichment
+type GeoIPEnricher interface {
+	Enrich(logEntry *types.LogEntry) error
+	Close() error
+}
+
+// WorkerPoolConfig holds configuration for the worker pool
+type WorkerPoolConfig struct {
+	WorkerCount     int
+	JobQueueSize    int
+	ResultQueueSize int
+}
+
+// DefaultConfig returns default configuration
+func DefaultConfig() *WorkerPoolConfig {
+	return &WorkerPoolConfig{
+		WorkerCount:     4,
+		JobQueueSize:    10000,
+		ResultQueueSize: 1000,
+	}
+}
+
+// NewWorkerPool creates a new worker pool
+func NewWorkerPool(config *WorkerPoolConfig, logger *slog.Logger, enricher GeoIPEnricher) *WorkerPool {
+	ctx, cancel := context.WithCancel(context.Background())
+
+	pool := &WorkerPool{
+		workerCount: config.WorkerCount,
+		jobQueue:    make(chan *Job, config.JobQueueSize),
+		resultQueue: make(chan *Result, config.ResultQueueSize),
+		ctx:         ctx,
+		cancel:      cancel,
+		logger:      logger,
+		enricher:    enricher,
+	}
+
+	// Create workers
+	pool.workers = make([]*Worker, config.WorkerCount)
+	for i := 0; i < config.WorkerCount; i++ {
+		pool.workers[i] = &Worker{
+			id:          i,
+			jobQueue:    pool.jobQueue,
+			resultQueue: pool.resultQueue,
+			enricher:    enricher,
+			logger:      logger,
+		}
+	}
+
+	return pool
+}
+
+// OnResult registers a handler that is called for every successfully
+// enriched log entry. Must be called before Start.
+func (wp *WorkerPool) OnResult(h ResultHandler) {
+	wp.resultHandlers = append(wp.resultHandlers, h)
+}
+
+// Start starts all workers in the pool
+func (wp *WorkerPool) Start() {
+	wp.logger.Info("Starting worker pool",
+		"worker_count", wp.workerCount,
+		"job_queue_size", cap(wp.jobQueue),
+		"result_queue_size", cap(wp.resultQueue),
+	)
+
+	// Start workers
+	for _, worker := range wp.workers {
+		wp.wg.Add(1)
+		go func(w *Worker) {
+			defer wp.wg.Done()
+			w.run(wp.ctx)
+		}(worker)
+	}
+
+	// Start result processor
+	wp.wg.Add(1)
+	go func() {
+		defer wp.wg.Done()
+		wp.processResults()
+	}()
+}
+
+// SubmitJob submits a job to the worker pool
+func (wp *WorkerPool) SubmitJob(logEntry *types.LogEntry, tenantID string) error {
+	if wp.stopped.Load() {
+		return ErrPoolStopped
+	}
+
+	job := &Job{
+		LogEntry:  logEntry,
+		TenantID:  tenantID,
+		Timestamp: time.Now(),
+	}
+
+	select {
+	case wp.jobQueue <- job:
+		wp.mu.Lock()
+		wp.jobsQueued++
+		wp.mu.Unlock()
+		return nil
+	case <-wp.ctx.Done():
+		return wp.ctx.Err()
+	default:
+		return ErrQueueFull
+	}
+}
+
+// GetQueueUtilization returns the current queue utilization (0.0 to 1.0)
+func (wp *WorkerPool) GetQueueUtilization() float64 {
+	return float64(len(wp.jobQueue)) / float64(cap(wp.jobQueue))
+}
+
+// GetMetrics returns worker pool metrics
+func (wp *WorkerPool) GetMetrics() map[string]interface{} {
+	wp.mu.RLock()
+	defer wp.mu.RUnlock()
+
+	return map[string]interface{}{
+		"worker_count":      wp.workerCount,
+		"jobs_queued":       wp.jobsQueued,
+		"jobs_processed":    wp.jobsProcessed,
+		"jobs_in_progress":  wp.jobsInProgress,
+		"queue_utilization": wp.GetQueueUtilization(),
+		"queue_size":        len(wp.jobQueue),
+		"queue_capacity":    cap(wp.jobQueue),
+	}
+}
+
+// Shutdown gracefully shuts down the worker pool
+func (wp *WorkerPool) Shutdown(timeout time.Duration) error {
+	wp.logger.Info("Shutting down worker pool", "timeout", timeout)
+
+	// Stop accepting new jobs
+	wp.stopped.Store(true)
+	wp.cancel()
+
+	// Close job queue to signal workers
+	close(wp.jobQueue)
+
+	// Wait for workers to finish with timeout
+	done := make(chan struct{})
+	go func() {
+		wp.wg.Wait()
+		close(done)
+	}()
+
+	select {
+	case <-done:
+		wp.logger.Info("Worker pool shutdown completed")
+		return nil
+	case <-time.After(timeout):
+		wp.logger.Warn("Worker pool shutdown timed out")
+		return ErrShutdownTimeout
+	}
+}
+
+// run executes the worker main loop
+func (w *Worker) run(ctx context.Context) {
+	w.logger.Debug("Worker started", "worker_id", w.id)
+
+	for {
+		select {
+		case job, ok := <-w.jobQueue:
+			if !ok {
+				// Queue closed, worker should exit
+				w.logger.Debug("Worker shutting down", "worker_id", w.id)
+				return
+			}
+
+			result := w.processJob(job)
+
+			select {
+			case w.resultQueue <- result:
+				// Result queued successfully
+			case <-ctx.Done():
+				// Context cancelled, worker should exit
+				return
+			default:
+				// Result queue full, log warning but continue
+				w.logger.Warn("Result queue full, dropping result", "worker_id", w.id)
+			}
+
+		case <-ctx.Done():
+			w.logger.Debug("Worker context cancelled", "worker_id", w.id)
+			return
+		}
+	}
+}
+
+// processJob processes a single job
+func (w *Worker) processJob(job *Job) *Result {
+	start := time.Now()
+	result := &Result{
+		Job:       job,
+		Processed: false,
+	}
+
+	// Enrich log entry with geo-IP data
+	if err := w.enricher.Enrich(job.LogEntry); err != nil {
+		result.Error = err
+		w.logger.Error("Geo-IP enrichment failed",
+			"worker_id", w.id,
+			"tenant_id", job.TenantID,
+			"error", err,
+		)
+	} else {
+		result.Processed = true
+		w.logger.Debug("Job processed successfully",
+			"worker_id", w.id,
+			"tenant_id", job.TenantID,
+		)
+	}
+
+	result.Duration = time.Since(start)
+	return result
+}
+
+// processResults handles processed job results
+func (wp *WorkerPool) processResults() {
+	wp.logger.Debug("Result processor started")
+
+	for {
+		select {
+		case result, ok := <-wp.resultQueue:
+			if !ok {
+				wp.logger.Debug("Result processor shutting down")
+				return
+			}
+
+			wp.handleResult(result)
+
+		case <-wp.ctx.Done():
+			wp.logger.Debug("Result processor context cancelled")
+			return
+		}
+	}
+}
+
+// handleResult processes a single result
+func (wp *WorkerPool) handleResult(result *Result) {
+	wp.mu.Lock()
+	wp.jobsProcessed++
+	wp.mu.Unlock()
+
+	if result.Error != nil {
+		wp.logger.Error("Job processing failed",
+			"tenant_id", result.Job.TenantID,
+			"error", result.Error,
+			"duration", result.Duration,
+		)
+	} else {
+		wp.logger.Debug("Job completed successfully",
+			"tenant_id", result.Job.TenantID,
+			"duration", result.Duration,
+		)
+
+		// Forward to all registered result handlers (aggregator, batch inserter, etc.)
+		for _, h := range wp.resultHandlers {
+			h(result.Job.LogEntry, result.Job.TenantID)
+		}
+	}
+}
+
+// Error definitions
+var (
+	ErrQueueFull       = errors.New("job queue is full")
+	ErrPoolStopped     = errors.New("worker pool is stopped")
+	ErrShutdownTimeout = errors.New("shutdown timed out")
+)
diff --git a/repository_after/types/types.go b/repository_after/types/types.go
new file mode 100644
index 00000000..805d95cc
--- /dev/null
+++ b/repository_after/types/types.go
@@ -0,0 +1,78 @@
+package types
+
+// LogBatchRequest represents a batch of log entries for ingestion
+type LogBatchRequest struct {
+	Logs []LogEntry `json:"logs" validate:"required,dive"`
+}
+
+// LogEntry represents a single log entry in the batch
+type LogEntry struct {
+	Timestamp  int64       `json:"timestamp" validate:"required"`
+	CustomerID string      `json:"customer_id" validate:"required"`
+	StatusCode int         `json:"status_code" validate:"required,min=100,max=599"`
+	BytesSent  int64       `json:"bytes_sent" validate:"required,min=0"`
+	IP         string      `json:"ip" validate:"required,ip"`
+	GeoIP      interface{} `json:"geoip,omitempty"` // Populated by enrichment layer
+}
+
+// BackpressureResponse represents response when backpressure is triggered
+type BackpressureResponse struct {
+	Error       string  `json:"error"`
+	RetryAfter  int     `json:"retry_after_seconds"`
+	Utilization float64 `json:"buffer_utilization"`
+	Message     string  `json:"message"`
+}
+
+// Response types for API consistency
+
+// ErrorResponse represents an error response
+type ErrorResponse struct {
+	Error   string `json:"error"`
+	Code    string `json:"code,omitempty"`
+	Message string `json:"message,omitempty"`
+}
+
+// SuccessResponse represents a success response
+type SuccessResponse struct {
+	Success bool        `json:"success"`
+	Data    interface{} `json:"data,omitempty"`
+	Message string      `json:"message,omitempty"`
+}
+
+// PaginatedResponse represents a paginated response
+type PaginatedResponse struct {
+	Data       interface{} `json:"data"`
+	Page       int         `json:"page"`
+	PageSize   int         `json:"page_size"`
+	TotalCount int64       `json:"total_count"`
+	TotalPages int         `json:"total_pages"`
+}
+
+// EventsRequest represents a batch events ingestion request
+type EventsRequest struct {
+	Events []EventPayload `json:"events"`
+}
+
+// EventPayload represents a single event in the request
+type EventPayload struct {
+	Timestamp     int64   `json:"timestamp"`
+	ClientIP      string  `json:"client_ip"`
+	EdgeLocation  string  `json:"edge_location"`
+	RequestPath   string  `json:"request_path"`
+	HTTPMethod    string  `json:"http_method"`
+	StatusCode    int     `json:"status_code"`
+	BytesSent     int64   `json:"bytes_sent"`
+	BytesReceived int64   `json:"bytes_received"`
+	ResponseTime  float64 `json:"response_time_ms"`
+	UserAgent     string  `json:"user_agent"`
+	Referer       string  `json:"referer"`
+	CacheStatus   string  `json:"cache_status"`
+}
+
+// StatsQuery represents query parameters for stats endpoint
+type StatsQuery struct {
+	StartTime   string `query:"start_time"`
+	EndTime     string `query:"end_time"`
+	Granularity string `query:"granularity"` // minute, hour, day
+	GroupBy     string `query:"group_by"`    // country, edge_location, etc.
+}
