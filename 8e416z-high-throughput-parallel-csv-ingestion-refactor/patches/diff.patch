diff --git a/repository_after/backpressure_queue.py b/repository_after/backpressure_queue.py
new file mode 100644
index 0000000..c6f455c
--- /dev/null
+++ b/repository_after/backpressure_queue.py
@@ -0,0 +1,114 @@
+import queue
+import threading
+import time
+
+class BackpressureQueue:
+    """
+    Bounded queue with backpressure control.
+    
+    When the queue is full, producers block until space is available,
+    preventing unbounded memory growth.
+    """
+    
+    def __init__(self, maxsize=1000):
+        """
+        Initialize the bounded queue.
+        
+        Args:
+            maxsize: Maximum number of items in the queue (default: 1000)
+        """
+        self.queue = queue.Queue(maxsize=maxsize)
+        self.maxsize = maxsize
+        self._max_observed_size = 0
+        self._size_lock = threading.Lock()
+    
+    def put(self, item, block=True, timeout=None):
+        """
+        Put an item into the queue.
+        
+        If the queue is full, this will block until space is available
+        (implementing backpressure).
+        
+        Args:
+            item: Item to put in the queue
+            block: Whether to block if queue is full (default: True)
+            timeout: Maximum time to wait in seconds (default: None = wait forever)
+        
+        Raises:
+            queue.Full: If block=False and queue is full
+        """
+        self.queue.put(item, block=block, timeout=timeout)
+        self._update_max_size()
+    
+    def get(self, block=True, timeout=None):
+        """
+        Get an item from the queue.
+        
+        Args:
+            block: Whether to block if queue is empty (default: True)
+            timeout: Maximum time to wait in seconds (default: None = wait forever)
+        
+        Returns:
+            Item from the queue
+        
+        Raises:
+            queue.Empty: If block=False and queue is empty
+        """
+        return self.queue.get(block=block, timeout=timeout)
+    
+    def qsize(self):
+        """
+        Get the approximate size of the queue.
+        
+        Returns:
+            Current queue size
+        """
+        return self.queue.qsize()
+    
+    def empty(self):
+        """
+        Check if the queue is empty.
+        
+        Returns:
+            True if empty, False otherwise
+        """
+        return self.queue.empty()
+    
+    def full(self):
+        """
+        Check if the queue is full.
+        
+        Returns:
+            True if full, False otherwise
+        """
+        return self.queue.full()
+    
+    def get_max_observed_size(self):
+        """
+        Get the maximum size the queue reached during its lifetime.
+        
+        Returns:
+            Maximum observed queue size
+        """
+        with self._size_lock:
+            return self._max_observed_size
+    
+    def _update_max_size(self):
+        """Update the maximum observed size."""
+        with self._size_lock:
+            current_size = self.queue.qsize()
+            if current_size > self._max_observed_size:
+                self._max_observed_size = current_size
+    
+    def task_done(self):
+        """
+        Indicate that a formerly enqueued task is complete.
+        Used by queue consumers.
+        """
+        self.queue.task_done()
+    
+    def join(self):
+        """
+        Block until all items in the queue have been processed.
+        """
+        self.queue.join()
diff --git a/repository_after/bloom_filter.py b/repository_after/bloom_filter.py
new file mode 100644
index 0000000..9dc63f2
--- /dev/null
+++ b/repository_after/bloom_filter.py
@@ -0,0 +1,39 @@
+import threading
+from pybloom_live import BloomFilter
+
+class ThreadSafeBloomFilter:
+    """
+    Thread-safe Bloom Filter for detecting duplicate transaction IDs.
+    
+    Uses an approximate membership structure to identify duplicates without
+    storing all IDs in memory. Safe for 100 million+ records.
+    """
+    
+    def __init__(self, capacity=100_000_000, error_rate=0.001):
+        """
+        Initialize the Bloom Filter.
+        
+        Args:
+            capacity: Expected number of elements (default: 100 million)
+            error_rate: False positive probability (default: 0.1%)
+        """
+        self.bloom_filter = BloomFilter(capacity=capacity, error_rate=error_rate)
+        self.lock = threading.Lock()
+    
+    def is_duplicate(self, transaction_id):
+        """
+        Check if a transaction ID is likely a duplicate.
+        
+        Args:
+            transaction_id: The transaction ID to check
+            
+        Returns:
+            True if the ID is likely a duplicate (already seen)
+            False if the ID is new (and records it)
+        """
+        with self.lock:
+            if transaction_id in self.bloom_filter:
+                return True
+            else:
+                self.bloom_filter.add(transaction_id)
+                return False
diff --git a/repository_after/db_writer.py b/repository_after/db_writer.py
new file mode 100644
index 0000000..bb2241b
--- /dev/null
+++ b/repository_after/db_writer.py
@@ -0,0 +1,135 @@
+import sqlite3
+import threading
+
+class BatchDatabaseWriter:
+    """
+    Batched database writer with atomic commits.
+    
+    Writes records in fixed-size batches to reduce I/O overhead
+    and database lock contention.
+    """
+    
+    def __init__(self, db_path, batch_size=1000):
+        """
+        Initialize the batch writer.
+        
+        Args:
+            db_path: Path to SQLite database
+            batch_size: Number of records per batch (default: 1000)
+        """
+        self.db_path = db_path
+        self.batch_size = batch_size
+        self.batch = []
+        self.lock = threading.Lock()
+        self.total_written = 0
+        self.commit_count = 0
+        self._initialize_db()
+    
+    def _initialize_db(self):
+        """Initialize the database with required schema."""
+        conn = sqlite3.connect(self.db_path)
+        cursor = conn.cursor()
+        cursor.execute('''
+            CREATE TABLE IF NOT EXISTS transactions (
+                transaction_id TEXT PRIMARY KEY,
+                amount REAL,
+                date TEXT
+            )
+        ''')
+        conn.commit()
+        conn.close()
+    
+    def write_record(self, record):
+        """
+        Add a record to the batch.
+        
+        If batch is full, commits automatically.
+        
+        Args:
+            record: Dictionary with keys: transaction_id, amount, date
+        """
+        with self.lock:
+            self.batch.append(record)
+            if len(self.batch) >= self.batch_size:
+                self._commit_batch()
+    
+    def _commit_batch(self):
+        """
+        Commit the current batch atomically.
+        
+        This is called automatically when batch is full,
+        or manually via flush().
+        """
+        if not self.batch:
+            return
+        
+        conn = sqlite3.connect(self.db_path)
+        cursor = conn.cursor()
+        
+        try:
+            # Begin transaction
+            cursor.execute('BEGIN TRANSACTION')
+            
+            # Insert all records in batch
+            for record in self.batch:
+                cursor.execute(
+                    'INSERT OR REPLACE INTO transactions (transaction_id, amount, date) VALUES (?, ?, ?)',
+                    (record['transaction_id'], record['amount'], record['date'])
+                )
+            
+            # Commit atomically
+            conn.commit()
+            self.commit_count += 1
+            self.total_written += len(self.batch)
+            self.batch = []
+            
+        except Exception as e:
+            # Rollback on error
+            conn.rollback()
+            raise e
+        finally:
+            conn.close()
+    
+    def flush(self):
+        """
+        Flush any remaining records in the batch.
+        
+        Should be called at the end of processing.
+        """
+        with self.lock:
+            if self.batch:
+                self._commit_batch()
+    
+    def get_commit_count(self):
+        """
+        Get the number of commits performed.
+        
+        Returns:
+            Total number of batch commits
+        """
+        with self.lock:
+            return self.commit_count
+    
+    def get_total_written(self):
+        """
+        Get the total number of records written.
+        
+        Returns:
+            Total records written to database
+        """
+        with self.lock:
+            return self.total_written
+    
+    def count_records(self):
+        """
+        Count total records in the database.
+        
+        Returns:
+            Number of records in transactions table
+        """
+        conn = sqlite3.connect(self.db_path)
+        cursor = conn.cursor()
+        cursor.execute('SELECT COUNT(*) FROM transactions')
+        count = cursor.fetchone()[0]
+        conn.close()
+        return count
diff --git a/repository_after/dlq.py b/repository_after/dlq.py
new file mode 100644
index 0000000..7a6536d
--- /dev/null
+++ b/repository_after/dlq.py
@@ -0,0 +1,88 @@
+import csv
+import threading
+import os
+
+class DeadLetterQueue:
+    """
+    Thread-safe Dead Letter Queue for persisting malformed rows.
+    
+    Records raw row data, line numbers, and exception messages,
+    allowing the pipeline to continue processing valid rows.
+    """
+    
+    def __init__(self, dlq_file_path='errors.csv'):
+        """
+        Initialize the DLQ.
+        
+        Args:
+            dlq_file_path: Path to the DLQ file (default: 'errors.csv')
+        """
+        self.dlq_file_path = dlq_file_path
+        self.lock = threading.Lock()
+        self.error_count = 0
+        self._initialize_file()
+    
+    def _initialize_file(self):
+        """Initialize the DLQ file with headers if it doesn't exist."""
+        with self.lock:
+            file_exists = os.path.isfile(self.dlq_file_path)
+            if not file_exists:
+                with open(self.dlq_file_path, 'w', newline='') as f:
+                    writer = csv.writer(f)
+                    writer.writerow(['line_number', 'raw_data', 'error_message'])
+    
+    def record_error(self, line_number, raw_row, exception):
+        """
+        Record a malformed row to the DLQ.
+        
+        Args:
+            line_number: Original line number from source file
+            raw_row: Raw row data (dict or string)
+            exception: Exception that was raised
+        """
+        with self.lock:
+            with open(self.dlq_file_path, 'a', newline='') as f:
+                writer = csv.writer(f)
+                writer.writerow([
+                    line_number,
+                    str(raw_row),
+                    str(exception)
+                ])
+            self.error_count += 1
+    
+    def get_error_count(self):
+        """
+        Get the total number of errors recorded.
+        
+        Returns:
+            Total error count
+        """
+        with self.lock:
+            return self.error_count
+    
+    def read_errors(self):
+        """
+        Read all errors from the DLQ file.
+        
+        Returns:
+            List of error dictionaries with keys: line_number, raw_data, error_message
+        """
+        errors = []
+        with self.lock:
+            if os.path.isfile(self.dlq_file_path):
+                with open(self.dlq_file_path, 'r', newline='') as f:
+                    reader = csv.DictReader(f)
+                    for row in reader:
+                        errors.append(row)
+        return errors
+    
+    def clear(self):
+        """Clear the DLQ file and reset error count."""
+        with self.lock:
+            if os.path.isfile(self.dlq_file_path):
+                os.remove(self.dlq_file_path)
+            self.error_count = 0
+            # Initialize file without re-acquiring lock
+            with open(self.dlq_file_path, 'w', newline='') as f:
+                writer = csv.writer(f)
+                writer.writerow(['line_number', 'raw_data', 'error_message'])
diff --git a/repository_before/ingestion_processor.py b/repository_after/ingestion_processor.py
index b9490c5..7fd8bc4 100644
--- a/repository_before/ingestion_processor.py
+++ b/repository_after/ingestion_processor.py
@@ -1,29 +1,20 @@
-# filename: ingestion_processor.py
-# Legacy ingestion logic using Pandas. 
-# Imports: pandas (data analysis), sqlite3 (database storage)
-
-import pandas as pd
-import sqlite3
-import logging
+from pipeline import IngestionPipeline
+import os
 
 def process_large_file(file_path, db_path):
     """
-    EXTREMELY MEMORY INTENSIVE: Loads the entire CSV into memory.
-    Lacks parallel processing and granular error handling.
+    Refactored entry point for high-throughput, parallel streaming CSV ingestion.
+    Matches the signature of the legacy processor.
     """
-    logging.info("Starting legacy ingestion...")
-    try:
-        # BUG: This will OOM on any file larger than available RAM
-        df = pd.read_csv(file_path)
-        
-        # Manual de-duplication is slow and memory intensive
-        df.drop_duplicates(subset=['transaction_id'], keep='first', inplace=True)
-        
-        conn = sqlite3.connect(db_path)
-        # BUG: Single-threaded insert is slow and lacks backpressure control
-        df.to_sql('transactions', conn, if_exists='append', index=False)
-        conn.close()
-        logging.info("Ingestion complete.")
-    except Exception as e:
-        # BUG: Swallows specific error contexts; fails entire file on one bad row
-        logging.error(f"Ingestion failed: {e}")
+    # Initialize and run the pipeline
+    # Using sensible defaults that meet all requirements
+    pipeline = IngestionPipeline(
+        csv_path=file_path,
+        db_path=db_path,
+        dlq_path='errors.csv',
+        max_workers=4,
+        queue_size=1000,
+        batch_size=1000
+    )
+    
+    return pipeline.run()
diff --git a/repository_after/pipeline.py b/repository_after/pipeline.py
new file mode 100644
index 0000000..c403543
--- /dev/null
+++ b/repository_after/pipeline.py
@@ -0,0 +1,195 @@
+import threading
+import signal
+from streaming_reader import stream_csv_reader
+from bloom_filter import ThreadSafeBloomFilter
+from worker_pool import WorkerPool
+from backpressure_queue import BackpressureQueue
+from dlq import DeadLetterQueue
+from db_writer import BatchDatabaseWriter
+
+class IngestionPipeline:
+    """
+    Integrated pipeline that orchestrates all components for end-to-end CSV ingestion.
+    
+    Components:
+    - Streaming Reader: Reads CSV incrementally
+    - Bloom Filter: De-duplicates transaction IDs
+    - Worker Pool: Parallel validation/transformation
+    - Backpressure Queue: Bounded queue with backpressure
+    - DLQ: Captures malformed rows
+    - Batch DB Writer: Atomic batch commits
+    """
+    
+    def __init__(self, csv_path, db_path, dlq_path='errors.csv', 
+                 max_workers=4, queue_size=1000, batch_size=1000):
+        """
+        Initialize the pipeline.
+        
+        Args:
+            csv_path: Path to input CSV file
+            db_path: Path to output SQLite database
+            dlq_path: Path to DLQ file (default: 'errors.csv')
+            max_workers: Number of worker threads (default: 4)
+            queue_size: Maximum queue size (default: 1000)
+            batch_size: Database batch size (default: 1000)
+        """
+        self.csv_path = csv_path
+        self.db_path = db_path
+        
+        # 100M capacity at 2% error rate takes ~100MB bit array.
+        # Tuned to stay strictly under 256MB RSS including Python overhead.
+        self.bloom_filter = ThreadSafeBloomFilter(capacity=100_000_000, error_rate=0.02)
+        self.worker_pool = WorkerPool(max_workers=max_workers)
+        # Reduced queue and batch size to minimize in-memory row overhead
+        self.queue = BackpressureQueue(maxsize=min(queue_size, 200))
+        self.dlq = DeadLetterQueue(dlq_file_path=dlq_path)
+        self.db_writer = BatchDatabaseWriter(db_path=db_path, batch_size=min(batch_size, 500))
+        
+        self.producer_thread = None
+        self.consumer_thread = None
+        self.stop_flag = threading.Event()
+        self.shutdown_requested = False
+        
+        # Register signal handlers for graceful shutdown
+        signal.signal(signal.SIGINT, self._signal_handler)
+        signal.signal(signal.SIGTERM, self._signal_handler)
+    
+    def _signal_handler(self, signum, frame):
+        """
+        Handle interrupt signals for graceful shutdown.
+        
+        Args:
+            signum: Signal number
+            frame: Current stack frame
+        """
+        print(f"\nReceived signal {signum}. Initiating graceful shutdown...")
+        self.shutdown_requested = True
+        self.stop()
+    
+    def _validate_and_transform(self, line_number, row):
+        """
+        Validate and transform a single row.
+        
+        Args:
+            line_number: Line number from source file
+            row: Dictionary representing CSV row
+            
+        Returns:
+            (line_number, transformed_row) or None if invalid
+        """
+        try:
+            # Validate required fields
+            if not row.get('transaction_id'):
+                raise ValueError("Missing transaction_id")
+            
+            # Check for duplicates using Bloom filter
+            if self.bloom_filter.is_duplicate(row['transaction_id']):
+                return None  # Skip duplicate
+            
+            # Transform data
+            transformed = {
+                'transaction_id': row['transaction_id'],
+                'amount': float(row.get('amount', 0)),
+                'date': row.get('date', '')
+            }
+            
+            return (line_number, transformed)
+            
+        except Exception as e:
+            # Record error to DLQ
+            self.dlq.record_error(line_number, row, e)
+            return None
+    
+    def _producer(self):
+        """
+        Producer thread: reads CSV and feeds queue.
+        """
+        try:
+            for line_number, row in stream_csv_reader(self.csv_path):
+                if self.stop_flag.is_set():
+                    break
+                self.queue.put((line_number, row))
+        finally:
+            # Signal end of input
+            self.queue.put(None)
+    
+    def _consumer(self):
+        """
+        Consumer thread: processes queue and writes to DB.
+        """
+        batch = []
+        
+        while True:
+            # Check if we should stop AND queue is empty
+            if self.stop_flag.is_set() and self.queue.empty():
+                break
+                
+            try:
+                # Use a timeout so we can periodically check stop_flag
+                item = self.queue.get(timeout=0.1)
+            except:
+                # Timeout reached, loop again to check stop_flag
+                continue
+            
+            if item is None:
+                # End of input signal
+                break
+            
+            batch.append(item)
+            
+            # Process batch when it reaches a reasonable size
+            if len(batch) >= 100:
+                self._process_batch(batch)
+                batch = []
+        
+        # Process remaining items
+        if batch:
+            self._process_batch(batch)
+        
+        # Flush DB writer
+        self.db_writer.flush()
+    
+    def _process_batch(self, batch):
+        """
+        Process a batch of rows using worker pool.
+        
+        Args:
+            batch: List of (line_number, row) tuples
+        """
+        # Process in parallel
+        results = self.worker_pool.process_rows(batch, self._validate_and_transform)
+        
+        # Write valid results to database
+        for line_number, transformed_row in results:
+            if transformed_row and 'error' not in transformed_row:
+                self.db_writer.write_record(transformed_row)
+    
+    def run(self):
+        """
+        Run the pipeline end-to-end.
+        
+        Returns:
+            Dictionary with statistics: rows_written, errors, commits
+        """
+        # Start producer and consumer threads
+        self.producer_thread = threading.Thread(target=self._producer)
+        self.consumer_thread = threading.Thread(target=self._consumer)
+        
+        self.producer_thread.start()
+        self.consumer_thread.start()
+        
+        # Wait for completion
+        self.producer_thread.join()
+        self.consumer_thread.join()
+        
+        # Return statistics
+        return {
+            'rows_written': self.db_writer.get_total_written(),
+            'errors': self.dlq.get_error_count(),
+            'commits': self.db_writer.get_commit_count(),
+            'records_in_db': self.db_writer.count_records()
+        }
+    
+    def stop(self):
+        """Stop the pipeline gracefully."""
+        self.stop_flag.set()
diff --git a/repository_after/streaming_reader.py b/repository_after/streaming_reader.py
new file mode 100644
index 0000000..cc5df5b
--- /dev/null
+++ b/repository_after/streaming_reader.py
@@ -0,0 +1,13 @@
+import csv
+
+def stream_csv_reader(file_path):
+    """
+    Streaming CSV reader that yields rows incrementally without loading the full file.
+    
+    Yields:
+        tuple: (line_number, row_dict) where line_number is 1-indexed from source file
+    """
+    with open(file_path, 'r', newline='') as csvfile:
+        reader = csv.DictReader(csvfile)
+        for line_number, row in enumerate(reader, start=2):
+            yield (line_number, row)
diff --git a/repository_after/worker_pool.py b/repository_after/worker_pool.py
new file mode 100644
index 0000000..988a19c
--- /dev/null
+++ b/repository_after/worker_pool.py
@@ -0,0 +1,92 @@
+from concurrent.futures import ThreadPoolExecutor, as_completed
+import threading
+
+class WorkerPool:
+    """
+    Parallel worker pool for processing CSV rows concurrently.
+    
+    Uses ThreadPoolExecutor to parallelize validation and transformation
+    across multiple worker threads.
+    """
+    
+    def __init__(self, max_workers=4):
+        """
+        Initialize the worker pool.
+        
+        Args:
+            max_workers: Maximum number of worker threads (default: 4)
+        """
+        self.max_workers = max_workers
+        self.executor = None
+    
+    def process_rows(self, rows, process_func):
+        """
+        Process rows in parallel using worker threads.
+        
+        Args:
+            rows: Iterable of (line_number, row_dict) tuples
+            process_func: Function to apply to each row, signature: func(line_number, row_dict)
+                         Should return (line_number, result) or (line_number, None) for errors
+        
+        Returns:
+            List of (line_number, result) tuples in deterministic order
+        """
+        results = []
+        
+        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
+            # Submit all tasks
+            future_to_row = {
+                executor.submit(process_func, line_num, row): line_num 
+                for line_num, row in rows
+            }
+            
+            # Collect results as they complete
+            for future in as_completed(future_to_row):
+                line_num = future_to_row[future]
+                try:
+                    result = future.result()
+                    if result is not None:
+                        results.append(result)
+                except Exception as e:
+                    # Store error information
+                    results.append((line_num, {'error': str(e)}))
+        
+        # Sort by line number to maintain deterministic order
+        results.sort(key=lambda x: x[0])
+        return results
+    
+    def get_active_worker_count(self):
+        """
+        Get the number of active worker threads.
+        
+        Returns:
+            Number of active threads
+        """
+        return threading.active_count()
+
+
+def validate_and_transform_row(line_number, row):
+    """
+    Example validation and transformation function.
+    
+    Args:
+        line_number: Line number from source file
+        row: Dictionary representing the CSV row
+        
+    Returns:
+        (line_number, transformed_row) or None if validation fails
+    """
+    # Basic validation
+    if not row.get('transaction_id'):
+        return None
+    
+    # Simple transformation (example: convert amount to float)
+    try:
+        transformed = {
+            'transaction_id': row['transaction_id'],
+            'amount': float(row.get('amount', 0)),
+            'date': row.get('date', '')
+        }
+        return (line_number, transformed)
+    except (ValueError, TypeError):
+        return None
