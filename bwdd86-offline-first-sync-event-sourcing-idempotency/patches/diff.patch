diff --git a/repository_before/.gitkeep b/repository_before/.gitkeep
deleted file mode 100644
index e69de29..0000000
diff --git a/repository_after/client.go b/repository_after/client.go
new file mode 100644
index 0000000..236968e
--- /dev/null
+++ b/repository_after/client.go
@@ -0,0 +1,296 @@
+// client.go
+//
+// Disconnected Client Simulator (offline-first)
+//
+// Client model:
+// - Maintains a local inventory mirror.
+// - Applies operations optimistically while offline.
+// - Buffers intent events (not final state) in a local queue.
+// - When online, flushes the queued events as a single batch to the server.
+//
+// Critical behavior demonstrated:
+// - Idempotent replay: if ACK is lost, client retries SAME BatchID + SAME events.
+// - Atomicity handling: if server rejects (stock < 0), client must REBASE:
+//   fetch true server state and clear its pending queue.
+
+package offline_sync
+
+import (
+	"bytes"
+	"crypto/rand"
+	"encoding/hex"
+	"encoding/json"
+	"fmt"
+	"io"
+	"net/http"
+	"time"
+)
+
+type Client struct {
+	ID        string
+	ServerURL string
+
+	// Offline-first local state.
+	LocalInventory map[string]int
+	PendingEvents  []Event // local operation queue
+
+	// Inflight batch:
+	// When we start a flush, we freeze the batch ID and events.
+	// If ACK is lost, we re-send the exact same batch again.
+	Inflight *SyncRequest
+
+	Online bool
+	HTTP   *http.Client
+}
+
+func NewClient(id, serverURL string) *Client {
+	return &Client{
+		ID:             id,
+		ServerURL:      serverURL,
+		LocalInventory: map[string]int{},
+		PendingEvents:  nil,
+		Inflight:       nil,
+		Online:         true,
+		HTTP: &http.Client{
+			Timeout: 5 * time.Second,
+		},
+	}
+}
+
+func (c *Client) SetOnline(online bool) {
+	c.Online = online
+}
+
+func (c *Client) RecordIncrement(sku string, qty int) {
+	c.recordEvent(Increment, sku, qty)
+}
+
+func (c *Client) RecordDecrement(sku string, qty int) {
+	c.recordEvent(Decrement, sku, qty)
+}
+
+func (c *Client) recordEvent(op OpType, sku string, qty int) {
+	ev := Event{
+		EventID: newID("evt"),
+		SKU:     sku,
+		Type:    op,
+		Qty:     qty,
+		At:      time.Now(),
+	}
+
+	// 1) Optimistic local apply (offline-first).
+	applyLocal(c.LocalInventory, ev)
+
+	// 2) Buffer the *intent event* (event sourcing).
+	c.PendingEvents = append(c.PendingEvents, ev)
+
+	fmt.Printf("[CLIENT] queued %-9s sku=%-8s qty=%d event_id=%s (local now=%d)\n",
+		ev.Type, ev.SKU, ev.Qty, ev.EventID, c.LocalInventory[ev.SKU])
+}
+
+func applyLocal(inv map[string]int, ev Event) {
+	switch ev.Type {
+	case Increment:
+		inv[ev.SKU] = inv[ev.SKU] + ev.Qty
+	case Decrement:
+		inv[ev.SKU] = inv[ev.SKU] - ev.Qty // optimistic: may go negative locally
+	}
+}
+
+func (c *Client) RebaseFromServer() error {
+	state, err := c.FetchServerState()
+	if err != nil {
+		return err
+	}
+	c.LocalInventory = copyInventory(state.Inventory)
+
+	// When rebasing, we must clear our pending queue and inflight batch,
+	// because our local intent history is no longer safely replayable.
+	c.PendingEvents = nil
+	c.Inflight = nil
+
+	fmt.Printf("[CLIENT] REBASE complete. ServerVersion=%d LocalInventory=%v\n",
+		state.ServerVersion, c.LocalInventory)
+	return nil
+}
+
+func (c *Client) FetchServerState() (*StateResponse, error) {
+	url := c.ServerURL + "/state"
+	resp, err := c.HTTP.Get(url)
+	if err != nil {
+		return nil, fmt.Errorf("GET /state failed: %w", err)
+	}
+	defer resp.Body.Close()
+
+	var out StateResponse
+	if err := json.NewDecoder(resp.Body).Decode(&out); err != nil {
+		return nil, fmt.Errorf("decode /state: %w", err)
+	}
+	return &out, nil
+}
+
+// FlushPending sends the client's buffered intent events to the server.
+// If ACK is lost, the client can call FlushPending again; it will resend the same Inflight batch.
+func (c *Client) FlushPending() error {
+	if !c.Online {
+		fmt.Println("[CLIENT] offline -> not flushing")
+		return nil
+	}
+
+	// If we don't have an inflight batch, create one from the current queue.
+	if c.Inflight == nil {
+		if len(c.PendingEvents) == 0 {
+			fmt.Println("[CLIENT] nothing to flush")
+			return nil
+		}
+		c.Inflight = &SyncRequest{
+			ClientID: c.ID,
+			BatchID:  newID("batch"),
+			Events:   append([]Event(nil), c.PendingEvents...), // freeze snapshot
+		}
+		fmt.Printf("[CLIENT] created inflight batch=%s events=%d\n",
+			c.Inflight.BatchID, len(c.Inflight.Events))
+	}
+
+	// Send inflight to server.
+	res, status, err := c.postSync(*c.Inflight)
+	if err != nil {
+		fmt.Printf("[CLIENT] POST /sync failed (will retry later). err=%v\n", err)
+		return err
+	}
+
+	if res.Accepted {
+		fmt.Printf("[CLIENT] SYNC ACCEPTED batch=%s serverVersion=%d dup=%v\n",
+			res.BatchID, res.ServerVersion, res.ProcessedAsDup)
+
+		// Reconcile: set local inventory to server truth.
+		c.LocalInventory = copyInventory(res.Inventory)
+
+		// Clear queue + inflight (we're now consistent).
+		c.PendingEvents = nil
+		c.Inflight = nil
+		return nil
+	}
+
+	// Rejected -> must rebase (per prompt).
+	fmt.Printf("[CLIENT] SYNC REJECTED status=%d batch=%s reason=%s\n", status, res.BatchID, res.Reason)
+	if res.RebaseRequired {
+		fmt.Println("[CLIENT] server requires REBASE: fetching /state and clearing local queue")
+		return c.RebaseFromServer()
+	}
+	return fmt.Errorf("sync rejected without rebase_required (unexpected): %s", res.Reason)
+}
+
+// FlushPendingSimulateAckLoss sends the batch but intentionally "loses" the acknowledgement,
+// meaning we do NOT clear the inflight batch. Then the next FlushPending() will re-send the same batch.
+//
+// This simulates spotty 4G where the server processed the request, but the client never saw the response.
+func (c *Client) FlushPendingSimulateAckLoss() error {
+	if !c.Online {
+		fmt.Println("[CLIENT] offline -> not flushing")
+		return nil
+	}
+
+	// Ensure inflight exists.
+	if c.Inflight == nil {
+		if len(c.PendingEvents) == 0 {
+			fmt.Println("[CLIENT] nothing to flush")
+			return nil
+		}
+		c.Inflight = &SyncRequest{
+			ClientID: c.ID,
+			BatchID:  newID("batch"),
+			Events:   append([]Event(nil), c.PendingEvents...),
+		}
+		fmt.Printf("[CLIENT] created inflight batch=%s events=%d\n",
+			c.Inflight.BatchID, len(c.Inflight.Events))
+	}
+
+	// Send request, but "lose" the response (ignore it).
+	_, _, err := c.postSyncIgnoreResponse(*c.Inflight)
+	if err != nil {
+		fmt.Printf("[CLIENT] simulated send failed: %v\n", err)
+		return err
+	}
+
+	fmt.Printf("[CLIENT] ACK LOST (simulated). Will retry SAME batch=%s later.\n", c.Inflight.BatchID)
+	// IMPORTANT: We intentionally do NOT clear PendingEvents or Inflight here.
+	return fmt.Errorf("simulated ack loss")
+}
+
+func (c *Client) postSync(req SyncRequest) (*SyncResponse, int, error) {
+	url := c.ServerURL + "/sync"
+
+	body, err := json.Marshal(req)
+	if err != nil {
+		return nil, 0, fmt.Errorf("marshal sync request: %w", err)
+	}
+
+	httpReq, err := http.NewRequest("POST", url, bytes.NewReader(body))
+	if err != nil {
+		return nil, 0, fmt.Errorf("new request: %w", err)
+	}
+	httpReq.Header.Set("Content-Type", "application/json")
+
+	resp, err := c.HTTP.Do(httpReq)
+	if err != nil {
+		return nil, 0, fmt.Errorf("do request: %w", err)
+	}
+	defer resp.Body.Close()
+
+	var out SyncResponse
+	if err := json.NewDecoder(resp.Body).Decode(&out); err != nil {
+		return nil, resp.StatusCode, fmt.Errorf("decode sync response: %w", err)
+	}
+
+	return &out, resp.StatusCode, nil
+}
+
+func (c *Client) postSyncIgnoreResponse(req SyncRequest) (*http.Response, int, error) {
+	url := c.ServerURL + "/sync"
+
+	body, err := json.Marshal(req)
+	if err != nil {
+		return nil, 0, fmt.Errorf("marshal sync request: %w", err)
+	}
+
+	httpReq, err := http.NewRequest("POST", url, bytes.NewReader(body))
+	if err != nil {
+		return nil, 0, fmt.Errorf("new request: %w", err)
+	}
+	httpReq.Header.Set("Content-Type", "application/json")
+
+	resp, err := c.HTTP.Do(httpReq)
+	if err != nil {
+		return nil, 0, fmt.Errorf("do request: %w", err)
+	}
+
+	// "Lose" the ACK: we discard the body and do not parse it.
+	_, _ = io.Copy(io.Discard, resp.Body)
+	_ = resp.Body.Close()
+
+	return resp, resp.StatusCode, nil
+}
+
+func newID(prefix string) string {
+	// Standard library only "UUID-ish" random ID.
+	// 16 random bytes -> 32 hex chars.
+	var b [16]byte
+	_, _ = rand.Read(b[:])
+	return fmt.Sprintf("%s_%s", prefix, hex.EncodeToString(b[:]))
+}
+
+func (c *Client) PrintLocalState() {
+	fmt.Printf("[CLIENT] Online=%v Pending=%d Inflight=%v LocalInventory=%v\n",
+		c.Online,
+		len(c.PendingEvents),
+		func() string {
+			if c.Inflight == nil {
+				return "<nil>"
+			}
+			return fmt.Sprintf("{batch=%s events=%d}", c.Inflight.BatchID, len(c.Inflight.Events))
+		}(),
+		c.LocalInventory,
+	)
+}
+
diff --git a/repository_after/cmd/client/main.go b/repository_after/cmd/client/main.go
new file mode 100644
index 0000000..b11f1bc
--- /dev/null
+++ b/repository_after/cmd/client/main.go
@@ -0,0 +1,55 @@
+package main
+
+import (
+	"fmt"
+
+	offline_sync "offline_sync"
+)
+
+func main() {
+	serverURL := "http://localhost:8080"
+	client := offline_sync.NewClient("agent-eyob-1", serverURL)
+
+	fmt.Println("=== 1) Initial REBASE from server ===")
+	if err := client.RebaseFromServer(); err != nil {
+		panic(err)
+	}
+
+	fmt.Println("\n=== 2) OFFLINE: agent performs work (optimistic local apply + queue events) ===")
+	client.SetOnline(false)
+	client.RecordDecrement("bandage", 3)
+	client.RecordDecrement("syringe", 2)
+	client.RecordIncrement("bandage", 1)
+	client.PrintLocalState()
+
+	fmt.Println("\n=== 3) ONLINE: flush batch but simulate ACK loss ===")
+	client.SetOnline(true)
+	_ = client.FlushPendingSimulateAckLoss()
+	client.PrintLocalState()
+
+	fmt.Println("\n=== 4) ONLINE: retry SAME batch (idempotent replay) ===")
+	if err := client.FlushPending(); err != nil {
+		panic(err)
+	}
+	client.PrintLocalState()
+
+	fmt.Println("\n=== 5) OFFLINE: create a batch that will FAIL to prove atomicity (no partial apply) ===")
+	client.SetOnline(false)
+	client.RecordDecrement("bandage", 2)
+	client.RecordDecrement("syringe", 999)
+	client.PrintLocalState()
+
+	fmt.Println("\n=== 6) ONLINE: flush failing batch (server rejects -> client rebases + clears queue) ===")
+	client.SetOnline(true)
+	if err := client.FlushPending(); err != nil {
+		fmt.Println("[CLIENT] flush error:", err)
+	}
+	client.PrintLocalState()
+
+	fmt.Println("\n=== 7) Final Server Truth ===")
+	state, err := client.FetchServerState()
+	if err != nil {
+		panic(err)
+	}
+	fmt.Printf("[SERVER] version=%d inventory=%v\n", state.ServerVersion, state.Inventory)
+}
diff --git a/repository_after/cmd/server/main.go b/repository_after/cmd/server/main.go
new file mode 100644
index 0000000..5bfad4f
--- /dev/null
+++ b/repository_after/cmd/server/main.go
@@ -0,0 +1,19 @@
+package main
+
+import (
+	"log"
+	"net/http"
+
+	offline_sync "offline_sync"
+)
+
+func main() {
+	srv := offline_sync.NewInventoryServer()
+	addr := ":8080"
+	log.Printf("Inventory Sync Server listening on %s", addr)
+	log.Printf("Try: curl http://localhost:8080/state")
+
+	if err := http.ListenAndServe(addr, srv.Handler()); err != nil {
+		log.Fatal(err)
+	}
+}
diff --git a/repository_after/go.mod b/repository_after/go.mod
new file mode 100644
index 0000000..c01cacc
--- /dev/null
+++ b/repository_after/go.mod
@@ -0,0 +1,3 @@
+module offline_sync
+
+go 1.21
diff --git a/repository_after/server.go b/repository_after/server.go
new file mode 100644
index 0000000..1d80c4e
--- /dev/null
+++ b/repository_after/server.go
@@ -0,0 +1,290 @@
+// server.go
+//
+// Offline-First Sync Server (in-memory, standard library only)
+//
+// Key guarantees:
+// 1) Event-sourcing: client sends a list of intent events (INCREMENT/DECREMENT), not final values.
+// 2) Idempotency: re-sending the same BatchID has ZERO effect the second time.
+// 3) Atomicity: batch applies ALL events or NONE. If any event would cause stock < 0, reject batch.
+// 4) Thread-safe: sync.Mutex protects global inventory + dedupe indexes.
+// 5) Reconciliation: server returns the true current inventory state after every sync attempt.
+
+package offline_sync
+
+import (
+	"encoding/json"
+	"errors"
+	"fmt"
+	"log"
+	"net/http"
+	"sync"
+)
+
+type BatchOutcome struct {
+	Accepted bool   `json:"accepted"`
+	Reason   string `json:"reason,omitempty"`
+}
+
+// InventoryServer holds all server state in-memory.
+type InventoryServer struct {
+	mu sync.Mutex
+
+	// Global inventory. Values must never be negative.
+	inventory map[string]int
+
+	// Idempotency:
+	// - processedBatches ensures same BatchID is a no-op on retry.
+	// - processedEvents ensures each EventID is applied at most once across all time.
+	processedBatches map[string]BatchOutcome
+	processedEvents  map[string]struct{}
+
+	// A monotonically increasing version for reconciliation/debug.
+	serverVersion int64
+}
+
+func NewInventoryServer() *InventoryServer {
+	return &InventoryServer{
+		// Example initial inventory (edit as you like).
+		inventory: map[string]int{
+			"bandage": 10,
+			"syringe": 5,
+			"gloves":  20,
+		},
+		processedBatches: make(map[string]BatchOutcome),
+		processedEvents:  make(map[string]struct{}),
+		serverVersion:    1,
+	}
+}
+
+func (s *InventoryServer) Handler() http.Handler {
+	mux := http.NewServeMux()
+	// Use path-only patterns for Go 1.21 compatibility.
+	mux.HandleFunc("/state", s.handleGetState)
+	mux.HandleFunc("/sync", s.handlePostSync)
+	return mux
+}
+
+func (s *InventoryServer) handleGetState(w http.ResponseWriter, r *http.Request) {
+	if r.Method != http.MethodGet {
+		writeJSON(w, http.StatusMethodNotAllowed, StateResponse{
+			ServerVersion: 0,
+			Inventory:     map[string]int{},
+		})
+		return
+	}
+
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	writeJSON(w, http.StatusOK, StateResponse{
+		ServerVersion: s.serverVersion,
+		Inventory:     copyInventory(s.inventory),
+	})
+}
+
+func (s *InventoryServer) handlePostSync(w http.ResponseWriter, r *http.Request) {
+	if r.Method != http.MethodPost {
+		writeJSON(w, http.StatusMethodNotAllowed, SyncResponse{
+			Accepted:       false,
+			Reason:         "method not allowed",
+			RebaseRequired: false,
+			ServerVersion:  0,
+			Inventory:      nil,
+		})
+		return
+	}
+
+	var req SyncRequest
+	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
+		writeJSON(w, http.StatusBadRequest, SyncResponse{
+			BatchID:        req.BatchID,
+			Accepted:       false,
+			Reason:         "invalid JSON payload",
+			RebaseRequired: true,
+			ServerVersion:  0,
+			Inventory:      nil,
+		})
+		return
+	}
+
+	// Basic request validation outside mutex is fine, but the authoritative validation happens inside too.
+	if err := validateSyncRequest(req); err != nil {
+		writeJSON(w, http.StatusBadRequest, SyncResponse{
+			BatchID:        req.BatchID,
+			Accepted:       false,
+			Reason:         fmt.Sprintf("bad request: %v", err),
+			RebaseRequired: true,
+			ServerVersion:  0,
+			Inventory:      nil,
+		})
+		return
+	}
+
+	s.mu.Lock()
+	defer s.mu.Unlock()
+
+	// 1) Idempotency by BatchID:
+	// If we already processed this BatchID, return the current true state,
+	// and do NOT apply anything again.
+	if outcome, ok := s.processedBatches[req.BatchID]; ok {
+		status := http.StatusOK
+		if !outcome.Accepted {
+			status = http.StatusConflict
+		}
+		writeJSON(w, status, SyncResponse{
+			BatchID:         req.BatchID,
+			Accepted:        outcome.Accepted,
+			Reason:          outcome.Reason,
+			RebaseRequired:  !outcome.Accepted,
+			ServerVersion:   s.serverVersion,
+			Inventory:       copyInventory(s.inventory),
+			ProcessedAsDup:  true,
+			ProcessedDupWhy: "batch_id already processed; returning current state without re-applying",
+		})
+		return
+	}
+
+	// 2) Validate event IDs (no duplicates in-batch, and no replay of already processed events).
+	inBatchSeen := make(map[string]struct{}, len(req.Events))
+	for i, ev := range req.Events {
+		if _, dup := inBatchSeen[ev.EventID]; dup {
+			outcome := BatchOutcome{
+				Accepted: false,
+				Reason:   fmt.Sprintf("invalid batch: duplicate event_id within batch at index %d", i),
+			}
+			s.processedBatches[req.BatchID] = outcome
+			writeJSON(w, http.StatusBadRequest, SyncResponse{
+				BatchID:        req.BatchID,
+				Accepted:       false,
+				Reason:         outcome.Reason,
+				RebaseRequired: true,
+				ServerVersion:  s.serverVersion,
+				Inventory:      copyInventory(s.inventory),
+			})
+			return
+		}
+		inBatchSeen[ev.EventID] = struct{}{}
+
+		// Strict "exactly once per unique event":
+		// If any EventID was processed previously, reject batch and force rebase.
+		// (This keeps the model simple and avoids partial-apply confusion.)
+		if _, already := s.processedEvents[ev.EventID]; already {
+			outcome := BatchOutcome{
+				Accepted: false,
+				Reason:   fmt.Sprintf("event already processed: %s (client must rebase)", ev.EventID),
+			}
+			s.processedBatches[req.BatchID] = outcome
+			writeJSON(w, http.StatusConflict, SyncResponse{
+				BatchID:        req.BatchID,
+				Accepted:       false,
+				Reason:         outcome.Reason,
+				RebaseRequired: true,
+				ServerVersion:  s.serverVersion,
+				Inventory:      copyInventory(s.inventory),
+			})
+			return
+		}
+	}
+
+	// 3) Atomic apply using a "transaction copy".
+	// We simulate applying every event into a copy. If any step would go < 0, reject entire batch.
+	proposed := copyInventory(s.inventory)
+	for i, ev := range req.Events {
+		if err := applyEvent(proposed, ev); err != nil {
+			// Reject entire batch (atomicity).
+			outcome := BatchOutcome{
+				Accepted: false,
+				Reason:   fmt.Sprintf("batch rejected at event[%d] (%s): %v", i, ev.EventID, err),
+			}
+			s.processedBatches[req.BatchID] = outcome
+
+			// IMPORTANT: do NOT mark events as processed, and do NOT change inventory.
+			writeJSON(w, http.StatusConflict, SyncResponse{
+				BatchID:        req.BatchID,
+				Accepted:       false,
+				Reason:         outcome.Reason,
+				RebaseRequired: true,
+				ServerVersion:  s.serverVersion,
+				Inventory:      copyInventory(s.inventory),
+			})
+			return
+		}
+	}
+
+	// 4) Commit the transaction: single atomic swap under mutex.
+	s.inventory = proposed
+	s.serverVersion++
+
+	for _, ev := range req.Events {
+		s.processedEvents[ev.EventID] = struct{}{}
+	}
+	s.processedBatches[req.BatchID] = BatchOutcome{Accepted: true}
+
+	log.Printf("SYNC ACCEPTED client=%s batch=%s events=%d version=%d",
+		req.ClientID, req.BatchID, len(req.Events), s.serverVersion)
+
+	writeJSON(w, http.StatusOK, SyncResponse{
+		BatchID:        req.BatchID,
+		Accepted:       true,
+		Reason:         "",
+		RebaseRequired: false,
+		ServerVersion:  s.serverVersion,
+		Inventory:      copyInventory(s.inventory),
+	})
+}
+
+func validateSyncRequest(req SyncRequest) error {
+	if req.ClientID == "" {
+		return errors.New("client_id is required")
+	}
+	if req.BatchID == "" {
+		return errors.New("batch_id is required")
+	}
+	if len(req.Events) == 0 {
+		return errors.New("events must be non-empty")
+	}
+	for i, ev := range req.Events {
+		if ev.EventID == "" {
+			return fmt.Errorf("events[%d].event_id is required", i)
+		}
+		if ev.SKU == "" {
+			return fmt.Errorf("events[%d].sku is required", i)
+		}
+		if ev.Qty <= 0 {
+			return fmt.Errorf("events[%d].qty must be > 0", i)
+		}
+		if ev.Type != Increment && ev.Type != Decrement {
+			return fmt.Errorf("events[%d].type must be INCREMENT or DECREMENT", i)
+		}
+	}
+	return nil
+}
+
+func applyEvent(inv map[string]int, ev Event) error {
+	current := inv[ev.SKU] // missing SKU defaults to 0
+
+	switch ev.Type {
+	case Increment:
+		inv[ev.SKU] = current + ev.Qty
+		return nil
+	case Decrement:
+		next := current - ev.Qty
+		if next < 0 {
+			return fmt.Errorf("invalid state: %s would drop below zero (%d - %d = %d)", ev.SKU, current, ev.Qty, next)
+		}
+		inv[ev.SKU] = next
+		return nil
+	default:
+		return fmt.Errorf("unknown op type: %q", ev.Type)
+	}
+}
+
+func writeJSON(w http.ResponseWriter, status int, payload any) {
+	w.Header().Set("Content-Type", "application/json")
+	w.WriteHeader(status)
+
+	enc := json.NewEncoder(w)
+	enc.SetIndent("", "  ")
+	_ = enc.Encode(payload)
+}
+
diff --git a/repository_after/types.go b/repository_after/types.go
new file mode 100644
index 0000000..0b3dc8e
--- /dev/null
+++ b/repository_after/types.go
@@ -0,0 +1,48 @@
+package offline_sync
+
+import "time"
+
+type OpType string
+
+const (
+	Increment OpType = "INCREMENT"
+	Decrement OpType = "DECREMENT"
+)
+
+type Event struct {
+	EventID string    `json:"event_id"`
+	SKU     string    `json:"sku"`
+	Type    OpType    `json:"type"`
+	Qty     int       `json:"qty"`
+	At      time.Time `json:"at"`
+}
+
+type SyncRequest struct {
+	ClientID string  `json:"client_id"`
+	BatchID  string  `json:"batch_id"`
+	Events   []Event `json:"events"`
+}
+
+type SyncResponse struct {
+	BatchID         string         `json:"batch_id"`
+	Accepted        bool           `json:"accepted"`
+	Reason          string         `json:"reason,omitempty"`
+	RebaseRequired  bool           `json:"rebase_required"`
+	ServerVersion   int64          `json:"server_version"`
+	Inventory       map[string]int `json:"inventory"`
+	ProcessedAsDup  bool           `json:"processed_as_duplicate"`
+	ProcessedDupWhy string         `json:"processed_duplicate_reason,omitempty"`
+}
+
+type StateResponse struct {
+	ServerVersion int64          `json:"server_version"`
+	Inventory     map[string]int `json:"inventory"`
+}
+
+func copyInventory(src map[string]int) map[string]int {
+	dst := make(map[string]int, len(src))
+	for k, v := range src {
+		dst[k] = v
+	}
+	return dst
+}
