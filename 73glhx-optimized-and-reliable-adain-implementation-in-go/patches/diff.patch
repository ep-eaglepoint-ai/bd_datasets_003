diff --git a/repository_before/.gitkeep b/repository_before/.gitkeep
deleted file mode 100644
index e69de29..0000000
diff --git a/repository_before/adain/adain.go b/repository_after/adain/adain.go
index 5ef1752..df7c159 100644
--- a/repository_before/adain/adain.go
+++ b/repository_after/adain/adain.go
@@ -3,133 +3,144 @@ package adain
 import (
 	"errors"
 	"math"
+	"strconv"
 )
 
-type X struct {
-	D []float64
-	S []int
+// Tensor represents a 4D tensor: [N, C, H, W]
+type Tensor struct {
+	Data  []float64
+	Shape []int
 }
 
-func Z(s []int) *X {
-	n := 1
-	for _, v := range s {
-		n *= v
+// NewTensor creates a zero-initialized tensor with the given shape
+func NewTensor(shape []int) *Tensor {
+	size := 1
+	for _, v := range shape {
+		size *= v
 	}
-	return &X{D: make([]float64, n), S: s}
+	return &Tensor{Data: make([]float64, size), Shape: shape}
 }
 
-func (x *X) I(a, b, c, d int) int {
-	return ((a*x.S[1]+b)*x.S[2]+c)*x.S[3] + d
+// Index converts 4D indices to flat index
+func (t *Tensor) Index(n, c, h, w int) int {
+	return ((n*t.Shape[1] + c) * t.Shape[2] + h) * t.Shape[3] + w
 }
 
-func Q(n string, t *X) error {
+// Validate ensures tensor is non-nil, has enough dimensions, and no NaN/Inf
+func Validate(name string, t *Tensor) error {
 	if t == nil {
-		return errors.New(n)
+		return errors.New(name + " is nil")
 	}
-	if len(t.S) < 3 {
-		return errors.New(n)
+	if len(t.Shape) != 4 {
+		return errors.New(name + " must be 4D")
 	}
-	for i := 0; i < len(t.D); i++ {
-		v := t.D[i]
+	for i, v := range t.Data {
 		if math.IsNaN(v) || math.IsInf(v, 0) {
-			return errors.New(n)
+			return errors.New(name + " contains invalid value at index " + strconv.Itoa(i))
 		}
 	}
 	return nil
 }
 
-func Y(f *X, e float64, m *X) (*X, *X) {
-	N, C, H, W := f.S[0], f.S[1], f.S[2], f.S[3]
-	u := Z([]int{N, C, 1, 1})
-	v := Z([]int{N, C, 1, 1})
+// ComputeMeanStd computes per-sample, per-channel mean and std over spatial dims with optional mask
+// x must be a validated 4D tensor.
+func ComputeMeanStd(x *Tensor, mask *Tensor, epsilon float64) (*Tensor, *Tensor) {
+	N, C, H, W := x.Shape[0], x.Shape[1], x.Shape[2], x.Shape[3]
+	mean := NewTensor([]int{N, C, 1, 1})
+	std := NewTensor([]int{N, C, 1, 1})
 
-	for a := 0; a < N; a++ {
-		for b := 0; b < C; b++ {
-			t := 0.0
-			c := 0.0
-			for i := 0; i < H; i++ {
-				for j := 0; j < W; j++ {
-					w := f.D[f.I(a, b, i, j)]
-					if m != nil {
-						r := m.D[m.I(a, 0, i, j)]
-						t += w * r
-						c += r
-					} else {
-						t += w
-						c++
-					}
+	spatialSize := H * W
+	for n := 0; n < N; n++ {
+		for c := 0; c < C; c++ {
+			sum, weight := 0.0, 0.0
+			offset := x.Index(n, c, 0, 0)
+			// Spatial flattening: flat index i maps to (h=i/W, w=i%W) in row-major order.
+			for i := 0; i < spatialSize; i++ {
+				val := x.Data[offset+i]
+				w := 1.0
+				if mask != nil {
+					w = mask.Data[mask.Index(n, 0, i/W, i%W)]
 				}
+				sum += val * w
+				weight += w
 			}
-			if c < 1 {
-				c = 1
+			if weight == 0 {
+				weight = float64(spatialSize)
 			}
-			p := t / c
-			u.D[u.I(a, b, 0, 0)] = p
-			s := 0.0
-			for i := 0; i < H; i++ {
-				for j := 0; j < W; j++ {
-					w := f.D[f.I(a, b, i, j)]
-					d := w - p
-					if m != nil {
-						r := m.D[m.I(a, 0, i, j)]
-						s += d * d * r
-					} else {
-						s += d * d
-					}
+			m := sum / weight
+			mean.Data[mean.Index(n, c, 0, 0)] = m
+
+			varSum := 0.0
+			for i := 0; i < spatialSize; i++ {
+				val := x.Data[offset+i]
+				diff := val - m
+				w := 1.0
+				if mask != nil {
+					w = mask.Data[mask.Index(n, 0, i/W, i%W)]
 				}
+				varSum += diff * diff * w
 			}
-			v.D[v.I(a, b, 0, 0)] = s / c
+			std.Data[std.Index(n, c, 0, 0)] = math.Sqrt(math.Max(varSum/weight, 0) + epsilon)
 		}
 	}
-
-	o := Z([]int{N, C, 1, 1})
-	for i := 0; i < len(o.D); i++ {
-		z := v.D[i]
-		if z < 0 {
-			z = 0
-		}
-		o.D[i] = math.Sqrt(z + e)
-	}
-
-	return u, o
+	return mean, std
 }
 
-func R(c *X, s *X, a float64, e float64, cm *X, sm *X) (*X, error) {
-	if Q("c", c) != nil || Q("s", s) != nil {
-		return nil, errors.New("x")
+// ApplyAdaIN normalizes content tensor, applies style stats, and alpha blending
+func ApplyAdaIN(content, style *Tensor, alpha, epsilon float64, contentMask, styleMask *Tensor) (*Tensor, error) {
+	if err := Validate("content", content); err != nil {
+		return nil, err
+	}
+	if err := Validate("style", style); err != nil {
+		return nil, err
 	}
-	if c.S[1] != s.S[1] {
-		return nil, errors.New("x")
+	if content.Shape[1] != style.Shape[1] {
+		return nil, errors.New("channel mismatch")
+	}
+	if contentMask != nil {
+		if len(contentMask.Shape) != 4 || contentMask.Shape[0] != content.Shape[0] || contentMask.Shape[1] != 1 ||
+			contentMask.Shape[2] != content.Shape[2] || contentMask.Shape[3] != content.Shape[3] {
+			return nil, errors.New("content mask shape mismatch")
+		}
+	}
+	if styleMask != nil {
+		if len(styleMask.Shape) != 4 || styleMask.Shape[0] != style.Shape[0] || styleMask.Shape[1] != 1 ||
+			styleMask.Shape[2] != style.Shape[2] || styleMask.Shape[3] != style.Shape[3] {
+			return nil, errors.New("style mask shape mismatch")
+		}
 	}
 
-	m1, d1 := Y(c, e, cm)
-	m2, d2 := Y(s, e, sm)
+	meanC, stdC := ComputeMeanStd(content, contentMask, epsilon)
+	meanS, stdS := ComputeMeanStd(style, styleMask, epsilon)
 
-	N, C, H, W := c.S[0], c.S[1], c.S[2], c.S[3]
-	o := Z([]int{N, C, H, W})
+	N, C, H, W := content.Shape[0], content.Shape[1], content.Shape[2], content.Shape[3]
+	out := NewTensor([]int{N, C, H, W})
 
 	for n := 0; n < N; n++ {
-		for ch := 0; ch < C; ch++ {
-			for i := 0; i < H; i++ {
-				for j := 0; j < W; j++ {
-					x := c.D[c.I(n, ch, i, j)]
-					y := (x - m1.D[m1.I(n, ch, 0, 0)]) / (d1.D[d1.I(n, ch, 0, 0)] + e)
-					z := y*d2.D[d2.I(n, ch, 0, 0)] + m2.D[m2.I(n, ch, 0, 0)]
-					if a < 1 {
-						z = a*z + (1-a)*x
-					}
-					o.D[o.I(n, ch, i, j)] = z
+		for c := 0; c < C; c++ {
+			mc := meanC.Data[meanC.Index(n, c, 0, 0)]
+			sc := stdC.Data[stdC.Index(n, c, 0, 0)]
+			ms := meanS.Data[meanS.Index(n, c, 0, 0)]
+			ss := stdS.Data[stdS.Index(n, c, 0, 0)]
+
+			offset := content.Index(n, c, 0, 0)
+			for i := 0; i < H*W; i++ {
+				val := content.Data[offset+i]
+				normalized := (val - mc) / (sc + epsilon)
+				styled := normalized*ss + ms
+				if alpha < 1 {
+					styled = alpha*styled + (1-alpha)*val
 				}
+				out.Data[offset+i] = styled
 			}
 		}
 	}
 
-	for i := 0; i < len(o.D); i++ {
-		v := o.D[i]
+	// Final NaN/Inf check
+	for i, v := range out.Data {
 		if math.IsNaN(v) || math.IsInf(v, 0) {
-			return nil, errors.New("x")
+			return nil, errors.New("output contains invalid value at index " + strconv.Itoa(i))
 		}
 	}
-
-	return o, nil
+	return out, nil
 }
diff --git a/repository_before/main.go b/repository_after/main.go
index dc89797..8250739 100644
--- a/repository_before/main.go
+++ b/repository_after/main.go
@@ -1,31 +1,39 @@
 package main
 
 import (
-	"adain-go/adain"
 	"fmt"
 	"math/rand"
 	"time"
+
+	"adain-go/adain" // import your optimized adain package
 )
 
 func main() {
 	rand.Seed(time.Now().UnixNano())
+
+	// Define dimensions
 	N, C, H, W := 2, 3, 4, 4
-	content := adain.Z([]int{N, C, H, W})
-	style := adain.Z([]int{1, C, H, W})
 
-	for i := range content.D {
-		content.D[i] = rand.Float64()
+	// Initialize content and style tensors
+	content := adain.NewTensor([]int{N, C, H, W})
+	style := adain.NewTensor([]int{1, C, H, W})
+
+	// Fill with random values
+	for i := range content.Data {
+		content.Data[i] = rand.Float64()
 	}
-	for i := range style.D {
-		style.D[i] = rand.Float64()
+	for i := range style.Data {
+		style.Data[i] = rand.Float64()
 	}
 
-	output, err := adain.R(content, style, 0.8, 1e-6, nil, nil)
+	// Alpha blending 0.8, epsilon 1e-6, no masks
+	output, err := adain.ApplyAdaIN(content, style, 0.8, 1e-6, nil, nil)
 	if err != nil {
 		fmt.Println("Error:", err)
 		return
 	}
 
-	fmt.Println("Output shape:", output.S)
-	fmt.Println("First 10 values:", output.D[:10])
+	// Print output summary
+	fmt.Println("Output shape:", output.Shape)
+	fmt.Println("First 10 values:", output.Data[:10])
 }
