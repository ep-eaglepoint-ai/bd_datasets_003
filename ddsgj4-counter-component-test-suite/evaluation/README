# Evaluation Script

This directory contains the evaluation logic for comparing test and meta-test results.

## Files

- `evaluation.js` - Main evaluation script (JavaScript/Node.js)
- `reports/` - Output directory for evaluation reports

## Running the Evaluation

```bash
# From the project root directory
node evaluation/evaluation.js

# Or using docker compose
docker compose run evaluation
```

## Report Output

The evaluation generates a `latest.json` report in the `evaluation/reports/` directory with the following structure:

```json
{
  "run_id": "uuid",
  "started_at": "ISO-8601 timestamp",
  "finished_at": "ISO-8601 timestamp",
  "duration_seconds": 0.0,
  "environment": {
    "node_version": "v18.x.x",
    "platform": "linux/win32/darwin",
    "arch": "x64/arm64"
  },
  "before": {
    "tests": {
      "passed": true/false,
      "return_code": 0/1,
      "output": "jest output (truncated)"
    },
    "metrics": {}
  },
  "after": {
    "tests": {
      "passed": true/false,
      "return_code": 0/1,
      "output": "jest output (truncated)"
    },
    "metrics": {}
  },
  "comparison": {
    "passed_gate": true/false,
    "improvement_summary": "short human-readable summary"
  },
  "success": true/false,
  "error": null
}
```

## Test Definitions

- **Test (before)**: Tests from `repository_after/Counter.test.js`
- **Meta-Test (after)**: Tests from `tests/meta.test.js` (validates the test implementation)

## Exit Codes

- `0` - Passed gate (meta-test passed)
- `1` - Failed or error occurred
