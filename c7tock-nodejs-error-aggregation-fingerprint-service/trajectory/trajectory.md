# Problem-Solving Trajectory

1. Analyze the Requirements (Identify Core Technical Challenges):
   I analyzed the requirements for the SentinelProcessor service. The core challenge is the "Issue Explosion" problem — a single bug in a distributed system can generate thousands of near-identical crash logs per second, each with slightly different metadata (memory addresses, file paths, container IDs, timestamps). If every log is treated as unique, the monitoring system becomes useless. The fingerprinting algorithm must be precise enough to distinguish genuinely different bugs but tolerant enough to merge cosmetically different instances of the same root cause. Additional challenges include in-memory state management under high throughput, rolling-window frequency detection, and state-driven notification dispatch.
   Understanding error aggregation and fingerprinting in production: https://docs.sentry.io/concepts/data-management/event-grouping/

2. Define Determinism and Stability Constraints for the Fingerprinting Engine:
   I established strict constraints for the fingerprint algorithm: it must be fully deterministic (same logical input always yields the same SHA-256 hash), order-sensitive (normalization rules must be applied in a specific sequence to prevent one regex from consuming tokens intended for another), and environment-agnostic (hex addresses like `0x0045f2a3`, absolute file paths from any OS, line/column numbers, UUIDs, timestamps, and session/container IDs must all be stripped before hashing). I chose SHA-256 via Node's built-in `crypto` module because it provides a 64-character hex digest with negligible collision probability, and it is part of the standard library — no external dependencies needed.
   SHA-256 properties and why it suits fingerprinting: https://en.wikipedia.org/wiki/SHA-2

3. Design the Normalization Pipeline with Ordered Regex Rules:
   I implemented 11 regex-based normalization rules applied in a carefully ordered pipeline. The order matters because greedy patterns can consume tokens before more specific patterns match them. UUIDs are stripped first (they contain hex characters that the generic hex-ID pattern would otherwise fragment). Session and container IDs follow because they embed variable suffixes. Then `0x`-prefixed memory addresses are removed, followed by OS-specific file paths (Windows `C:\...` before Unix `/path/...` to avoid partial matches). Line/column numbers and timestamps come next, and the catch-all long hex-ID pattern is applied last. Each replacement uses a semantic placeholder (`<HEX>`, `<FILEPATH>`, `<UUID>`, etc.) to preserve the structural skeleton of the trace. Finally, whitespace is collapsed so formatting differences across containers don't affect the hash.
   Regex ordering pitfalls and greedy matching: https://www.regular-expressions.info/repeat.html

4. Implement the ErrorStore as an In-Memory Aggregation Map:
   I built the ErrorStore around a JavaScript `Map` keyed by fingerprint. Each entry tracks five fields: `hitCount` (incremented on every upsert), `firstSeenAt` (set once on creation, never overwritten), `lastSeenAt` (updated every upsert), `lastMessage` (replaced with the most recent report's message for debugging context), and `highestSeverity` (updated only if the incoming severity ranks higher on a numeric scale from `debug=0` to `fatal=5`). The `upsert` method returns an `{ isNew, entry }` tuple so the caller can make notification decisions without a second lookup. Map-based storage gives O(1) insert and lookup, which is critical for the target throughput of hundreds of reports per second.
   JavaScript Map performance characteristics: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map

5. Build the FrequencyMonitor with a Rolling Timestamp Window:
   I implemented the velocity monitor using a Map of arrays, where each fingerprint maps to a sorted array of Unix timestamps. On every `record()` call, the current timestamp is appended and entries older than the configurable `windowMs` (default 60 seconds) are pruned from the front of the array. Burst detection checks how many timestamps fall within the `burstWindowMs` (default 10 seconds) by scanning backward from the end of the array, which is O(k) where k is the burst count — typically small. The pruning strategy prevents memory exhaustion because no fingerprint's timestamp list can grow beyond `windowMs / minimum_interval` entries. For a 60-second window at 1000 reports/second, the worst case is 60,000 entries per fingerprint, which is manageable.
   Sliding window algorithms for rate limiting: https://blog.bytebytego.com/p/rate-limiting-fundamentals

6. Implement State-Based Notification Dispatch Rules:
   I designed the NotificationDispatcher with two independent trigger paths. First, a `Set` of already-notified fingerprints ensures the "new issue" notification fires exactly once per unique fingerprint — subsequent calls with the same fingerprint are no-ops. Second, a burst cooldown map tracks the last time a burst notification was sent for each fingerprint. If a burst is detected (threshold exceeded within the burst window) and the cooldown period has elapsed, a burst alert fires. Both notification types are dispatched through a configurable `notifyFn` callback, making it trivial to swap in a real webhook, Slack integration, or message queue in the future. All dispatched notifications are logged in an internal array for auditability and testing.

7. Wire the SentinelProcessor Orchestrator with Graceful Error Handling:
   I built the SentinelProcessor as the top-level orchestrator that composes the fingerprint engine, error store, frequency monitor, and notification dispatcher. The `ingest()` method follows a strict sequence: validate input → generate fingerprint → upsert into store → record in frequency monitor → dispatch notifications. If the input is null or non-object, it increments an unparseable counter and returns null without crashing. If the fingerprint engine throws (e.g., on a pathologically malformed stack trace), it catches the exception, increments the unparseable metric, and falls back to a deterministic fallback fingerprint derived from the error message. This ensures the service never crashes on bad input while still tracking the metric for operational alerting. The `ingestBatch()` method processes arrays sequentially, and `reset()` clears all internal state for testing.

8. Result: Deterministic Aggregation Engine + Full Requirement Coverage:
   The solution implements a complete SentinelProcessor service using only Node.js standard libraries (crypto for SHA-256 hashing). The fingerprinting engine applies 11 ordered regex normalizations to produce stable 64-character hex fingerprints regardless of deployment environment. The ErrorStore provides O(1) upsert and lookup via Map. The FrequencyMonitor uses bounded timestamp arrays with automatic pruning to detect burst patterns within configurable windows. The NotificationDispatcher enforces exactly-once semantics for new issues and cooldown-gated burst alerts.
