Trajectory: Thread-Safe Snowflake ID Generator

The Problem: UUIDs Don't Play Nice With Databases

We needed IDs for a social media platform—things like posts, comments, user actions. UUIDs were the obvious choice at first, but they cause real pain. They're 128 bits. That's big. More importantly, they're random. When you insert a new row, the ID might sort anywhere in the index. The database keeps having to split pages and shuffle things around. Indexes get fragmented and writes get slower. We needed something smaller (64 bits) and time-ordered so new rows land at the end of the index and the database stays happy.

The other headache is the clock. Servers sometimes correct their time—NTP adjusts the clock, or someone changes it by hand. If the clock jumps backwards and we keep generating IDs, we might hand out an ID we already gave out five seconds ago. Duplicate primary key, crash. So the generator had to refuse to generate when it detected the clock going backwards.

The Solution: Snowflake-Style 64-Bit IDs

We pack one 64-bit integer like this: 1 bit unused (sign, always 0), 41 bits for milliseconds since our custom epoch, 10 bits for which server (machine ID), and 12 bits for a sequence number so we can do many IDs in the same millisecond. That gives us roughly time-sorted IDs, uniqueness per server, and up to 4096 IDs per millisecond per server. We use a fixed epoch (Jan 1, 2024) so the 41-bit timestamp has a clear meaning and we're not tied to 1970. When the clock goes backwards we don't guess—we raise a dedicated error (ClockMovedBackwardsError) and let the caller decide what to do. When we hit 4096 IDs in one millisecond we don't reuse sequence numbers; we wait until the next millisecond. That keeps the time sort guarantee and avoids collisions.

Implementation Steps

Every call to next_id() runs inside a single threading.Lock. Only one thread is building an ID at a time. That way we never have two threads sharing the same last timestamp and sequence and stepping on each other. At the start of next_id() we read the current time in milliseconds. If it's less than the last time we used, we raise ClockMovedBackwardsError and stop. We never emit an ID when the clock has moved back. If the current millisecond is the same as the last one we used, we bump the sequence. If sequence would go past 4095, we busy-wait in a loop until time.time() gives us a new millisecond, then we reset sequence to 0 and use that new timestamp. If we're already in a new millisecond, we just reset sequence to 0 and use the new time. We take the timestamp (relative to our custom epoch), mask it to 41 bits, shift it left. We mask machine_id to 10 bits and sequence to 12 bits and shift them into the right positions. One integer with bitwise OR. No strings, no extra libraries.

Why I did it this way (Refinement)

I thought about using time.sleep(0.001) when we run out of sequence in the same millisecond, to avoid burning CPU. The spec said busy-wait until the next millisecond, so I stuck with a tight loop that only reads time.time() until it advances. It keeps behavior predictable and matches the requirement. For clock rollback I didn't try to wait until time catches up or auto-correct. Raising an error is safer. The app can log, alert, or retry. Generating a duplicate ID would be worse than failing fast. I kept the lock around the entire body of next_id(). You could try to shrink the critical section and only lock around the read time, update state, build ID part, but the logic is already small and the extra contention from holding the lock a bit longer is acceptable. One lock for the whole call keeps the code easy to reason about.

Testing

We can't rely on the real clock for repeatable tests, so we mock time.time(). For the structure test we generate one ID, unpack the bits (right shifts and masks), and check that the timestamp in the ID matches now (relative to our epoch) and the machine ID matches what we configured. That proves the 64-bit layout is correct. For concurrency we spawn 100 threads, each generating 1000 IDs, and throw every ID into a set. If the set has 100,000 elements we know there are no duplicates. That only holds if the generator is actually thread-safe. For clock rollback we patch time.time() to return T, generate one ID, then patch it to return T minus 1 and call next_id() again. We assert that ClockMovedBackwardsError is raised. No real clock move required. For sequence overflow we patch time.time() to return the same value for the first 4097 calls (so the generator thinks it's the same millisecond), then return a time one millisecond later so the busy-wait can exit. We generate 4097 IDs and check that the last one has a timestamp one millisecond ahead. That proves we waited for the next ms instead of reusing sequence numbers.
